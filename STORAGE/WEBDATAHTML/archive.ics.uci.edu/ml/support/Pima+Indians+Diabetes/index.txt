UCI Machine Learning Repository: Pima Indians Diabetes Data Set: Support



<!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.01 Transitional//EN\">
<html>
<head>
<title>UCI Machine Learning Repository: Pima Indians Diabetes Data Set: Support</title>

<!-- Stylesheet link -->
<link rel="stylesheet" type="text/css" href="../assets/ml.css" />

<script language="JavaScript" type="text/javascript">
<!--
function checkform ( form )
{
  // see http://www.thesitewizard.com/archive/validation.shtml
  // for an explanation of this script and how to use it on your
  // own website

  // ** START **
  if (form.q.value == "")
  {
    alert( "Please enter search terms." );
    form.q.focus();
    return false ;
  }

  if (getCheckedValue(form.sitesearch) == "ics.uci.edu" && form.q.value.indexOf("site:archive.ics.uci.edu/ml") == -1)
  {
    form.q.value = form.q.value + " site:archive.ics.uci.edu/ml";
  }

  // ** END **
  return true ;
}

// return the value of the radio button that is checked
// return an empty string if none are checked, or
// there are no radio buttons
function getCheckedValue(radioObj) {
	if(!radioObj)
		return "";
	var radioLength = radioObj.length;
	if(radioLength == undefined)
		if(radioObj.checked)
			return radioObj.value;
		else
			return "";
	for(var i = 0; i < radioLength; i++) {
		if(radioObj[i].checked) {
			return radioObj[i].value;
		}
	}
	return "";
}
//-->
</script>

</head>

<body>


<!-- SITE HEADER (INCLUDES LOGO AND SEARCH BOX) -->

<table width=100% bgcolor="#003366">
<tr>
	<td>
		<span class="normal"><a href="../index.html" 
alt="Home"><img src="../assets/logo.gif" 
border=0></img></a><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="http://cml.ics.uci.edu"><font color="FFDD33">Center for Machine Learning and Intelligent Systems</font></a></span>
	</td>
	<td width=100% valign=top align="right">
		<span class="whitetext">
		<a href="../about.html">About</a>&nbsp;
		<a href="../citation_policy.html">Citation Policy</a>&nbsp;
		<a href="../donation_policy.html">Donate a Data Set</a>&nbsp;
		<a href="../contact.html">Contact</a>
		</span>

		<br>
		<br>
		<!-- Search Google -->

		<FORM method=GET action=http://www.google.com/custom onsubmit="return checkform(this);">
		<INPUT TYPE=text name=q size=30 maxlength=255 value="">
		<INPUT type=submit name=sa VALUE="Search">
		<INPUT type=hidden name=cof VALUE="AH:center;LH:130;L:http://archive.ics.uci.edu/assets/logo.gif;LW:384;AWFID:869c0b2eaa8d518e;">
		<input type=hidden name=domains value="ics.uci.edu">
		<br>
		<input type=radio name=sitesearch value="ics.uci.edu" checked> <span class="whitetext"><font size="1">Repository</font></span>
		<input type=radio name=sitesearch value=""> <span class="whitetext"><font size="1">Web</font></span>
		&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
		<A HREF=http://www.google.com/search><IMG SRC=http://www.google.com/logos/Logo_25blk.gif border=0 ALT=Google align=middle height=27></A>
		<br>
		</FORM>
		<!-- Search Google -->


		<span class="whitetext"><a href="../datasets.html"><font size="3" color="#FFDD33"><b>View ALL Data Sets</b></font></a></span>
		<br>
	</td>
</tr>
</table>

<br />
<table width=100% border=0 cellpadding=2><tr><td>


   <table><tr>
     <td valign=top>
	<p>
	<span class="heading"><b>Pima Indians Diabetes Data Set</b></span>

		
				<p class="normal">Below are papers that cite this data set, with context shown.
		Papers were automatically harvested and associated with this data set, in collaboration with <a href="http://rexa.info">Rexa.info</a>.</p>
		<img src="../assets/rexa.jpg" />
		<p class="normal"><a href="/ml/datasets/Pima+Indians+Diabetes">Return to Pima Indians Diabetes data set page</a>.
		<hr><p class="normal"><a name="d64d2705cabed449e8cb2ecc3c3c77c54ee71051"></a><i>Jeroen Eggermont and Joost N. Kok and Walter A. Kosters. <a href="http://rexa.info/paper/d64d2705cabed449e8cb2ecc3c3c77c54ee71051">Genetic Programming for data classification: partitioning the search space</a>. SAC. 2004. </i><br><br>The results of our refined gp algorithm using the gain ratio criterion are again worse than those of our clustering and other refined gp algorithms. The <b>Pima</b> <b>Indians</b> <b>diabetes</b> Data Set On the Pima Indians diabetes data set (see Table 5) the refined gp algorithms using the gain criterion are again better than those using the gain ratio criterion. If we compare the results of our<br></p><hr><p class="normal"><a name="bf8052c51e6338fa2ab5479c08d657a06e4dbd4a"></a><i>Eibe Frank and Mark Hall. <a href="http://rexa.info/paper/bf8052c51e6338fa2ab5479c08d657a06e4dbd4a">Visualizing Class Probability Estimators</a>. PKDD. 2003. </i><br><br>(although they are not explicitly represented in the classifier). To provide a more realistic example Figure 8 shows four visualizations for pairs of attributes from the <b>pima</b> <b>indians</b> <b>diabetes</b> dataset [1]. This dataset has eight attributes and 768 instances (500 belonging to class tested_negative plas mass <= 127 mass } 127 tested_negative (132.0/3.0) <= 26.4 age } 26.4 tested_negative<br></p><hr><p class="normal"><a name="076c5b4e7bf69148bcde8c83621ecf1140a7d55c"></a><i>Michael L. Raymer and Travis E. Doom and Leslie A. Kuhn and William F. Punch. <a href="http://rexa.info/paper/076c5b4e7bf69148bcde8c83621ecf1140a7d55c">Knowledge discovery in medical and biological datasets using a hybrid Bayes classifier/evolutionary algorithm</a>. IEEE Transactions on Systems, Man, and Cybernetics, Part B, 33. 2003. </i><br><br>with the nonlinear discriminant function and the knn classifier. In all cases the nonlinear discriminant classifier is significantly faster than the EC/knn---in the case of the <b>Pima</b> Indian <b>diabetes</b> data set the difference is nearly tenfold. B. Classification of Medical Data Two additional data sets, also selected from the UCI repository, were employed by [40, 41] in a comparative study of<br></p><hr><p class="normal"><a name="d8bc3a2d334e33553c2c7e052ce8c2fd9ae5718b"></a><i>Marina Skurichina and Ludmila Kuncheva and Robert P W Duin. <a href="http://rexa.info/paper/d8bc3a2d334e33553c2c7e052ce8c2fd9ae5718b">Bagging and Boosting for the Nearest Mean Classifier: Effects of Sample Size on Diversity and Accuracy</a>. Multiple Classifier Systems. 2002. </i><br><br>are taken from the UCI Repository [22]. They are the 8dimensional <b>pima</b> <b>diabetes</b> data set, the 34-dimensional ionosphere data set and the 60-dimensional sonar data set. Training sets are chosen randomly and the remaining data are used for testing. All experiments are repeated 50 times on<br></p><hr><p class="normal"><a name="ded3146242d6b322f31afbf57a8afde22e4ec8e1"></a><i>Ilya Blayvas and Ron Kimmel. <a href="http://rexa.info/paper/ded3146242d6b322f31afbf57a8afde22e4ec8e1">Multiresolution Approximation for Classification</a>. CS Dept. Technion. 2002. </i><br><br>� D). 3 Experimental Results The proposed method was implemented in VC++ 6.0 and run on `IBM PC 300 PL' with 600MHZ Pentium III processor and 256MB RAM. It was tested on the <b>Pima</b> <b>Indians</b> <b>Diabetes</b> dataset [10], and a large artificial dataset generated with the DatGen program [11]. The results were compared to the Smooth SVM [12] and Sparse Grids [3]. 3.1 Pima Indians The Pima Indians Diabetes<br></p><hr><p class="normal"><a name="da335006c9ae6a44b5b680d00b15c00c0e22e0de"></a><i>Tao Jiang and Art B. Owen. <a href="http://rexa.info/paper/da335006c9ae6a44b5b680d00b15c00c0e22e0de">Quasi-regression for visualization and interpretation of black box functions</a>. Department of Statistics Stanford University. 2002. </i><br><br>is a determination of whether a given woman is diabetic. There are 7 predictors, including medical measurements and personal history. All of the women are <b>Pima</b> <b>Indians</b>  We used the version of this data set found in Ripley (1996). There are @#@# complete cases for training and �$�$ for a test set. The number of pregnancies was replaced by б�$�##2# � number of pregnancies # . Then it and the other<br></p><hr><p class="normal"><a name="d328ae33fb50756832a1c6cd703f7176c361923f"></a><i>Peter Sykacek and Stephen J. Roberts. <a href="http://rexa.info/paper/d328ae33fb50756832a1c6cd703f7176c361923f">Adaptive Classification by Variational Kalman Filtering</a>. NIPS. 2002. </i><br><br>which were both used as training and independent test sets respectively. We also use the <b>pima</b> <b>diabetes</b> data set from [16] 3 . Table 1 compares the generalization accuracies (in fractions) obtained with the variational Kalman filter with generalization accuracies obtained with sequential variational inference.<br></p><hr><p class="normal"><a name="4018f616747e1e9c96771daa20bca34c484b966a"></a><i>Jochen Garcke and Michael Griebel and Michael Thess. <a href="http://rexa.info/paper/4018f616747e1e9c96771daa20bca34c484b966a">Data Mining with Sparse Grids</a>. Computing, 67. 2001. </i><br><br>more than 96 % of the computation time is spent for the matrix assembly. Again, the execution times scale linearly with the number of data points. 3.3 8-dimensional problem The <b>Pima</b> <b>Indians</b> <b>Diabetes</b> data set from Irvine Machine Learning Database Repository consists of 768 instances with 8 features plus a class label which splits the data into 2 sets with 500 instances and 268 instances respectively, see<br></p><hr><p class="normal"><a name="c185d513badef2336ca48f64098d4b5df17bf5a4"></a><i>Robert Burbidge and Matthew Trotter and Bernard F. Buxton and Sean B. Holden. <a href="http://rexa.info/paper/c185d513badef2336ca48f64098d4b5df17bf5a4">STAR - Sparsity through Automated Rejection</a>. IWANN (1). 2001. </i><br><br>has 270 examples in 13 dimensions. The <b>Pima</b> <b>Indians</b> <b>diabetes</b> data set has 768 examples in eight dimensions. These last two data sets have a high degree of overlap which leads to a dense model for the standard SVM as many training errors contribute to the solution. The<br></p><hr><p class="normal"><a name="989443f4feec00743b3521c1b450717f6013de20"></a><i>Simon Tong and Daphne Koller. <a href="http://rexa.info/paper/989443f4feec00743b3521c1b450717f6013de20">Restricted Bayes Optimal Classifiers</a>. AAAI/IAAI. 2000. </i><br><br>exact training error. We investigated whether using a non-zero value of oe would achieve a similar effect to that of the soft margin error function. 1 We used the  <b>Pima</b> Indian <b>Diabetes</b>  UC Irvine data set (Blake, Keogh, & Merz 1998) and a synthetic data set. The Pima data set has eight features, with 576 training instances of which 198 are labeled as positive. The synthetic data were generated from<br></p><hr><p class="normal"><a name="dc4f2babc5ca4534b435280aec32f5816ddb53b0"></a><i>Stavros J. Perantonis and Vassilis Virvilis. <a href="http://rexa.info/paper/dc4f2babc5ca4534b435280aec32f5816ddb53b0">Input Feature Extraction for Multilayered Perceptrons Using Supervised Principal Component Analysis</a>. Neural Processing Letters, 10. 1999. </i><br><br>the basis of 6 attributes originating from blood test results and daily alcohol consumption figures. The set comprises 345 patterns with 6 features for each pattern. 3. the  <b>Pima</b> <b>Indians</b> <b>Diabetes</b>  data set [15]. It comprises 768 patterns taken from patients who may show signs of diabetes. Each sample is described by 8 attributes. 4. The "Sonar Targets" dataset [16]. The task is to distinguish between<br></p><hr><p class="normal"><a name="96246754b7958de8a635165e8b145c264240cd3d"></a><i>Huan Liu and Rudy Setiono. <a href="http://rexa.info/paper/96246754b7958de8a635165e8b145c264240cd3d">Feature Transformation and Multivariate Decision Tree Induction</a>. Discovery Science. 1998. </i><br><br>those of OC1's, in which two of the OC1's trees are smaller. In 9 cases, trees by BMDT are significantly different from those of CART's, in which only one of CART's trees is smaller. An example: The dataset is <b>Pima</b> <b>diabetes</b>  In Table 3, it is seen that C4.5 creates a UDT with average tree size of 122.4 nodes, BMDT builds an MDT with average tree size of 3 nodes. That means the MDT has one root and two<br></p><hr><p class="normal"><a name="5f8eb537fc397ad5e506b2eae4f6676b48990ba6"></a><i>Thomas G. Dietterich. <a href="http://rexa.info/paper/5f8eb537fc397ad5e506b2eae4f6676b48990ba6">Approximate Statistical Test For Comparing Supervised Classification Learning Algorithms</a>. Neural Computation, 10. 1998. </i><br><br>measured on the 10,000 calibration examples) matched the average performance of C4.5 to within 0.1%. For the <b>Pima</b> <b>Indians</b> <b>Diabetes</b> data set, we drew 1000 data sets of size 300 from the 768 available examples. For each of these data sets, the remaining 468 examples were retained for calibration. Each of the 1000 data sets of size 300 was<br></p><hr><p class="normal"><a name="63ebbe51c9c4dea76320f7b6a40f2a59f10cc7c0"></a><i>Kristin P. Bennett and Erin J. Bredensteiner. <a href="http://rexa.info/paper/63ebbe51c9c4dea76320f7b6a40f2a59f10cc7c0">A Parametric Optimization Method for Machine Learning</a>. INFORMS Journal on Computing, 9. 1997. </i><br><br>is available via anonymous ftp from the UCI Repository Of Machine Learning Databases [MA92]. <b>Pima</b> <b>Indians</b> <b>Diabetes</b> Database The Pima Diabetes dataset consists of 768 female patients who are at least 21 years of age and are of Pima Indian heritage. The 8 numeric attributes describe physical features of each patient. This dataset is also available<br></p><hr><p class="normal"><a name="19e1b6e0932bbe665a2c4a069a0636d8d5cf0c6f"></a><i>Jennifer A. Blue and Kristin P. Bennett. <a href="http://rexa.info/paper/19e1b6e0932bbe665a2c4a069a0636d8d5cf0c6f">Hybrid Extreme Point Tabu Search</a>. Department of Mathematical Sciences Rensselaer Polytechnic Institute. 1996. </i><br><br>are: the BUPA Liver Disease dataset (Liver); the <b>PIMA</b> <b>Indians</b> <b>Diabetes</b> dataset (Diabetes), the Wisconsin Breast Cancer Database (Cancer) [23], and the Cleveland Heart Disease Database (Heart) [9]. We used 5-fold cross validation. Each<br></p><hr><p class="normal"><a name="f6f302674e1188614eaa6f23c782d8d1c0ea2320"></a><i>Peter D. Turney. <a href="http://rexa.info/paper/f6f302674e1188614eaa6f23c782d8d1c0ea2320">Cost-Sensitive Classification: Empirical Evaluation of a Hybrid Genetic Decision Tree Induction Algorithm</a>. CoRR, csAI/9503102. 1995. </i><br><br>sum of the absolute values of the differences. The difference between two values was defined to be 1 if one or both of the two values was missing. A.4 <b>Pima</b> <b>Indians</b> <b>Diabetes</b> The Pima Indians Diabetes dataset was donated by Vincent Sigillito. 22 The data were collected by the National Institute of Diabetes and Digestive and Kidney Diseases. Table 21 shows the test costs for the Pima Indians Diabetes<br></p><hr><p class="normal"><a name="87afa910f706df9e28bfa697d6d2eea7c0cb53ef"></a><i>Ilya Blayvas and Ron Kimmel. <a href="http://rexa.info/paper/87afa910f706df9e28bfa697d6d2eea7c0cb53ef">Efficient Classification via Multiresolution Training Set Approximation</a>. CS Dept. Technion. </i><br><br>� D). 3 Experimental Results The proposed method was implemented in VC++ 6.0 and run on `IBM PC 300 PL' with 600MHZ Pentium III processor and 256MB RAM. It was tested on the <b>Pima</b> <b>Indians</b> <b>Diabetes</b> dataset [10], and a large artificial dataset generated with the DatGen program [11]. The results were compared to the Smooth SVM [12] and Sparse Grids [3]. Figure 7: Partition of 2D feature space for a<br></p><hr><p class="normal"><a name="19d5e6db33eb1a6c31be626538ea656f79e901b1"></a><i>Matthias Scherf and W. Brauer. <a href="http://rexa.info/paper/19d5e6db33eb1a6c31be626538ea656f79e901b1">Feature Selection by Means of a Feature Weighting Approach</a>. GSF - National Research Center for Environment and Health. </i><br><br>of feature f2g. The application of the RBF-DDA lead to an approximately equal result, i.e. 97% classification accuracy with 152 RBF nodes. <b>Pima</b> <b>Indians</b> <b>Diabetes</b> Data Base The Pima Indians Diabetes data set contains 768 instances with 8 real valued features. The underlying task is to decide, whether an at least 21 year old female of Pima Indian heritage shows signs of diabetes according to World Health<br></p><hr><p class="normal"><a name="9a75a9a8ce786d6a05ad51afa124cd4f70bfbb36"></a><i>Rudy Setiono and Huan Liu. <a href="http://rexa.info/paper/9a75a9a8ce786d6a05ad51afa124cd4f70bfbb36">Neural-Network Feature Selector</a>. Department of Information Systems and Computer Science National University of Singapore. </i><br><br>or republican. We selected 197 patterns for training randomly, 21 patterns for cross-validation, and 217 patterns for testing. Schlimmer [18] reported getting an accuracy rate of 90%-95% on this dataset. 3. <b>Pima</b> <b>Indians</b> <b>Diabetes</b> Dataset. The dataset consists of 768 samples taken from patients who may show signs of diabetes. 15 Each sample is described by 8 attributes, 1 attribute has discrete<br></p><hr><p class="normal"><a name="21ab0b68f14e5f4b1f401fa7bca294c1dc48061b"></a><i>Christopher P. Diehl and Gert Cauwenberghs. <a href="http://rexa.info/paper/21ab0b68f14e5f4b1f401fa7bca294c1dc48061b">SVM Incremental Learning, Adaptation and Optimization</a>. Applied Physics Laboratory Johns Hopkins University. </i><br><br>that h = 0 when the final perturbation is complete. IV. EXPERIMENTAL RESULTS In order to assess the benefits offered by the incremental framework, we conducted two experiments using the <b>Pima</b> <b>Indians</b> dataset from the UCI machine learning repository [1]. Using an RBF kernel K(x, y) = exp # - kx - yk 2 # 2 # , we first fixed the kernel width and varied (increased or decreased) the regularization parameter<br></p><hr><p class="normal"><a name="8afa6796645ce4b0642db26c822cf6bfa8cc4d0d"></a><i>Wl odzisl/aw Duch and Rudy Setiono and Jacek M. Zurada. <a href="http://rexa.info/paper/8afa6796645ce4b0642db26c822cf6bfa8cc4d0d">Computational intelligence methods for rule-based data understanding</a>. </i><br><br>vector. Prototype-based rules are little known so far and computationally more costly to find, but certainly for some data, they may be simple and accurate. D. <b>Diabetes</b>  The  <b>Pima</b> Indian Diabetes" dataset [118] is also frequently used as a benchmark data [89], [135], [134], [136]. All patients were females, at least 21 years old, of Pima Indian heritage. 768 cases have been collected, 500 (65.1%)<br></p><hr><p class="normal"><a name="d26e510dd613f1f7821b5f6c4e9fe5868aa5f006"></a><i>Michalis K. Titsias and Aristidis Likas. <a href="http://rexa.info/paper/d26e510dd613f1f7821b5f6c4e9fe5868aa5f006">Shared Kernel Models for Class Conditional Density Estimation</a>. </i><br><br>and two from the UCI repository [13]  <b>Pima</b> <b>Indians</b> and Ionosphere data sets). To assess the performance of the models for each problem we have selected the five-fold cross-validation method. For each problem the original set was divided into five independent parts<br></p><hr><p class="normal"><a name="4773165a9efff672af50f07beb5d38c9afc92f5f"></a><i>Lawrence O. Hall and Nitesh V. Chawla and Kevin W. Bowyer. <a href="http://rexa.info/paper/4773165a9efff672af50f07beb5d38c9afc92f5f">Combining Decision Trees Learned in Parallel</a>. Department of Computer Science and Engineering, ENB 118 University of South Florida. </i><br><br>The Iris data (Fisher 1936; Merz & Murphy ) which has 4 continuous valued attributes and classifies 150 examples as one of 3 classes of Iris plant. The second is the <b>Pima</b> <b>Indians</b> <b>Diabetes</b> data set (Merz & Murphy ) which has 8 numeric attributes and classifies 768 examples into one of 2 classes. We have done an experiment simulating a parallel 2-processor implementation for both data sets and<br></p><hr><p class="normal"><a name="b6e169d69cd67763b95698e8961696fec9ca93bf"></a><i>Charles Campbell and Nello Cristianini. <a href="http://rexa.info/paper/b6e169d69cd67763b95698e8961696fec9ca93bf">Simple Learning Algorithms for Training Support Vector Machines</a>. Dept. of Engineering Mathematics. </i><br><br>Service [17]. As examples of the improvements with generalisation ability which can be achieved with a soft margin we will also describe experiments with the ionosphere and <b>Pima</b> <b>Indians</b> <b>diabetes</b> datasets from the UCI Repository [4]. Though we have successfully used other kernels with KA we will only describe experiments using Gaussian kernels in this section. We will predominantly use the KA<br></p><hr><p class="normal"><a name="c5ac7c07f095dd7068c3ab10d4e7615b6d1564d5"></a><i>Liping Wei and Russ B. Altman. <a href="http://rexa.info/paper/c5ac7c07f095dd7068c3ab10d4e7615b6d1564d5">An Automated System for Generating Comparative Disease Profiles and Making Diagnoses</a>. Section on Medical Informatics Stanford University School of Medicine, MSOB X215. </i><br><br>diagnosis. This finding is consistent with the study by Ohno-Machado. 9 We realize that the difference may not be statistically significant and that further studies are needed. <b>diabetes</b> The diabetes data set consisted of 768 females patients of <b>Pima</b> Indian heritage who were at least 21 years old. Eight attributes were collected for each patient. A class variable was also documented, 1 as "having<br></p><hr><p class="normal"><a name="4695569c53cd581fcc193415a8a94a1f92abf607"></a><i>Chotirat Ann and Dimitrios Gunopulos. <a href="http://rexa.info/paper/4695569c53cd581fcc193415a8a94a1f92abf607">Scaling up the Naive Bayesian Classifier: Using Decision Trees for Feature Selection</a>. Computer Science Department University of California. </i><br><br>instances, 22 attributes, 2 classes. Attributes selected by SBC = 6. <b>Pima</b> <b>Indians</b> Diabetes 60 65 70 75 80 85 10203040506070809099 Training Data (%) Accuracy (%) NBC SBC C4.5 Figure 7. Pima-Indians dataset. 768 instances, 8 attributes, 2 classes. Attributes selected by SBC = 5. Promoter Gene Sequences 40 50 60 70 80 90 100 10 20 30 40 50 60 70 80 90 99 Training Data (%) Accuracy (%) NBC SBC C4.5<br></p><hr><p class="normal"><a name="c5da3959b262c7f9f7d339d5a61d1af0ed0ee805"></a><i>Federico Divina and Elena Marchiori. <a href="http://rexa.info/paper/c5da3959b262c7f9f7d339d5a61d1af0ed0ee805">Knowledge-Based Evolutionary Search for Inductive Concept Learning</a>. Vrije Universiteit of Amsterdam. </i><br><br>training examples. It can be seen that in most of the cases, the EWUS selection operator leads to a population characterized by a higher diversity. Only in two cases (the breast and the <b>pima</b> <b>indians</b> datasets) the population evolved with the use of the standard US operators has a higher diversity. However, also in these two cases, the diversity of the two populations are comparable. The WUS selection<br></p><hr><p class="normal"><a name="9a0080df56f0a8fb004723a7dece29109bc316ce"></a><i>Michael Lindenbaum and Shaul Markovitch and Dmitry Rusakov. <a href="http://rexa.info/paper/9a0080df56f0a8fb004723a7dece29109bc316ce">Selective Sampling Using Random Field Modelling</a>. </i><br><br>Among them there were three natural datasets: <b>Pima</b> <b>Indians</b> <b>Diabetes</b> dataset, Ionosphere dataset and Image Segmentation dataset, one synthetic dataset: Letters dataset and three artificial problems: Two-Spirals problem, Two-Gaussians problem<br></p><hr><p class="normal"><a name="954de642cab661d060a2dbc68d3023ba3a9763e1"></a><i>Federico Divina and Elena Marchiori. <a href="http://rexa.info/paper/954de642cab661d060a2dbc68d3023ba3a9763e1">Handling Continuous Attributes in an Evolutionary Inductive Learner</a>. Department of Computer Science Vrije Universiteit. </i><br><br>by the results of the t-test, summarized in Table 7. Using 1% confidence level we get that ECL-LSDc is never outperformed, while it is significantly better than the other methods on the <b>Pima</b> <b>Indians</b> dataset, better than ECL-GSD on the Breast dataset, and better than ECL-LUD on the Ionosphere dataset, together with ECL-LSDf and ECL-GSD. If we increase the confidence level to 5% then we get that ECL-LUD<br></p><hr><p class="normal"><a name="5a08d3d508c193c4964f1f3f66bdcb78b98a905b"></a><i>Ilya Blayvas and Ron Kimmel. <a href="http://rexa.info/paper/5a08d3d508c193c4964f1f3f66bdcb78b98a905b">INVITED PAPER Special Issue on Multiresolution Analysis Machine Learning via Multiresolution Approximation</a>. </i><br><br>problems with huge training sets seems to be important family of problems, it was hard to find such training sets in public databases. Our method was tested on the <b>Pima</b> <b>Indians</b> <b>Diabetes</b> dataset [4], a large artificial dataset generated with the DatGen program [14] and the Forest Cover Type data set. The results were compared to [3], [8], [9], [11], [12]. 3.1 Pima Indians Dataset This is an<br></p><hr><p class="normal"><a name="b4b7ea88c807c5c8746bf3067f13b1d07899bcc9"></a><i>Andrew Watkins and Jon Timmis and Lois C. Boggess. <a href="http://rexa.info/paper/b4b7ea88c807c5c8746bf3067f13b1d07899bcc9">Artificial Immune Recognition System (AIRS): An ImmuneInspired Supervised Learning Algorithm</a>. (abw5,jt6@kent.ac.uk) Computing Laboratory, University of Kent. </i><br><br>where classification accuracy of 98% was achieved using a k-value of 3. This seemed to bode well, and further experiments were undertaken using the Fisher Iris data set, <b>Pima</b> <b>diabetes</b> data, Ionosphere data and the Sonar data set, all obtained from the repository at the University of California at Irvine [4]. Table II shows the performance of AIRS on these data sets<br></p>


	</td></tr></table>



<hr>

<p class="normal"><a href="/datasets/Pima+Indians+Diabetes">Return to Pima Indians Diabetes data set page</a>.


<table cellpadding=5 align=center><tr valign=center>
		<td><p class="normal">Supported By:</p></td>
        <td><img src="../assets/nsfe.gif" height=60 /> </td>
        <td><p class="normal">&nbsp;In Collaboration With:</p></td>
        <td><img src="../assets/rexaSmall.jpg" /></td>
</tr></table>

<center>
<span class="normal">
<a href="../about.html">About</a>&nbsp;&nbsp;||&nbsp;
<a href="../citation_policy.html">Citation Policy</a>&nbsp;&nbsp;||&nbsp;
<a href="../donation_policy.html">Donation Policy</a>&nbsp;&nbsp;||&nbsp;
<a href="../contact.html">Contact</a>&nbsp;&nbsp;||&nbsp;
<a href="http://cml.ics.uci.edu">CML</a>
</span>
</center>




</body>
</html>
