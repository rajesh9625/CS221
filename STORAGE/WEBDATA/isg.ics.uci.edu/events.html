ISGISGInformation Systems GroupBren School of ICSUC IrvineAboutNewsPeopleResearchPublicationsEventsCoursesPartnershipsVisitorsISG SeminarsInvited TalksUpcomingPastISG SeminarsRegular ISG seminar Time: Every Fri afternoon, 2pm - 3:00pm; Location: Bren Hall 3011
		2009-2010 ISG Scalable Data Management Seminar Series [talks]Support for the ISG Seminar Series from Yahoo! is gratefully acknowledged.
        Invited Talks Upcoming Events Feb 12, 2016SPEAKER: Wenjun ZengHuman Centric Video Analytic
             DetailsDate and TimeFeb 12, 2016 2PMLocationDBH 3011SpeakerWenjun ZengTitleHuman Centric Video Analytic
             AbstractVideo is the biggest big data that contains an enormous amount of information. At Microsoft Research Asia, we are leveraging computer vision and deep learning to develop a cloud-based intelligence engine that can turn raw video data into insights to facilitate various applications and services. In this talk, I will introduce our recent effort on human centric video analysis and present some latest technologies we have developed including online learning based face/human tracking/identification, skeleton-based human action recognition using regularized deep LSTM networks, and real-time human action detection and forecasting in streaming video, etc. I will also shed some light on the go-to-market aspect of this intelligent cloud effort.Speaker BioWenjun (Kevin) Zeng is a Principal Research Manager overseeing the Internet Media Group and the Media Computing Group at Microsoft Research Asia, while on leave from the Univ. of Missouri (MU). He had worked for PacketVideo Corp., Sharp Labs of America, Bell Labs, and Panasonic Technology prior to joining MU in 2003. Wenjun has contributed significantly to the development of international standards (ISO MPEG, JPEG2000, and OMA).  He received his B.E., M.S., and Ph.D. degrees from Tsinghua Univ., the Univ. of Notre Dame, and Princeton Univ., respectively.  His current research interest includes mobile-cloud media computing, computer vision, social network/media analysis, multimedia communications, and content/network security.
              
              He is an Associate Editor-in-Chief of IEEE Multimedia Magazine, an AE of IEEE Trans. on Circuits & Systems for Video Technology (TCSVT), and was an AE of IEEE Trans. on Info. Forensics & Security and IEEE Trans. on Multimedia (TMM). He is/was on the Steering Committee of IEEE Trans. on Mobile Computing (current) and IEEE TMM (2009-2012). He served as the Steering Committee Chair of IEEE ICME in 2010 and 2011, and has served as the TPC Chair of several IEEE conferences (e.g., ChinaSIP’15, WIFS’13, ICME’09, CCNC’07). He will be a general co-Chair of ICME2018. He is currently guest editing a TCSVT Special Issue on Visual Computing in the Cloud - Mobile Computing, and was a Special Issue Guest Editor for the Proceedings of the IEEE, IEEE TMM, and ACM TOMCCAP.  He is a Fellow of the IEEE. Past Events Jan 22, 2016SPEAKER: Pat Helland (Salesforce.com)Subjective Consistency 
             DetailsDate and TimeJan 22, 2016 2PMLocationDBH 3011SpeakerPat Helland (Salesforce.com)TitleSubjective Consistency 
             AbstractGray and Reuter define consistency as: “A transaction is a correct transformation of the state. The actions taken as a group do not violate any of the integrity constraints associated with the state. This requires that the transaction be a correct program.” Loosely translated, you should be happy with yourself. In your own opinion, you should be sane. Through the years, this most ill defined property of the ACID transaction has been conflated with isolation and interpreted through the prism of read/write updates to an apparently centralized record oriented database. This suits systems developers that need a narrow definition to feel they can accomplish something. Unfortunately, it leaves many real world problems in the dust. In this talk, we will explore the models of consistency and the facades of reality used in practical Systems.Speaker BioPat Helland has 37 years experience implementing databases, transaction systems, application platforms, replication systems, fault tolerance, and distributed systems.
             
             From 1973-1976, Pat attended UC Irvine in the ICS department, loved his ICS undergrad and grad classes, and lived non-stop at the computing facility as an operator on the PDP-10 and Xerox Sigma-7.  This was back in the day when a computer was the size of a room and the only way to program was to stay up all night accessing the timesharing system.  
             
             During the 1980s, Pat was Chief Architect of the Tandem NonStop's TMF (Transaction Monitoring Facility), the transaction processing and recovery engine behind NonStop SQL.
             
             He started working at Microsoft in 1994 and drove the design and architecture for MTS (Microsoft Transaction Server), the N-tier transactional computing environment for Windows as well DTC, the Distributed Transaction Coordinator. A few years later, Pat led the development of SQL Service Broker, a high speed exactly once transactional messaging system.
             
             From 2005 to early 2007, Pat worked at Amazon on the Product Catalog. In 2007, he returned to Microsoft working on a number of projects including adding indexing and affinitized placement of data into Cosmos, the massively parallel computation and storage engine behind Bing (Big Data). Cosmos supports exabytes of data running on hundreds of thousands of computers. He was one of the original architects for the real-time event driven transactional engine for Cosmos.
             
             Since early 2012, Pat has worked at Salesforce.com on database management, storage, and data center issues.  He was recently added to the UCI ICS Inaugural Hall of Fame.
             Dec 4th, 2015SPEAKER: Volker Mische (Couchbase)Indexing Data at Scale in Couchbase Server
             DetailsDate and TimeDec 4th, 2015 2PMLocationDBH 4011SpeakerVolker Mische (Couchbase)TitleIndexing Data at Scale in Couchbase Server
             AbstractThis talk will provide an overview of Couchbase Server with a focus on its new query and indexing capabilities. The talk will include a look under the hood at how NoSQL data is stored, scaled, indexed, and queried at scale in the Couchbase world.Speaker BioVolker Mische was the creator of GeoCouch, a geospatial extension for Apache CouchDB and Couchbase. A degree (diploma) in computer science with a minor in geography (University of Augsburg, Germany) and a one year intership at LISAsoft Pty Ltd, Australia in 2008 deepened his experience in the geospatial field. As a proponent of open source, Volker has contributed to various projects, including Apache CouchDB, MapQuery (project lead), OpenLayers, TileCache, GeoNetwork, Rockbox and WYMeditor.Nov 13th, 2015SPEAKER: Ahmed Eldawy (U. Minnesota)SpatialHadoop: A MapReduce Framework for Spatial Data
             DetailsDate and TimeNov 13th, 2015 2PMLocationDBH 4011SpeakerAhmed Eldawy (U. Minnesota)TitleSpatialHadoop: A MapReduce Framework for Spatial Data
             Abstract This talk describes SpatialHadoop, a full-fledged MapReduce framework which extends Hadoop to support spatial data processing efficiently. SpatialHadoop injects spatial data awareness inside the core of Hadoop to make it orders of magnitude more efficient than plain-vanilla Hadoop. It adapts traditional spatial indexes, such as R-tree, Quad tree and K-d tree, to HDFS and uses these indexes to support a wide range of spatial operations, including, range query, kNN, spatial join, and computational geometry operations. In addition, it provides a high level language, termed Pigeon, which makes the system easier for non-technical users. Furthermore, SpatialHadoop has an extensible and scalable visualization layer that is able to produce giga-pixel images for terabytes of data, which allows users to explore big spatial data. The open source nature of SpatialHadoop allows it to be used in a wide range of research projects and live applications, including, SHAHED, for querying and visualizing satellite data, TAREEG, a web-based extractor for OpenStreetMap data, and TAGHREED, a system for querying and analyzing Twitter data.Speaker BioAhmed Eldawy is a sixth year PhD candidate at the Computer Science and Engineering department in University of Minnesota. He is a member of the Data Management Lab under the supervision of Prof. Mohamed Mokbel. His broad area of research interest is in databases and data management. His main research topic for PhD purposes is spatial data management in distributed environments. He is the founder and creator of SpatialHadoop, one of the most widely used systems for processing spatial data using MapReduce. He will be visiting UCI to work with the AsterixDB team for one month in the Nov./Dec. 2015 timeframe.
             Nov 6th, 2015SPEAKER: Profs. Vassilis Tsotras (UC Riverside) & Sharad Mehrotra (UC Irvine)Dealing with BAD TIPPERS
             DetailsDate and TimeNov 6th, 2015 2PMLocationDBH 4011SpeakerProfs. Vassilis Tsotras (UC Riverside) & Sharad Mehrotra (UC Irvine)TitleDealing with BAD TIPPERS
             AbstractThe first full ISG Seminar of 2015-16 will be a sequence of two half-hour talks on new projects that are happening in the world of ISG.
             The first talk will describe the BAD project, where BAD is short for Big Active Data. BAD is a joint effort between UCR and UCI. First-generation Big Data management projects like AsterixDB have been passive in nature - queries, updates, and data analysis techniques have been scaled to handle very large volumes of data. In contrast, the BAD effort aims to develop techniques to continuously and reliably capture Big Data collections (arising from social, mobile, Web, and sensed data sources) and to provide timely delivery of the right information to the relevant end users. In short, BAD aims to provide a scalable foundation for moving from Big Passive Data to Big Active Data. Techniques are being developed to enable the accumulation and monitoring of petabytes of data of potential interest to millions of end users; when "interesting" new data appears, it should be delivered to end users in a time frame measured in (100's of) milliseconds. The NSF-sponsored BAD project has been underway for just one year, and it will eventually lead to a series of BAD results, BAD papers, and BAD open source software.
             The second talk will describe a brand new UCI project, sponsored by DARPA, called TIPPERS. To learn more about TIPPERS, you'll just have to come to the talk!Speaker BioVassilis Tsotras is a Professor of Computer Science at UC Riverside and co-PI of the AsterixDB project and now the BAD project, both of which span UCR and UCI. He is also the director of the newly formed Data Science Center at UCR. Sharad Mehrotra is a Professor of Computer Science at UC Irvine and co-PI of the TIPPERS project. He is also the Vice Chair for Graduate Studies here at UCI.Oct 9, 2015SPEAKER: David Lomet (Microsoft Research)Multi-Version Range Concurrency Control in Deuteronomy
             DetailsDate and TimeOct 9, 2015 2PMLocationDBH 4011SpeakerDavid Lomet (Microsoft Research)TitleMulti-Version Range Concurrency Control in Deuteronomy
             AbstractThe Deuteronomy transactional key value store executes millions of serializable transactions/second by exploiting multi-version timestamp order concurrency control.  However, it has not supported range operations, only individual record operations (e.g., create, read, update, delete).  In this paper, we enhance our multi-version timestamp order technique to handle range concurrency and prevent phantoms. Importantly, we maintain high performance while respecting the clean separation of duties required by Deuteronomy, where a transaction component performs purely logical concurrency control (including range support), while a data component performs data storage and management duties. Our range technique continues to manage concurrency information in a latch-free manner.  With our range enhancement, Deuteronomy can reach scan speeds of nearly 250 million records/s (more than 27 GB/s) on modern hardware, while providing serializable isolation complete with phantom prevention.Speaker BioDavid Lomet founded and  manages the Database Group at Microsoft Research Redmond. Earlier, he worked at DEC, IBM, and as professor at Wang  Institute. He has a PhD from University of Pennsylvania.  David has worked in architecture, languages, and distributed systems. His primary focus is database systems. He is an inventor of transactions while on sabbatical at University of Newcastle-on- Tyne in 1975.  He has  authored 100+ papers, including two SIGMOD best papers, and he holds over 50 patents. He and his group have made multiple contributions to Microsoft products. His recent Bw-tree is used  in SQL Server's Hekaton main-memory dbms (http://research.microsoft.com/en-us/news/features/hekaton-122012.aspx).  David has been ICDE PC co-chair and conference co-chair, TC on Data Engineering chair, ICDE Steering Committee member, and member of the Computer Society Board of Governors. David was awarded IEEE Meritorious Service and SIGMOD Contributions Awards  for serving as editor in chief  of the  IEEE Data Engineering Bulletin for 20+ years (http://tab.computer.org/tcde/bull_about.html). He has served as VLDB PC co-chair, and on the VLDB Board, and been an editor of ACM TODS and VLDB Journal. He is a Fellow of the IEEE (and Golden  Core Member), ACM, and AAAS.Sep 8, 2015SPEAKER: Themis Palpanas (Paris Descartes University)Data Series Management: The Road to Big Sequence Analytics
             DetailsDate and TimeSep 8, 2015 NoonLocationDBH 3011SpeakerThemis Palpanas (Paris Descartes University)TitleData Series Management: The Road to Big Sequence Analytics
             AbstractThere is an increasingly pressing need, by several applications in
             diverse domains, for developing techniques able to index and mine very
             large collections of sequences, or data series. Examples of such
             applications come from the Inernet of Things, biology, astronomy,
             entomology, the web, and other domains. It is not unusual for these
             applications to involve numbers of data series in the order of hundreds
             of millions to billions, which are often times not analyzed in their
             full detail due to their sheer size.
             In this talk, we describe recent efforts in designing techniques for
             indexing and mining truly massive collections of data series that will
             enable scientists to easily analyze their data. We show that the main
             bottleneck in mining such massive datasets is the time taken to build
             the index, and we thus introduce solutions to this problem. Furthermore,
             we discuss novel techniques that adaptively create data series indexes,
             allowing users to correctly answer queries before the indexing task is
             finished. We also show how our methods allow mining on datasets that
             would otherwise be completely untenable, including the first published
             experiments using one billion data series.
             Finally, we present our vision for the future in big sequence management
             research.Speaker BioThemis Palpanas is a professor of computer science at the Paris
             Descartes University, France. He received the BS degree from the
             National Technical University of Athens, Greece, and the MSc and
             PhD degrees from the University of Toronto, Canada. He has
             previously held positions at the University of Trento and the IBM
             T.J. Watson Research Center. He has also been a Visiting Professor
             at the National University of Singapore, worked for the University
             of California, Riverside, and visited Microsoft Research and the
             IBM Almaden Research Center. His research solutions have been
             implemented in world-leading commercial data management products
             and he is the author of eight US patents. He is the recipient of
             three Best Paper awards (including ICDE and PERCOM), and the IBM
             Shared University Research (SUR) Award in 2012, which represents
             a recognition of research excellence at worldwide level. He has been
             a member of the IBM Academy of Technology Study on Event Processing,
             and is a founding member of the Event Processing Technical Society.
             He has served as General Chair for VLDB 2013, the top international
             conference on databases.August 20, 2015SPEAKER: Yingyi Bu On Software Infrastructure for Scalable Graph Analytics DetailsDate and TimeAugust 20, 2015 1PMLocationDBH 3011 (TBD)SpeakerYingyi Bu TitleOn Software Infrastructure for Scalable Graph Analytics AbstractRecently, there is a growing need for distributed graph processing systems that are capable of gracefully scaling to very large datasets.  In the meantime,  in real-world applications, it is highly desirable to reduce the tedious, inefficient ETL (extraction, transformation, and loading) gap between tabular data processing systems and graph processing systems. Unfortunately, those challenges have not been easily met due to the intense memory pressure imposed by process-centric, message passing designs that many graph processing systems follow, as well as the separation of tabular data processing runtimes and graph processing runtimes.
             
             In this thesis, we first propose a bloat-aware design paradigm towards the development of efficient and scalable Big Data applications in object-oriented, GC enabled languages and demonstrate that programming under this paradigm does not incur significant programming burden but obtains remarkable performance gains (e.g., 2x). Based on the design paradigm, we then build Pregelix, an open source distributed graph processing system which is based on an iterative dataflow design that is better tuned to handle both in-memory and out-of-core workloads. As such, Pregelix offers improved performance characteristics and scaling properties over current open source systems (e.g., we have seen up to 15x speedup compared to Apache Giraph and up to 35x speedup compared to distributed GraphLab). Finally, we integrate Pregelix with the open source Big Data management system AsterixDB to offer users a mix of a vertex-oriented programming model and a declarative query language for richer forms of Big Graph analytics with reduced ETL pains.Speaker Bio
                Yingyi Bu is a PhD Candidate from University of California, Irvine
             Jun 5, 2015SPEAKER: Michael Carey (UCI)DetailsDate and TimeJun 5, 2015 3 pmLocationDBH 3011SpeakerMichael Carey (UCI)TitleAbstractSpeaker BioMay 29, 2015 (Special Time\Place)SPEAKER: Michael Franklin, UC Berkeley Computer ScienceBig Data, Data Science, and other Buzzwords that Really MatterDetailsDate and TimeMay 29, 2015 (Special Time\Place) 11 amLocationDBH 6011SpeakerMichael Franklin, UC Berkeley Computer ScienceTitleBig Data, Data Science, and other Buzzwords that Really MatterAbstractData is all the rage across industry and across campuses.  While it may be temping to dismiss the buzz as just another spin of the hype cycle, there are substantial shifts and realignments underway that are fundamentally changing how Computer Science, Statistics and virtually all subject areas will be taught, researched, and perceived as  disciplines.  In this talk I will give my personal perspectives on this new landscape based on experiences organizing a large, industry-engaged academic Computer Science research project (the AMPLab), in helping to establish a campus-wide Data Science research initiative (the Berkeley Institute for Data Science), and my participation on a campus task force charged with mapping out Data Science Education for all undergraduates at Berkeley.Speaker BioMichael Franklin is the Thomas M. Siebel Professor of Computer Science and Chair of the Computer Science Division of the EECS Department at UC Berkeley.   He is director of the Berkeley AMPLab, a 70+ person effort fusing scalable computing, machine learning, and human computation to make sense of data at scale.   AMPLab software including: Spark, Shark, and Mesos, plays a significant role in the emerging Big Data ecosystem. The lab is funded by an NSF CISE Expeditions Award, the Darpa XData program, and 26 companies including founding sponsors Amazon Web Services, Google, and SAP.May 15, 2015 (special place)SPEAKER: Yunyao Li (IBM Research - Almaden)SystemT: an Algebraic Approach to Declarative Information Extraction DetailsDate and TimeMay 15, 2015 (special place) 3 pmLocationDBH 4011SpeakerYunyao Li (IBM Research - Almaden)TitleSystemT: an Algebraic Approach to Declarative Information Extraction AbstractIn recent years, Information Extraction (IE) has become increasingly
             important to a wide array of enterprise applications, ranging from
             Business Intelligence and Semantic Search to Data-as-a-Service. Such
             applications drive four main requirements for IE systems: accuracy,
             embeddability, scalability, transparency, and usability. In this talk
             I will give an overview of SystemT, a declarative IE system designed
             to address these requirements. SystemT ships today with multiple
             products across 4 IBM Software Brands and is used in multiple ongoing
             research projects.
             
             SystemT is based on the basic principle underlying relational database
             technology: complete separation of specification from execution.
             SystemT uses a declarative rule language, AQL, and an optimizer that
             generates high-performance algebraic execution plans for AQL rules. We
             show that SystemT removes the expressivity and performance limitations
             of previous state-of-art rule-based systems based on cascading
             grammars, delivering comparable result quality, and an order of
             magnitude higher annotation throughput with much lower memory
             footprint. We also present the ongoing research and development
             efforts in making SystemT more usable for both technical and business
             users.
             
             If time permits, I will also briefly cover IBM technologies built
             based on similar principle to enable large-scale data analytics.
             Speaker BioYunyao Li is a Master Inventor, Research Staff Member and Research
             Manager with IBM Almaden Research Center, where she manages the
             Scalable Natural Language Processing group. Her expertise is in the
             interdisciplinary areas of databases, natural language processing,
             human-computer interaction, and information retrieval. She has
             published over 30 peer-reviewed, referred articles, and filed nearly
             20 patents in these areas.
             
             Yunyao is particularly interested in designing, developing, and
             analyzing large scale systems that are usable by a wide spectrum of
             users. Towards this direction, her current focus is on text analytics.
             She is a founding member of SystemT, a state-of-the-art information
             extraction engine, and Gumshoe, a novel enterprise search engine that
             has been powering IBM intranet and ibm.com search since 2010. Her
             contributions in these projects have recognized by multiple
             prestigious IBM internal awards.
             
             She received her PhD degree in Computer Science and Engineering from
             the University of Michigan, Ann Arbor in 2007. Before that, she
             obtained dual-degrees of M.S.E in Computer Science & Engineering and
             M.S in Information from Computer Science and Engineering and School of
             Information respectively at the University of Michigan - Ann Arbor.
             She went to college at Tsinghua University, Beijing, China, and
             graduated with dual-degrees of B.E in Automation and B.S in Economics.May 8, 2015 (Informatics Seminar)SPEAKER: Sean Young (UCLA)Reading between the Tweets: Using Social Technology Data to Predict Real-World OutcomesDetailsDate and TimeMay 8, 2015 (Informatics Seminar) 3 pmLocationDBH 6011SpeakerSean Young (UCLA)TitleReading between the Tweets: Using Social Technology Data to Predict Real-World OutcomesAbstractSocial big data from technologies like social media, online search, and mobile apps are being used to better understand and predict real-world events and outcomes. For example, researchers in the health and medical space have been studying whether social data could be used to monitor and predict outbreaks of disease, susceptibility to disease, and patient response to treatment.
             
             The University of California Institute for Prediction Technology (UCIPT) was created to address questions like these. Funded by the UC Office of the President, the Institute brings together researchers across University of California campuses as well as community stakeholders to study whether and how social technology data can be used to predict events in areas like health/medicine, politics, and security. This talk with provide an overview of the field of Prediction Technology as well as research being addressed by the Institute, such as 1) how Twitter is being used to predict HIV risk behaviors and outbreaks, 2) how google search terms are being used to predict political events, and 3) social media users' perceptions of having their data used for research.Speaker BioSean Young, PhD, MS is the Director of the University of California Institute for Prediction Technology (UCIPT), the UCLA Center for Digital Behavior, and an Assistant Professor of Family Medicine. As the Director of UCIPT, Dr. Young seeks to bridge researchers across University of California campuses to study how data from social technologies like social media and wearable devices can be used to predict real-world events in areas like health/medicine, politics, and security. His research at the Center for Digital Behavior is focused on use of social media and mobile health technologies to change and predict behavior. He has studied how social media can address issues related to HIV and drug use prevention and treatment behavior in the U.S., Peru, and South Africa, and Iran, and among various at-risk populations around the world. He was the Primary Investigator of the Harnessing Online Peer Education (HOPE) UCLA, HOPE Peru, and HOPE Care studies, showing the effectiveness of!
               using social media to increase HIV testing. His team completed the first study to create methods of using observational big data (> 150 million tweets) from social media for drug and HIV-related surveillance. He teaches a course for UCLA undergraduates called Hacking Global Health on how to use social media and mobile technologies to quickly address global health needs.May 1, 2015SPEAKER: C. Mohan (IBM Research - Almaden)Modern Database Systems: Modernized Classic Systems, NewSQL and NoSQLDetailsDate and TimeMay 1, 2015 3 pmLocationDBH 3011SpeakerC. Mohan (IBM Research - Almaden)TitleModern Database Systems: Modernized Classic Systems, NewSQL and NoSQLAbstractThe last many years have seen the emergence of a new class of database systems which I am calling Modern Database Systems (MDS). They are classified broadly as NoSQL and NewSQL systems. Apart from the development of such systems, traditional DBMS vendors have also extended their decades old systems like DB2, Informix, Oracle and SQL Server with major architectural enhancements that address modern requirements and take advantage of hardware (processor, memory and storage) advances. This talk will do a broad brush analysis of MDS. It is targeted at a broad set of database systems and applications people. It is intended to let the audience better appreciate what is really behind the covers of many of these systems, going beyond the hype associated with these open source, commercial and research systems. The capabilities and limitations of such systems will be addressed. Speaker BioMay 1, 2015 (Special Time\Place)SPEAKER: C. Mohan (IBM Research - Almaden)Big Data: Hype and RealityDetailsDate and TimeMay 1, 2015 (Special Time\Place) 11 amLocationDBH 6011SpeakerC. Mohan (IBM Research - Almaden)TitleBig Data: Hype and RealityAbstractBig Data has become a hot topic in the last few years in both industry and the research community. For the most part, these developments were initially triggered by the requirements of Web 2.0 companies. Both technical and non-technical issues have continued to fuel the rapid pace of developments in the Big Data space. Open source and non-traditional software entities have played key roles in the latter. As it always happens with any emerging technology, there is a fair amount of hype that accompanies the work being done in the name of Big Data. The set of clear-cut distinctions that were made initially between Big Data systems and traditional database management systems are being blurred as the needs of the broader set  of (“real world”) users and developers have come into sharper focus in the last couple of years. In this talk, I will survey the developments in Big Data and try to distill reality from the hype!  Speaker BioApr. 24, 2015SPEAKER: Dimitrios GunopoulousAnalysing spatiotemporal document collectionsDetailsDate and TimeApr. 24, 2015 3 pmLocationDBH 3011SpeakerDimitrios GunopoulousTitleAnalysing spatiotemporal document collectionsAbstractWe consider the problem of searching in live, time-stamped
             and geolocated document collections. As the number and size of such
             data collections increase, the problem of efficiently indexing and
             searching becomes more important. We present novel approaches
             for keyword search and for event discovery in this space. We also
             describe applications of such techniques in the emergency response
             domain.Speaker BioDimitrios Gunopulos is a Professor in the Department of Informatics and
             Telecommunications, University of Athens. He got his PhD from Princeton
             University in 1995. He was a Postoctoral Fellow at the Max-Planck-Institut
             for Informatics, a Research Associate at the IBM Almaden Research Center,
             and an Assistant, Associate, and Full Professor at the Department of Computer
             Science and Engineering in the University of California Riverside. His
             research is in the areas of Data Mining, Knowledge Discovery in Databases,
             Data Science, Databases, Sensor Networks, Peer-to-Peer systems, and Algorithms.
             He has co-authored over a hundred journal and conference papers that have
             been widely cited. His research has been supported by NSF (including an
             NSF CAREER award), the DoD, the Institute of Museum and Library Services,
             the Tobacco Related Disease Research Program, the European Commission,
             the General Secretariat of Research and Technology, AT&T, and Nokia.
             He has served as a General co-Chair in IEEE ICDM 2010, as a PC co-Chair
             in ECML/PKDD 2011, in IEEE ICDM 2008, in ACM SIGKDD 2006, in SSDBM 2003,
             and in DMKD 2000.Apr. 17, 2015SPEAKER: Xifeng Yan (UCSB)Schemaless Graph Querying DetailsDate and TimeApr. 17, 2015 3 pmLocationDBH 3011SpeakerXifeng Yan (UCSB)TitleSchemaless Graph Querying AbstractQuerying complex graph databases such as knowledge graphs and social
              networks is a challenging task for non-expert users. Due to their complex
              schemas and variational information descriptions, it becomes very hard for
              users to formulate a query that can be properly processed by the existing
              systems.  We argue that for a user-friendly graph query engine, it must
              support various kinds of transformations such as synonym, abbreviation, and
              ontology. Furthermore, the derived query results must be ranked in a
              principled manner.
         In this talk, we will review our recent efforts that address the usability
         and scalability issues arising in querying large, heterogeneous graph data
         from three perspectives: query formulation, query execution and result
         presentation.  Our goal is to build an integrated query infrastructure that
         mitigates the complexity of accessing graph data. It provides simple and
         intuitive query interfaces, including a schema-less query engine for
         keyword, graph pattern, and natural language queries. It also helps users
         digest query output quickly through query result summarization and
         continuously refines query results online based on user feedback. We will
         conclude the talk with new directions in managing and accessing graph data.
              Speaker Bio
             Xifeng Yan is an associate professor at the University of California, Santa
             Barbara. He holds the Venkatesh Narayanamurti Chair of Computer Science. He
             has been working on modeling, managing, and mining graphs in information
             networks, computer systems, social media and bioinformatics. He received NSF
             CAREER Award, IBM Invention Achievement Award, ACM-SIGMOD Dissertation
             Runner-Up Award, and IEEE ICDM 10-year Highest Impact Paper Award. He
             received his Ph.D. from the University of Illinois at Urbana-Champaign in
             2006 and was a research staff member at the IBM T. J. Watson Research Center
             between 2006 and 2008.
             Apr. 10, 2015SPEAKER: Liuba Shrira (Brandeis University)Modular and efficient past state protocols for transactional systemsDetailsDate and TimeApr. 10, 2015 3 pmLocationDBH 3011SpeakerLiuba Shrira (Brandeis University)TitleModular and efficient past state protocols for transactional systemsAbstractThe remarkable drop in storage costs makes it possible and attractive to capture past application states 
             and store them for a long time.
             This opens the possibility that kinds of demanding analysis like forecasting, formerly dependent on
             data warehouses and temporal databases, can become available to everyday applications in off-the-shelf data stores. The challenge is how to organize past states so that they are ``not in the way'' and ``always there'' when needed.
             
             Our  approach, called Retro, integrates a low-level consistent snapshot system into a data store
             storage manager, allowing to run unmodified data store programs against the snapshots, 
             side by side with programs running against the current state. The approach is attractive for several reasons.
             An application can take snapshots efficiently with any frequency, keep them indefinitely,
             or garbage-collect them at low cost, a useful feature in long-lived systems. 
             A principled methodology derives the snapshot protocols from the native data store storage manager mechanisms, allowing to implement the snapshot system in a modular way,
             without extensive modifications to the data store internals, making the approach suitable in off-the-shelf data 
             stores.
             The talk will describe the new techniques that underly Retro
             and present preliminary performance results from a prototype we built in Berkeley DB, indicating Retro
             is efficient, imposing moderate performance penalty on the native data store, on expected common workloads. Speaker BioLiuba Shrira is a Professor in the Computer Science Department at Brandeis University, 
             and is affiliated with the Computer Science and Artificial Intelligence Laboratory at MIT. 
             She received her Ph.D. from Technion, Israeli Institute of Technology, and
             has been affiliated with Microsoft Research, Cambridge, UK, Microsoft Research Asia, Beijing, and Computer Science Department, in the Technion, Haifa.
             Her research interests span aspects of design and implementation of distributed systems and especially storage systems. 
             This includes fault-tolerance, availability and performance issues. Her recent focus is on long-lived transactional  storage, time travel (in storage), software upgrades, and support for collaborative access to long-lived objects.Apr. 3, 2015 (Special time/place)SPEAKER: Vijay Chidambaram (University of Wisconsin - Madison)Performance and Reliability in Modern Storage SystemsDetailsDate and TimeApr. 3, 2015 (Special time/place) 1:30 pmLocationDBH 4011SpeakerVijay Chidambaram (University of Wisconsin - Madison)TitlePerformance and Reliability in Modern Storage SystemsAbstractStorage services form the platform on which widely-used cloud services, mobile applications, data analytics
             engines, and transactional databases are built. Such services are trusted with
             irreplaceable personal and commercial information by users, companies, and even governments.
             The designers of storage services often have to choose between performance and reliability. If the developer
             makes the system reliable, performance is often significantly reduced. If the developer instead maximizes
             performance, a crash could lead to data loss and corruption.
             In this talk, I describe how to build systems that achieve both strong reliability and high performance. In many
             systems, reliability is maintained by carefully ordering updates to storage. The key insight is that the low-level
             mechanism used to enforce ordering is overloaded: it provides durability as well as ordering. I introduce a new
             primitive, osync(), that decouples ordering from durability of writes. I present Optimistic Crash Consistency,
             a new crash-recovery protocol that builds on osync() to provide strong reliability guarantees and high
             performance. I implement these techniques in the Optimistic File System (OptFS) and show that it provides
             10X increased performance for some workloads. With researchers in Microsoft, I employ the principles of
             Optimistic Crash Consistency in a distributed storage system, resulting in 2-5X performance improvements.Speaker BioVijay Chidambaram is a Ph.D candidate in the Department of Computer Sciences at the University of
             Wisconsin-Madison. His current research focus is to ensure the reliability of applications in the rapidly
             changing landscape of storage and cloud computing. Specifically, he has contributed new reliability techniques
             in (local and distributed) storage systems, and built frameworks for finding reliability bugs in applications. His
             work has resulted in patent applications by Samsung and Microsoft. He was awarded the Microsoft Research
             Fellowship in 2014, and the University of Wisconsin-Madison Alumni Scholarship in 2009.Mar. 13, 2015SPEAKER: Lada Adamic, David Kempe, Mark Handcock, Carter ButtsNetworks, Algorithms, Statistics and Social Science (Data Science Initiative Sponsored Event)DetailsDate and TimeMar. 13, 2015 10am - 5pmLocationCalit2 AuditoriumSpeakerLada Adamic, David Kempe, Mark Handcock, Carter ButtsTitleNetworks, Algorithms, Statistics and Social Science (Data Science Initiative Sponsored Event)AbstractSee http://datascience.uci.edu/event-registration/?ee=14 for more detailsSpeaker BioMar. 6, 2015SPEAKER: Raman Grover (AsterixDB)Scalable Fault-Tolerant Elastic Data Ingestion in AsterixDBDetailsDate and TimeMar. 6, 2015 3 pmLocationDBH 3011SpeakerRaman Grover (AsterixDB)TitleScalable Fault-Tolerant Elastic Data Ingestion in AsterixDBAbstractIn this dissertation, we develop the support for continuous data ingestion in AsterixDB, an open-source Big Data Management System (BDMS) that provides a platform for storage and analysis of large volumes of semi-structured data. Data feeds are a mechanism for having continuous data arrive into a BDMS from external sources and incrementally populate a persisted dataset and associated indexes. The need to persist and index "fast-flowing'' high-velocity data (and support ad hoc analytical queries) is ubiquitous. However, the state of the art today involves 'gluing' together different systems. AsterixDB is different in being a unified system with "native support'' for data  ingestion.
             
             We discuss the challenges and present the design and implementation of the concepts involved in modeling and managing data feeds in AsterixDB. AsterixDB allows the runtime behavior, allocation of resources, and the offered degree of robustness to be customized (by associating an ingestion policy) to suit the application(s) that wish to consume the ingested data. Results from  experiments that evaluate the scalability and fault-tolerance of the AsterixDB data feeds facility are reported. We include an evaluation of the built-in ingestion policies and study their effect as well on throughput and latency. An evaluation and comparison with a `glued' together system formed from popular engines - Storm (for streaming) and MongoDB (for persistence) - is also included.Speaker BioFeb. 27, 2015SPEAKER: Fatma Ozcan (IBM Research - Almaden)SQL Comes Back, But This is not Your Father's DBMS!DetailsDate and TimeFeb. 27, 2015 3 pmLocationDBH 3011SpeakerFatma Ozcan (IBM Research - Almaden)TitleSQL Comes Back, But This is not Your Father's DBMS!AbstractRecent years have seen the resurgence of SQL; this time in the context of Big Data Platforms. SQL-on-Hadoop
             was one of the hottest topics in 2014, with many newcomers, and adaptations of old systems. There
             are significant differences between the new incarnation of SQL systems, and the traditional enterprise
             warehouses. First, semi-structured data is inherent in the Hadoop data as the data frequently
             comes from noSQL and web sources. Hence JSON data and complex types are more important than in 
             traditional systems. Second, SQL is only one of the steps in the bigger analytical flows. This requires
             SQL to interact with other frameworks, including streaming, ETL, as well as advanced analytics and 
             machine learning. Finally, we see more user-defined function in big data platforms, because
             a lot of business logic needs to be executed closer to the data. 
              
              In this talk, I will first describe IBM Big SQL, an SQL-on-Hadoop offering that works on all Hadoop
              data formats. I will describe how we adapted IBM database technology to this new world. In the second
              part of the talk, I will describe a couple of research projects in IBM Almaden that focus on new aspects
              of the Hadoop SQL engines and the big data eco-system.Speaker BioFatma Özcan is a Research Staff Member and a manager at IBM Almaden
             Research Center. Her current research focuses on platforms and infra-structure for large-scale
             data analysis, Hadoop and database integration, and query optimization for semi-structured data. 
             Dr Özcan got her PhD degree in computer science from University of Maryland, College Park. She has
             over 10 years of experience in semi-structured and structured data management, query processing and
             optimization, and has delivered core technologies into IBM DB2 and BigInsights products. 
             She is the co-author of the book "Heterogeneous Agent Systems", and 
             co-author of several conference papers and patents. 
             She has chaired program committees for various conferences, and 
             served on NSF (National Science Foundation) panels. She is a member of the ACM.Feb. 20, 2015SPEAKER: Karthik Ramasamy (Twitter) Real Time Analytics@Twitter DetailsDate and TimeFeb. 20, 2015 3 pmLocationDBH 3011SpeakerKarthik Ramasamy (Twitter)Title Real Time Analytics@Twitter AbstractReal time analytics seems to be a buzz word these days. Twitter identified the need for real time analytics early on and invested in a massive data pipeline that collects, aggregates, processes large volumes of data in real time. At the heart of the pipeline is Twitter Storm, a real-time stream processing engine widely used in Twitter. Storm is used for real-time data analytics, time series aggregation, and powering real-time features like trending topics. In this talk, we will give an overview of real time analytics, discuss the twitter real time data pipeline and how Storm is used for extracting analytics. We will also discuss the challenges we faced and lessons we have learned while building this infrastructure at Twitter. Speaker BioKarthik is the engineering manager and technical lead for Real Time Analytics at Twitter. He has two decades of experience working in parallel databases, big data infrastructure and networking. He cofounded Locomatix, a company that specializes in real timestreaming processing on Hadoop and Cassandra using SQL that was acquired by Twitter. Before Locomatix, he had a brief stint with Greenplum where he worked on parallel query scheduling. Greenplum was eventually acquired by EMC for more than $300M. Prior to Greenplum, Karthik was at Juniper Networks where he designed and delivered platforms, protocols, databases and high availability solutions for network routers that are widely deployed in the Internet. Before joining Juniper at University of Wisconsin, he worked extensively in parallel database systems, query processing, scale out technologies, storage engine and online analytical systems. Several of these research were spun as a company later acquired by Teradata.
             
             He is the author of several publications, patents and one of the best selling book "Network Routing: Algorithms, Protocols and Architectures." He has a Ph.D. in Computer Science from UW Madison with a focus on databases.Feb. 13, 2015SPEAKER: Yannis Papakonstantinou (UCSD)The SQL++ Query Language: Support for native JSON, while backwards-compatible with SQLDetailsDate and TimeFeb. 13, 2015 3 pmLocationDBH 3011SpeakerYannis Papakonstantinou (UCSD)TitleThe SQL++ Query Language: Support for native JSON, while backwards-compatible with SQLAbstractSQL-on-Hadoop, NewSQL and NoSQL databases provide semi-structured data models (typically JSON-based). They now drive towards declarative, SQL-alike query languages. However, their idiomatic, non-SQL language constructs, the many variations and the lack of formal syntax and semantics pose problems. Notably, database vendors end up with unclear semantics and complicated implementations, as they add one feature at-a-time.
             
             The presented SQL++ semi-structured data model bridges JSON and the SQL data model. The SQL++ query language is backwards compatible with SQL, while supporting native JSON. SQL++ includes configuration options that describe different options of language semantics and formally capture the variations of existing database languages. SQL++ is unifying: By appropriate choices of configuration options, the SQL++ semantics can morph into the semantics of any of eleven popular semistructured databases, which we surveyed, as the experimental validation shows. In this way, SQL++ allows a formal characterization of the capabilities of the emerging query languages.
             
             We briefly discuss the key role of SQL++ and SQL++ Incremental View Maintenance in the FORWARD application and visualization development platform. SQL++ also is the query language of the FORWARD middleware query processor. We briefly discuss issues and opportunities in federated queries over SQL and non-SQL database. Speaker BioYannis Papakonstantinou is a Professor of Computer Science and Engineering at the University of California, San Diego. His research is in the intersection of data management technologies and the web, where he has published over ninety research articles and received over 10,000 citations. He has given multiple tutorials and invited talks, has served on journal editorial boards and has chaired and participated in program committees for many international conferences and workshops. He also teaches for UCSD's Master of Advanced Studies in Data Science.
             
             Yannis enjoys to commercialize his research and to inform his research accordingly. He was the CEO and Chief Scientist of Enosys Software, which built and commercialized an early Enterprise Information Integration platform for structured and semistructured data, which became part of BEA's Aqualogic. His lab's recent FORWARD platform is in use by UCSD and commercial applications. He is in the technical advisory board of Brightscope Inc and GraphSQL Inc. Feb. 6, 2015SPEAKER: Ansgar ScherpExtraction and Analyses of Schema Information on the Linked Open Data CloudDetailsDate and TimeFeb. 6, 2015 3 pmLocationDBH 3011SpeakerAnsgar ScherpTitleExtraction and Analyses of Schema Information on the Linked Open Data CloudAbstractThe Linked Open Data (LOD) cloud interlinks information about entities from different data sources and across various domains using the Resource Description Framework (RDF). In contrast to traditional relational databases, the LOD cloud does not provide a fixed, pre-defined schema. Rather, RDF allows for flexibly modeling the data schema by attaching RDF types to the entities and by using domain-specific RDF properties to describe the entities. The talk presents recent developments on the extraction and analysis of schema information from the LOD cloud. For example, with SchemEX, we have developed an efficient approach and tool for a stream-based extraction and indexing of schema information from Linked Open Data (LOD) at web-scale. The schema index provided by SchemEX can be used to locate distributed data sources in the LOD cloud. The SchemEX approach is used in LODatio, a Google-inspired search engine designed for data engineers to find relevant sources of LOD. Further analysis of schema structures on the LOD cloud include investigating the redundancy between type and property information, use of vocabularies in pay-level domains, and change of schema information over weekly snapshots of a larger amount of LOD data. The talk will conclude with current developments and future work.Speaker BioAnsgar Scherp is a professor at the Leibniz Information Center for Economics and Kiel University, Kiel, GermanyJan. 30, 2015SPEAKER: Chris Jermaine (Rice University)Large-Scale Machine Learning with the SimSQL SystemDetailsDate and TimeJan. 30, 2015 3 pmLocationDBH 3011SpeakerChris Jermaine (Rice University)TitleLarge-Scale Machine Learning with the SimSQL SystemAbstractIn this talk, I'll describe the SimSQL system, which is a platform for writing and executing statistical codes over large data sets, particularly for machine learning applications. Codes that run on SimSQL can be written in a very high-level, declarative language called Buds. A Buds program looks a lot like a mathematical specification of an algorithm, and statistical codes written in Buds are often just a few lines long.
             
             At its heart, SimSQL is really a relational database system, and like other relational systems, SimSQL is designed to support data independence. That is, a single declarative code for a particular statistical inference problem can be used regardless of data set size, compute hardware, and physical data storage and distribution across machines. One concern is that a platform supporting data independence will not perform well. But we've done extensive experimentation, and have found that SimSQL performs as well as other competitive platforms that support writing and executing machine learning codes for large data sets.Speaker BioChris Jermaine is an associate professor of computer science at Rice University. He is the recipient of an Alfred P. Sloan Foundation Research Fellowship, a National Science Foundation CAREER award, and an ACM SIGMOD Best Paper Award. In his spare time, Chris enjoys outdoor activities such as hiking, climbing, and whitewater boating. In one particular exploit, Chris and his wife floated a whitewater raft (home-made from scratch using a sewing machine, glue, and plastic) over 100 miles down the Nizina River (and beyond) in Alaska.Jan. 16, 2015SPEAKER: Ryan Compton (Howard Hughes Research Laboratories)Geotagging One Hundred Million Twitter Accounts with Total Variation
Minimization DetailsDate and TimeJan. 16, 2015 3 pmLocationDBH 3011SpeakerRyan Compton (Howard Hughes Research Laboratories)TitleGeotagging One Hundred Million Twitter Accounts with Total Variation
Minimization Abstract Geographically annotated social media is extremely valuable for modern
information retrieval. However, when researchers can only access
publicly-visible data, one quickly finds that social media users
rarely publish location information. In this work, we provide a method
which can geolocate the overwhelming majority of active Twitter users,
independent of their location sharing preferences, using only
publicly-visible Twitter data.

Our method infers an unknown user's location by examining their
friend's locations. We frame the geotagging problem as an optimization
over a social network with a total variation-based objective and
provide a scalable and distributed algorithm for its solution.
Furthermore, we show how a robust estimate of the geographic
dispersion of each user's ego network can be used as a per-user
accuracy measure, allowing us to discard poor location inferences and
control the overall error of our approach.

Leave-many-out evaluation shows that our method is able to infer
location for 101,846,236 Twitter users at a median error of 6.38 km,
allowing us to geotag over 80% of public tweets.

http://arxiv.org/abs/1404.7152Speaker Bio Ryan Compton is postdoc in the Information and System Sciences
Laboratory at Howard Hughes Research Laboratories in Malibu, CA. His
work focuses on social media data mining for early detection of
newsworthy events. In 2012 Ryan finished a mathematics PhD at UCLA with
a thesis on sparsity promoting optimization for quantum mechanical
signal processing. His website is http://www.ryancompton.net.Dec. 12, 2014SPEAKER: Heri Ramampia (Norwegian University of Science and Technology)Boosting Event-Related Image Retrieval with Spatiotemporal Distribution of Tag Terms DetailsDate and TimeDec. 12, 2014 3 pmLocationDBH 3011SpeakerHeri Ramampia (Norwegian University of Science and Technology)TitleBoosting Event-Related Image Retrieval with Spatiotemporal Distribution of Tag Terms AbstractMedia sharing applications, such as Flickr and Panoramio, contain a large amount of pictures 
related to real life events.  For this reason,  although still being a challenging task, the development of 
effective methods to retrieve these pictures is important. Recognizing this importance, and to improve 
the retrieval effectiveness of tag-based event retrieval systems, we have proposed a new 
effective method to extract a set of geographical tag features from raw geo-spatial profiles of user tags.  
The main idea is to use these features to select the best expansion terms in a machine learning-based
query expansion approach. Specifically, we apply rigorous statistical exploratory analysis of spatial 
point patterns to extract the geo-spatial features. Then, we used the features both to summarize the spatial 
characteristics of the spatial distribution of a single term, and to determine the similarity between 
the spatial profiles of two terms -- i.e., term-to-term spatial similarity. To further improve our image retrieval 
approach, we investigated the effect of combining our geo-spatial features with temporal features. 
In this presentation, I will try to give an overview of the methods we used
(1) to extract the spatio-temporal featrues from image tags, and (2) how to use these features to
improve the retrieval performance, focusing on retrieval of event-related images. Finally, I will
discuss the results from our experiments, and show how our method has improved the
state-of-the-art approach.Speaker BioHeri Ramampiaro is an Associate Professor at the Dept of Computer and Information Science,
Norwegian University of Science and Technology (NTNU). He is Head of the Data and Information 
Management group. His main research interests include Information Retrieval, BigData,
Information Extraction/Text Mining, Bioinformatics and Health Informatics. Ramampiaro is currently
on a one-year research sabbatical, visiting the ISG group, UC Irvine.Dec. 5, 2014SPEAKER: Daniel Wood (DELL)Dell and Big Data Software: Data replication and ReorganizationDetailsDate and TimeDec. 5, 2014 3 pmLocationDBH 3011SpeakerDaniel Wood (DELL)TitleDell and Big Data Software: Data replication and ReorganizationAbstractThe role of an enterprise independent software vendor (ISV) is to develop tools and technologies that support the information systems of their customers. Customers cannot easily switch technologies or adapt them to their needs without causing disruptions to their business. This is where software vendors are able to help. I will discuss how an ISV like Quest managed to go from startup to acquisition and the details of two of the relevant enterprise technologies. The first technology, database replication, has become critical in scaling many of the household names we know. We will explore this technology and its evolution from homogeneous database replication to heterogeneous database replication that includes Oracle, SQL Server, and Hadoop systems. The second technology, data reorganization, is crucial to curbing storage hungry databases and improving database performance by defragmenting data. Speaker BioDaniel Wood is a manager of software development inside Dell Software Group (Formerly Quest Software) where he has worked for 13 years. Daniel focuses on database management tools and problems at scale. Daniel holds a BA in physics from University of California Santa Barbara. Nov. 21, 2014 (Special Time/Place)SPEAKER: Prof. Wei Wang (UCLA)Big Data Analytics in ScienceDetailsDate and TimeNov. 21, 2014 (Special Time/Place) 11 amLocationDBH 6011SpeakerProf. Wei Wang (UCLA)TitleBig Data Analytics in ScienceAbstractBig data analytics is the process of examining large amounts of data of a variety of types (big data) to uncover hidden patterns, unknown correlations and other useful information. Its revolutionary potential is now universally recognized. Data complexity, heterogeneity, scale, and timeliness make data analysis a clear bottleneck in many biomedical applications, due to the complexity of the patterns and lack of scalability of the underlying algorithms. Advanced machine learning and data mining algorithms are being developed to address one or more challenges listed above. It is typical that the complexity of potential patterns may grow exponentially with respect to the data complexity, and so is the size of the pattern space. To avoid an exhaustive search through the pattern space, machine learning and data mining algorithms usually employ a greedy approach to search for a local optimum in the solution space, or use a branch-and-bound approach to seek optimal solutions, and consequently, are often implemented as iterative or recursive procedures. To improve efficiency, these algorithms often exploit the dependencies between potential patterns to maximize in-memory computation and/or leverage special hardware for acceleration. In this talk, I will present some open challenges faced by data scientist in biomedical fields and our approaches to tackle these challenges through examples such as multi-locus QTL analysis and transcriptome quantification using RNAseq data.
Speaker BioWei Wang is a professor in the Department of Computer Science at University of California at Los Angeles and the director of the Scalable Analytics Institute (ScAi). She is a member of the UCLA Jonsson Comprehensive Cancer Center. She received her PhD degree in Computer Science from the University of California at Los Angeles in 1999. Before she rejoined UCLA, she was a professor in Computer Science and a member of the Carolina Center for Genomic Sciences and Lineberger Comprehensive Cancer Center at the University of North Carolina at Chapel Hill from 2002 to 2012, and was a research staff member at the IBM T. J. Watson Research Center between 1999 and 2002. Dr. Wang's research interests include big data, data mining, bioinformatics and computational biology, and databases.
 Nov. 18, 2014 (Special Time/Place)SPEAKER: Hwanjo Yu, Associate Professor
          POSTECH (Pohang University of Science and Technology)Search and Mining for Big DataDetailsDate and TimeNov. 18, 2014 (Special Time/Place) 4 pmLocationDBH 3011SpeakerHwanjo Yu, Associate Professor
          POSTECH (Pohang University of Science and Technology)TitleSearch and Mining for Big DataAbstractBig data is recently defined (by Gartner) as high volume, high velocity, and/or high variety information assets that require new forms of processing to enable enhanced decision making, insight discovery and process optimization. In this talk, we first present key challenges in Big data programming, that are distinct from conventional parallel processing. After that, we introduce several research projects dealing with large volume of data in the data mining lab at POSTECH, that are, PubMed relevance feedback search engine, blackbox video search, novel recommendation, and timing when to recommend.Speaker Bio Hwanjo Yu received his PhD in Computer Science at the University of Illinois at Urbana-Champaign at June 2004 under the supervision of Prof. Jiawei Han. From July 2004 to January 2008, he had been an assistant professor at the University of Iowa. He is now an associate professor at POSTECH (Pohang University of Science and Technology). He developed influential algorithms and systems in the areas of data mining, database, and machine learning, including (1) algorithms for classifying without negative examples (PEBL, SVMC), (2) privacy-preserving SVM algorithms, (3) SVM-JAVA : an educational java open source for SVM, (4) RefMed : relevance feedback search engine for PubMed, (5) TurboGraph : a fast parallel graph engine handling billion-scale graphs in a single PC. His methods and algorithms were published in prestigious journals and conferences including ACM SIGMOD, ACM SIGKDD, IEEE ICDE, IEEE ICDM, ACM CIKM, etc., where he is also serving as a program committee.Nov. 14, 2014SPEAKER: Dr. Jiannan Wang (UC Berkeley)SampleClean: Fast and Accurate Query Processing on Dirty DataDetailsDate and TimeNov. 14, 2014 3 pmLocationDBH 3011SpeakerDr. Jiannan Wang (UC Berkeley)TitleSampleClean: Fast and Accurate Query Processing on Dirty DataAbstractThe vision of AMPLab is to integrate Algorithms (Machine Learning), Machines (Cloud Computing) and People (Crowdsourcing) to make sense of Big Data. In the past several years, the lab has developed a variety of open-source software (e.g., Spark and MLBase) to integrate the three resources. For the People part, one of our main focuses is on data cleaning. Real-world data is often “dirty”. Data cleaning is usually a tedious and time-consuming process which requires a lot of human work. In the AMPLab, we have exploited the use of crowdsourcing to reduce the human cost. While crowdsourcing makes data cleaning more scalable, it is still highly inefficient for large datasets. To overcome this limitation, we started the SampleClean project last year. The project aims to investigate how to obtain accurate query results from dirty data, by only cleaning a small sample of the data. We achieved this goal by marrying data cleaning with sampling-based approximate query processing, and addressing many challenging statistical issues. We build a new system that combines our work on crowdsourcing data cleaning and SampleClean query processing. An initial version of the system has shown that our system can help users to obtain very accurate query results on dirty data, at significantly reduced cleaning cost. 
Speaker BioJiannan Wang is a postdoc in the AMPLab at UC Berkeley, where he works with Prof. Michael Franklin and leads the SampleClean project. His research is focusing on developing algorithms and systems for extracting value from “dirty" data. He obtained his PhD from the Computer Science Department at Tsinghua University. During his PhD, he has been a visiting scholar at Chinese University of Hong Kong and UC Berkeley, and an intern at Qatar Computing Research Institute. His PhD research work was supported by Google PhD Fellowship, Boeing Scholarship, and “New PhD Researcher Award” by Chinese Ministry of Education. His PhD dissertation has won the China Computer Federation (CCF) Distinguished Dissertation Award. His similarity-join algorithm has won the first place of EDBT String Similarity Search/Join Competition.   Nov. 7, 2014SPEAKER: Prof. Shahram Ghandeharizadeh (USC)BG:  A Benchmark for Interactive Social Networking ActionsDetailsDate and TimeNov. 7, 2014 3 pmLocationDBH 3011SpeakerProf. Shahram Ghandeharizadeh (USC)TitleBG:  A Benchmark for Interactive Social Networking ActionsAbstractBG is a benchmark for interactive social networking actions (also known as
simple or small data operations).  It is motivated by the flurry of novel data
store designs ranging from SQL to NoSQL and NewSQL, Cache Augmented SQL, graph
databases and others.  More than 40 data stores have been introduced in the past
decade including systems contributed by the social networking sites,
e.g., Cassandra by Facebook and Voldemort by LinkedIn.  Some systems sacrifice
strict ACID (Atomicity, Consistency, Isolation, Durability) properties and opt
for BASE (Basically Available, Soft-state, Eventual Consistency) to enhance
performance.  BG strives to compare these systems with one another quantitatively.

This  presentation details the design of BG and its SoAR metric to rate data stores.
We describe how BG quantifies the amount of unpredictable (stale, erroneous, or inconsistent)
data produced by a data store.  We present ratings from an industrial strength relational
database management system, a document store named MongoDB, a graph data store named
Neo4j, and an extensible data store named HBase.  We show the use of SoAR to evaluate
both vertical and horizontal scalability of MongoDB and HBase.  We also describe
the use of BG to evaluate novel cache replacement algorithms such as CAMP and
consistency frameworks such as IQ.  We conclude with the use of BG to demonstrate
a novel SQL middleware named KOSAR.

BG is joint work with Sumita Barahmand.  Visit http://bgbenchmark.org to download BG.Speaker Bio Shahram Ghandeharizadeh received his Ph.D. degree in Computer Science from the
University of Wisconsin, Madison, in 1990. Since then, he has been on the faculty
at the University of Southern California. In 1992, he received the National Science
Foundation Young Investigator's Award for his research on the physical design of parallel
database systems. In 1995, he received an award from the School of Engineering at USC
in recognition of his research activities. He was a recipient of the ACM Software
System Award 2008.  His primary motivation for developing BG is today's proliferation
of many data stores and a scarcity of benchmarks to substantiate their claims.Oct. 30, 2014 (Special Day/Time)SPEAKER: Dr. David Lomet (MSR) Achieving Ridiculously High TPSDetailsDate and TimeOct. 30, 2014 (Special Day/Time) 4-5 pmLocationDBH 3011SpeakerDr. David Lomet (MSR) TitleAchieving Ridiculously High TPSAbstractThe Deuteronomy architecture provides a clean separation of transaction functionality (performed in a transaction component, or TC) from data management functionality (performed in a data component, or DC). In prior work we implemented both a TC and DC that achieved modest performance. We recently built a high performance DC (the Bw-tree key value store) that achieves very high performance on modern hardware via latch-free and log structuring techniques and is currently shipping as an indexing and storage layer in Microsoft systems such as Hekaton and DocumentDB. The new DC executes operations more than 100x faster than the TC we previously implemented.  This talk describes how we achieved two orders of magnitude speedup in TC performance and shows that a full Deuteronomy stack can achieve very high performance overall. We built the TC using techniques analogous to the Bw-tree (latch-free data structures, log-structuring). The TC uses multi-version concurrency control (MVCC) to improve concurrency and performance.  Our new prototype TC scales to 32 cores on our 4 socket NUMA machine and commits more than a million of transactions per second for a variety of workloads.Speaker BioDavid Lomet has been a principal researcher and manager of the Database Group at Microsoft Research, Redmond since 1995.  Before that, he was at Digital Equipment Corporation, mainly at Cambridge Research Lab.  Earlier, he was a research staff member at IBM Research in Yorktown and subsequently a Professor at Wang Institute.  Lomet spent a sabbatical at Newcastle University working with Brian Randell.  He is best known for his work in database systems and is one of the inventors of the transaction concept.  His database work has focused on access methods, concurrency control, and recovery.  His recent Bw-tree work is part of Microsoft's Hekaton main memory database system. He has published over 100 papers, including two SIGMOD "best paper" awards, and has over 40 patents. Lomet has served on many PCs, including SIGMOD, VLDB, and ICDE.  He has been ICDE'2000 PC co-chair, VLDB'2006 Core Track Chair, and ICDE'2001 conference co-chair.  Lomet has been editor-in-chief of the Data Engineering Bulletin since 1992, and won the 2011 SIGMOD Contributions Award for this. He has also been an editor of ACM TODS and the VLDB Journal, has served on the VLDB Endowment Board and ICDE Steering Committee, and has been Chair of the IEEE TC on Data Engineering. Dr. Lomet is a Fellow of AAAS, ACM, and IEEE.  Oct. 24, 2014 (Special Time/Place)SPEAKER: Prof. Padhraic Smyth and othersUCI Data Science Kickoff MeetingDetailsDate and TimeOct. 24, 2014 (Special Time/Place) 1:30-5 PM (with reception to follow)LocationCalIIT2 AuditoriumSpeakerProf. Padhraic Smyth and othersTitleUCI Data Science Kickoff MeetingAbstractThis will be the official kickoff event for a new UCI campus-wide Data Sciences Initiative.
Come hear about the Initiative as well as efforts related to Data Sciences from across the campus.
The afternoon's program will be followed by a reception at 5 PM.
This event is open to anyone/everyone who is interested.
A detailed agenda for this afternoon event can be found hanging here: http://datascience.uci.edu/.Speaker Bio Oct. 17, 2014SPEAKER: Mark Callaghan (Facebook)Still Doing It WrongDetailsDate and TimeOct. 17, 2014 3 pmLocationDBH 3011SpeakerMark Callaghan (Facebook)TitleStill Doing It WrongAbstractFamous people have interesting things to say about my work and my fate. I hope to provide more context on doing "small data" (per request, e.g., OLTP) at scale. I will start with a short history of web-scale MySQL from 2005 until today and predict where it is heading in the next 5 years. My current work is to improve storage efficiency for small data (per request) workloads. Algorithms tend to be fixed in their behavior, while both workloads and storage device performance vary. There is thus an opportunity to improve efficiency by making algorithms more dynamic.Speaker BioMark Callaghan has worked with great teams to make MySQL better for scale-out deployments at Facebook and Google for 9+ years. His current focus at Facebook is the analysis and improvement of database algorithms and storage systems for small data (OLTP) workloads. He also works with WebScaleSQL and RocksDB to make MySQL and MongoDB better. Prior to his web-scale work Mark spent many years working on RDBMS internals at Oracle and Informix. He invented and implemented a very fast general purpose sort algorithm for the Oracle RDBMS. He has an MS in CS from UW-Madison.
Oct. 10, 2014SPEAKER: Prof. Jimeng Sun (Georgia Institute of Technology)Do it Once, Do it Right - Building a Scalable Predictive Modeling Platform for Healthcare Applications DetailsDate and TimeOct. 10, 2014 3 pmLocationDBH 3011SpeakerProf. Jimeng Sun (Georgia Institute of Technology)TitleDo it Once, Do it Right - Building a Scalable Predictive Modeling Platform for Healthcare Applications Abstract Predictive models are designed to predict the likelihood of one or more outcomes and are playing an increasing important role in biomedical research. Thanks to the explosion of Electronic Heart Records (EHR), the interest in building predictive models based on EHR data has skyrocketed in recent years.  There are some major challenges that remain to be addressed.  In this talk I will explore two of them. Effective algorithms are lacking in dealing with high-dimensional, longitudinal, sparse, inaccurate and inconsistent EHR data. The methodologies to develop predictive models are still labor intensive and ad-hoc. These rudimentary approaches are hindering the quality and throughput of healthcare and biomedical research.
In this talk, we promote a holistic approach that addresses both challenges by combining 1) algorithm development and 2) system building. We believe that a more robust and domain specific big-data platform could significantly speedup the development of robust and accurate predictive models for biomedical research. 
I will present different projects covering both aspects of such a platform:
Algorithms: I will first describe our work on computational phenotyping from EHR data using sparse tensor factorization; then I will present a patient similarity method using supervised distance metric learning
System: I will introduce a parallel predictive modeling platform using Hadoop for enabling large scale modeling and exploration of big healthcare dataSpeaker BioJimeng Sun is an Associate Professor of School of Computational Science and Engineering at College of Computing in Georgia Institute of Technology. Prior to joining Georgia Tech, he was a research staff member at IBM TJ Watson Research Center. His research focuses on health analytics using electronic health records and data mining, especially in designing novel tensor analysis and similarity learning methods and developing large-scale predictive modeling systems.
Dr. Sun has worked on various healthcare applications such as computational phenotyping from electronic health records, heart failure onset prediction and hypertension control management. He has collaborated with many healthcare institutions including Vanderbilt university medical center, Children's healthcare of Atlanta, Center for Disease Control and Prevention (CDC), Geisinger Health System and Sutter Health.
He has published over 70 papers, filed over 20 patents (5 granted). He has received ICDM best research paper award in 2008, SDM best research paper award in 2007, and KDD Dissertation runner-up award in 2008. Dr. Sun received his B.S. and M.Phil. in Computer Science from Hong Kong University of Science and Technology in 2002 and 2003, and a PhD in Computer Science from Carnegie Mellon University in 2007.Oct. 3, 2014SPEAKER: ISG Faculty2014-15 ISG Welcome (Back) SeminarDetailsDate and TimeOct. 3, 2014 3 pmLocationDBH 3011SpeakerISG FacultyTitle2014-15 ISG Welcome (Back) SeminarAbstractSpeaker Bio May. 30, 2014SPEAKER:  No SeminarDetailsDate and TimeMay. 30, 2014 11 pmLocationDBH 6011Speaker TitleNo SeminarAbstractGo to CS Colloquium for Ed Lazowska's Big Data talk.Speaker Bio May. 23, 2014SPEAKER: Odej Kao (TU Berlin)Dynamic Scheduling and Resource Management for Big DataDetailsDate and TimeMay. 23, 2014 3 pmLocationDBH 3011SpeakerOdej Kao (TU Berlin)TitleDynamic Scheduling and Resource Management for Big DataAbstract	     Speaker Bio May. 16, 2014SPEAKER: Daniel Ford (Dell Research) The Unintended Consequences of The Internet of ThingsDetailsDate and TimeMay. 16, 2014 3 pmLocationDBH 3011SpeakerDaniel Ford (Dell Research)Title The Unintended Consequences of The Internet of ThingsAbstract Computer technology is subject to rapid change and evolution.  Each new development seems to be subject to exuberant hyperbole.  The latest round is focusing on embedded electronics and is called The
Internet of Things, or just "The IoT."  The main force powering this "hype machine" is the suggestion of vast new commercial opportunities, estimated by some sources to be in the range of $19 Trillion. This talk begins with the conclusion that the "hype" about The Internet of Things is underplayed, and that the commercial implications of The Internet of Things are the smallest parts of a bigger story.  It compares The IoT to other historical technological developments, examining what is similar, and what is without historical precedent.  In particular, it finds the impact of applications of The Internet of Things to be spectacularly unconstrained.  All this leads to the idiom "Careful what you wish for."  So, while commerce is "wishing for" a $19 Trillion market, our society, and our economy, are going to get something else. The talk concludes with an examination of some of the potential unintended consequences of The Internet of Things.  It predicts that areas as diverse as Brand Management, Advertising, Propaganda, Healthcare, Law Enforcement, Insurance, Automobile ownership, Politics, and Warfare, to name just a few, will all be affected in ways few are considering.
            Speaker BioDr. Ford is Executive Director and Chief Scientist for Mobility and The Internet of Things for Dell Research in San Jose, California. Prior to joining Dell Research, Dr. Ford was CEO and co-founder of
Paupt Labs LLC, in New York.  He also was with IBM Research, in a variety of positions, for nineteen years before that. His immediate research interests are focused on The Internet of Things, including supporting software architectures, novel applications, and unintended consequences.  Previous research interests have included Healthcare Informatics, Pandemic Modeling, Social Networking, Mobile Computing, Web Search, and High Performance
Tertiary Storage Systems.  Dr. Ford has twenty-eight issued US patents and dozens of peer reviewed publications. Dr. Ford earned his Ph.D. in Computer Science from the University of Waterloo in 1992. 
            May. 9, 2014SPEAKER: Vinayak Borkar (UC Irvine)An Efficient Platform for Parallel Data Processing on Large ClustersDetailsDate and TimeMay. 9, 2014 3 pmLocationDBH 3011SpeakerVinayak Borkar (UC Irvine)TitleAn Efficient Platform for Parallel Data Processing on Large ClustersAbstractThe growth of user activity on the Internet and the rise of social networks has led to an exponential growth of data. Storage of this data and its subsequent analysis have posed significant challenges. Unlike the business data that drove research and development of relational databases for the past several decades, Web and Social data tend to have rich and varying structure, making traditional database systems a poor choice for their management. The astronomical size and semi-structured nature of this new data has forced companies in the business of managing it to look for other cost-effective solutions. In 2004 Google presented the MapReduce system as a way to harness the power of thousands of commodity machines to solve problems like building a search index over the entire World Wide Web in reasonable time at reasonable cost; MapReduce turned out to be a useful tool for performing other parallel computations over large amounts of data while presenting a simple programming model to users. Soon after the MapReduce paper, the open source community created the Hadoop platform to resemble Google's MapReduce system. Hadoop soon became a popular platform for processing large amounts of data using commodity computers. In an effort to boost user productivity, new declarative languages were designed and built to compile high-level declarative queries down to Hadoop MapReduce programs, making Hadoop the de-facto runtime layer for large-scale parallel data computation.

While widely used and popular today, Hadoop was not intentionally designed to be a runtime layer for higher-level declarative languages.
In this talk we explore an alternative to the Hadoop platform whose design is rooted in parallel database research from the 1980s and 1990s. Hyracks is an efficient runtime platform that accepts data-parallel jobs from users and from high-level language compilers and executes them on a cluster of commodity machines. We describe the design of Hyracks as well as salient aspects of its implementation. Using Hyracks we study the trade offs involved in building an extensible and reusable set of runtime components for large-scale data processing. We show experimentally that Hyracks is a highly configurable platform and well-suited for several different data processing tasks. We do so via three different use cases: executing queries expressed in high-level declarative languages, running actual Hadoop jobs using the Hadoop Compatibility Layer of Hyracks, and finally, running parallel graph computations using Pregelix, an open source graph analytics platform that uses Hyracks and emulates Google's Pregel programming model for analyzing large graphs in parallel.

A number of new declarative parallel languages have been proposed for querying and analyzing very large data sets.
To aid with the construction of declarative parallel query compilers, we propose an extensible algebraic framework called Algebricks. Algebricks is a model-agnostic compilation framework that provides the ability for a compiler to inject its own semantics in an extensible manner. Algebricks includes a reusable and extensible set of logical and physical operators and a large set of general purpose rewrite rules useful to most query compilers. We describe the implementation of three different language compilers (AsterixDB for AQL, Hivesterix for HiveQL, and VXQuery for XQuery) that use the Algebricks compiler framework to create parallel jobs to run on a Hyracks Cluster.Speaker BioMay. 2, 2014SPEAKER: Dick Bulterman (FXPAL)Authoring Support for Social Media Interaction: 
Understanding Compound Multimedia DependenciesDetailsDate and TimeMay. 2, 2014 3 pmLocationDBH 3011SpeakerDick Bulterman (FXPAL)TitleAuthoring Support for Social Media Interaction: 
Understanding Compound Multimedia DependenciesAbstract	   Creating compelling multimedia content is a difficult task. It involves not only the creative process 

of developing a compelling media-based story, but it also requires significant technical support for 

content editing and management. This process is made more complex by an increased desire for 

media personalization: the story you tell Mom about an event may be different than the version 

you’d like to share with your friends. It is also different from the version you’d like to tell your own 

children 15 years after the event had taken place. The makes media authoring a context- and time-
sensitive problem. No wonder most researchers analyze media instead of create it!

It is tempting to categorize multimedia authoring in terms of component areas: media encoding, 

media storage, media access, media transport, media rendering and overall presentation 

composition and control. Unfortunately, this partitioning blurs the dependencies that exist among 

these component areas that ultimately determine the success of an authoring system. Using the 

broad problem of social media interaction as an example, this talk will consider the composite 

effects of creating and accessing and transporting and presenting rich media objects for use by non-
technical end users.

The talk will survey several approaches to describe and manage media interactions. We will focus 

on the temporal modelling of context-sensitive personalized interactions of complex collections of 

independent media objects. Using the concepts of ‘togetherness’ being employed in the EU’s FP-7 

project TA2: Together Anywhere, Together Anytime, we will follow the process of media capture, 

profiling, composition, sharing and end-user manipulation. We will consider the promise of using 

automated tools and contrast this with the reality of letting real users manipulation presentation 

semantics in real time.

The talk will not present a closed form solution, but will present a series of topics and problems that 

can stimulate the development of a new generation of systems to stimulate social media interaction.      Speaker Bio Dr. Dick Bulterman is President of the FX Palo Alto Laboratory (FXPAL) and professor of 

computer science at the VU University in Amsterdam. Before joining FXPAL in 2013, he was a 

senior researcher at CWI in Amsterdam, where he founded the Distributed Multimedia Languages 

and Interfaces group. In 1999, he started Oratrix Development BV, a CWI spin-off company that 

transferred the group's SMIL-based GRiNS software to many parts of the civilized world. Prior to 

joining CWI in 1988, he was on the faculty of the Division of Engineering at Brown University, 

where he was part of the Laboratory for Engineering Man/Machine Systems. Dr. Bulterman 

received a Ph.D. in computer science from Brown University (USA) in 1982. In 2013 he was 

awarded the ACM SIGMM Lifetime Technical Achievement Award. He is a member of Sigma Xi, 

the ACM and the IEEE. April. 18, 2014SPEAKER: Pekka Kostamaa (Teradata)Big Data at Teradata – Teradata Unified Data ArchitectureDetailsDate and TimeApril. 18, 2014 3 pmLocationDBH 3011SpeakerPekka Kostamaa (Teradata)TitleBig Data at Teradata – Teradata Unified Data ArchitectureAbstract	 
Unified Data Architecture (UDA) is Teradata’s strategy and program for Big Data. UDA combines three platforms in a unified architecture:
1.       Data Warehouse;
2.       Discovery Platform;
3.       Data Platform.
This talk will describe the architecture and present real customer use cases.
     Speaker BioPekka Kostamaa is Senior Director of Product Management for the Teradata

Database. His team is responsible for the strategy and definition of new releases 

of Teradata, from concept phase through development to delivery to customers.

Previously, Pekka was the Vice President of Engineering and Big Data Lab 

for Teradata Aster and Director of Advanced Development and Enterprise 

Architecture for Teradata R and D.

He has several publications, holds twenty patents with several pending, and 

presented the Keynote Speech at the ICDE 2011 conference and an Invited Talk 

at the 2012 DOLAP Workshop. He is a member of the UCLA Computer Science 

Advisory Boards. March. 14, 2014SPEAKER: Christoph Freytag (Humboldt U)When to say NO to protect Privacy when answering QueriesDetailsDate and TimeMarch. 14, 2014 3 pmLocationDBH 3011SpeakerChristoph Freytag (Humboldt U)TitleWhen to say NO to protect Privacy when answering QueriesAbstractThis talk presents privacy concepts that keep the balance between utility and privacy when returning 

answers to a sequence of queries. In particular we show how to model the (increasing) knowledge of an 

adversary resulting from the answers to queries by a sequence of bipartite graphs. Those provide the 

foundation for deciding when a privacy breach occurs (might occur) and how to balance the need for 

accurate responses versus the right for privacy. Examples demonstrate the intricacies of managing this 

trade-off.	     Speaker BioJohann-Christoph Freytag is currently full professor for Databases and Information Systems (DBIS) at the Computer Science Department of the Humboldt-Universität zu Berlin, Germany. Before joining the department in 1994, he was a research staff member at the IBM Almaden Research Center (1985-1987), a researcher at the European Computer-Industry-Research Centre (ECRC, in Munich, Germany, 1987-1989), and the head of Digital's Database Technology Center (also in Munich, 1990-1993). He holds a Ph.D. in Applied Mathematics/Computer Science from Harvard University, MA.

Prof. Freytag's research interests include all aspects of query processing and query optimization in object-relational database systems, new developments in the database area (such as semi-structured data, data quality, databases and security), privacy in database systems, and applying database technology to applications such as GIS, genomics, and bioinformatics/life science. In the last years he received the IBM Faculty Award four times for collaborative work in the areas of databases, middleware, and bioinformatics/life science. He organized the VLDB conference in Berlin in 2003 and was a member of the VLDB Endowment (2001-2007) and in the head of the German database interest group of the GI (Fachbereich DBIS, Gesellschaft fur Informatik). March. 7, 2014SPEAKER: Michalis Petropoulos and Mohamed Soliman (Pivotal)Orca: A Modular Query Optimizer Architecture for Big DataDetailsDate and TimeMarch. 7, 2014 3 pmLocationDBH 3011SpeakerMichalis Petropoulos and Mohamed Soliman (Pivotal)TitleOrca: A Modular Query Optimizer Architecture for Big DataAbstractThe performance of analytical query processing in data management systems depends primarily on the capabilities of the system's query optimizer. Increased data volumes and heightened interest in processing complex analytical queries have prompted Pivotal to build a new query optimizer. In this talk we present the architecture of Orca, the new query optimizer for all Pivotal data management products, including Pivotal Greenplum Database and Pivotal HAWQ. Orca is a comprehensive development uniting state-of-the-art query optimization technology with own original research resulting in a modular and portable optimizer architecture. In addition to describing the overall architecture, we highlight several unique features and present performance comparisons against other systems.     Speaker Bio Michalis Petropoulos is managing the query processing team at Pivotal Inc. His R and D team develops the query optimizer and executor for Pivotal’s massively parallel and distributed data management products, Pivotal Greenplum Database and Pivotal HAWQ. Before that, Michalis was an Assistant Professor in the Computer Science and Engineering Department at SUNY Buffalo from 2006 to 2010. He received his PhD in Computer Science from the University of California, San Diego in 2005. In 2006, Michalis co-authored a publication that was awarded an Honorable Mention as top-3 finalist in the SIGMOD 2006 Best Paper Award competition. In 2010, he co-authored a publication that received the Best Interdisciplinary Paper Award at the ACM Conference on Information and Knowledge Management (CIKM).

Mohamed Soliman is a Staff 1 at Pivotal, where he works on building massively distributed database systems for efficient support of data warehousing and analytics. His work at Pivotal is mainly in the research and development of Orca, a next generation query optimizer for Big Data. Prior to that, Mohamed has conducted graduate studies at University of Waterloo, where he received his PhD in computer science in 2010 on the topic of rank-aware retrieval in probabilistic databases.Feb. 21, 2014SPEAKER:  Inci Cetindil (UCI ISG)  DetailsDate and TimeFeb. 21, 2014 3 pmLocationDBH 3011Speaker Inci Cetindil (UCI ISG)Title  Abstract	     Speaker Bio Feb. 14, 2014SPEAKER: Siripen Pongpaichet (UCI ISG) EventShopDetailsDate and TimeFeb. 14, 2014 3 pmLocationDBH 3011SpeakerSiripen Pongpaichet (UCI ISG)Title EventShopAbstract
EventShop is a computational framework that has the ability to integrate and process streaming data from heterogeneous data sources.  Data from all data sources are first transformed into a Space/Time/Theme (STT) data model, with a hierarchical extension of STT being used to handle data coming from sources that have different resolutions in space and/or time.   Various types of spatio-temporal operators can then be applied to recognize and predict actionable situations.  Appropriate actions/recommendations can be sent to individuals based on their circumstances.   This talk will provide an overview of the EventShop project and the sorts of use cases it is intending to address.
(Here is the link to our website http://eventshop.ics.uci.edu:8080/sln/)
	     Speaker Bio Siripen is a UCI CS Ph.D. student working in Ramesh Jain's EventShop group.Feb. 7, 2014SPEAKER:  Inci Cetindil (UCI ISG) (Postponed due to illness) DetailsDate and TimeFeb. 7, 2014 3 pmLocationDBH 3011Speaker Inci Cetindil (UCI ISG)Title (Postponed due to illness) Abstract	     Speaker Bio Jan 31, 2014 (Special Location)SPEAKER: Padhraic Smyth (UCI)Statistical Machine Learning with Count Data (Informatics Seminar)DetailsDate and TimeJan 31, 2014 (Special Location) 3 pmLocationDBH 6011 SpeakerPadhraic Smyth (UCI)TitleStatistical Machine Learning with Count Data (Informatics Seminar)Abstract	(Regular ISG Seminar attendees are encouraged to attend this week's
very interesting Informatics Seminar - we don't want to conflict with
this talk!)

Data represented in the form of sets of counts is easy to acquire and 
can be surprisingly useful in practice. For example, a simple way to 
represent a set of documents is as a "bag of words" where each document 
is represented just by the counts of words that occur in the document, a 
representation that has been the basis for many successful applications 
of machine learning to text data. In this talk we will review some 
important developments over the past 10 years in modeling data 
represented in the form of counts, combining ideas from statistics and 
machine learning. The talk will describe the general principles involved 
and then illustrate how these ideas can be applied to text documents, 
email communications, and social networks, including recent work in my 
research group. The talk will conclude with some speculative comments on 
future directions.
     Speaker BioPadhraic Smyth is a Professor in the Department of Computer Science 
(with a joint appointment in Statistics) and Director of the Center for 
Machine Learning and Intelligent Systems at the University of 
California, Irvine. His research interests include machine learning, 
data mining, pattern recognition, and applied statistics. He received a 
first class honors degree in Electronic Engineering from University 
College Galway (National University of Ireland) in 1984, and the MSEE 
and PhD degrees from the Electrical Engineering Department at the 
California Institute of Technology in 1985 and 1988 respectively. From 
1988 to 1996 he was a Technical Group Leader at the Jet Propulsion 
Laboratory, Pasadena, and has been on the faculty at UC Irvine since 
1996. Dr. Smyth is an ACM Fellow, a AAAI Fellow, and recieved the ACM 
SIGKDD Innovation Award in 2009. He is co-author of two well-known 
research texts in data mining: Modeling the Internet and the Web: 
Probabilistic Methods and Algorithms (with Pierre Baldi and Paolo 
Frasconi in 2003), and Principles of Data Mining, MIT Press, August 
2001, co-authored with David Hand and Heikki Mannila. He has served in 
editorial positions for journals such as the Journal of the American 
Statistical Association, the IEEE Transactions on Knowledge and Data 
Engineering, and the Journal of Machine Learning Research. His research 
has been funded by a variety of government agencies such as NSF, NIH, 
ONR, DARPA and DOE, as well by companies such as Google, IBM, Microsoft, 
and Yahoo! In addition to his academic research he is also active in 
consulting, working with companies such as Samsung, Netflix, eBay, 
Oracle, Microsoft, Yahoo!, Nokia, and ATT. Jan. 24, 2014SPEAKER: Inna Giguere (Data Architect, Disney Interactive Media BI)Web Analytics at the happiest place on earthDetailsDate and TimeJan. 24, 2014 3 pmLocationDBH 3011SpeakerInna Giguere (Data Architect, Disney Interactive Media BI)TitleWeb Analytics at the happiest place on earthAbstract
Business analytics requirements at Disney Interactive have pushed the limits of the Omniture reporting systems that has been used for the past decade into building an internal tracking and data warehouse solution. Consequently, we have built a data warehouse and enabled Video and Game Producers to fine-tune new content in near real-time, as well as provide an exhaustive platform for Data Scientists to build recommendation systems.
The presentation will focus on current data pipeline architecture at Disney Interactive and cover specific steps and challenges.  I will discuss how we (BI team) were able to leverage Hadoop’s map/reduce processing capabilities and Vertica MPP engine to load data continuously from multiple sources. However, one of our biggest challenges remains handling memory intensive hash joins in Vertica without sacrificing performance. 
	     Speaker BioInna Giguere is Data Architect at Disney Interactive Media Business Intelligence group. For the last 2 years she has been leading the architecture design and implementation of the Analytics Data Warehouse utilizing Hadoop, Vertica, and Scribe technology. Previously based out of San Francisco Bay Area and London, Inna has 16 years on industry experience creating scalable Data Warehouse solutions with focus on DB performance optimization in transactional and reporting systems. Her experience spans across technologies starting from COBOL/DB2 to Oracle (8i – 11g), to SQL Server 2005-2012, to Vertica 6.1 working on datasets ranging from hundreds of megabytes to hundreds of terabytes. She has earned MS in Statistics in 2010.
Jan. 17, 2014SPEAKER: Phillip Sheu (Department of EECS)Semantic Computing and ApplicationsDetailsDate and TimeJan. 17, 2014 3 pmLocationDBH 3011SpeakerPhillip Sheu (Department of EECS)TitleSemantic Computing and ApplicationsAbstract
Semantic Computing (SC) is an emerging field that addresses computing
technologies which allow users to search, create, manipulate and connect
computational resources (including data, documents, tools, people, agents,
devices, etc.) based on semantics.

Semantic Computing includes the computing technologies (e.g., artificial
intelligence, natural language, software engineering, data and knowledge
engineering, computer systems, signal processing, etc.), and their
interactions, that may be used to extract or process computational content
and descriptions. While some areas of Semantic Computing have appeared as
pieces in different disciplines, Semantic Computing glues these pieces
together into an integrated theme with synergetic interactions. It
addresses not only the analysis and transformation of signals (e.g.,
pixels, words) into useful information, but also how such information can
be accessed and used to synthesize new signals.

The National Science Foundation has approved the planning of an
Industry/University Cooperative Research Center (I/UCRC) for Semantic
Computing currently involving UCI, UCSD and UCLA. The missions of the
I/UCRC are to develop semantic technologies that may facilitate the
transition of the Internet into its next generation, and develop new
business models to stimulate, strengthen, and grow the economy.

An important outcome of this I/UCRC is a Semantic Problem Solving Network
(SPSN) which is a public consortium of resources from all domains
including data, documents, devices, products, services, and people. The
resources are interconnected and integrated with a service-oriented
architecture and a semantic layer to help the public to solve general
problems and professional users to solve domain specific problems (e.g.,
finance, IT, health, defense, entertainment, education, manufacturing).

This talk will introduce Semantic Computing and its applications, the
operations of the I/UCRC, the architecture of the SPSN, how companies and
academic researchers can join or affiliate with the I/UCRC and SPSN, and
how companies and academic researchers can benefit.
	     Speaker Bio
Phillip C.-Y. Sheu is a professor of EECS, Computer Science and Biomedical
Engineering at the University of California, Irvine. He received his B.S.
degree in EE from National Taiwan University, and MS and Ph.D degrees in
EECS from the University of California at Berkeley.

Dr. Sheu’s current research interests include semantic computing and
complex biomedical systems. He is a fellow of IEEE, a founder of the IEEE
Computer Society Technical Committee on Semantic Computing (TCSEM), IEEE
International Conference on Semantic Computing (ICSC), International
Journal of Semantic Computing (IJSC), the NSF I/UCRC (Industry University
Cooperative Research Center) for Semantic Computing (ISC) being planned,
and a main author of the book Semantic Computing (SC, eds. P. Sheu, H. Yu,
C.V. Ramamoorthy, A. Joshi and L.A. Zadeh, IEEE and Wiley, 2010).
Nov. 15, 2013SPEAKER: José A. Blakeley (Microsoft Corporation)Microsoft SQL Server Parallel Data Warehouse - Architecture OverviewDetailsDate and TimeNov. 15, 2013 3 pmLocationDBH 3011SpeakerJosé A. Blakeley (Microsoft Corporation)TitleMicrosoft SQL Server Parallel Data Warehouse - Architecture OverviewAbstract
 In this talk I will present an architectural overview of the SQL Server Parallel Data Warehouse DBMS system. PDW is a massively parallel processing, share nothing, scale-out version of SQL Server for data warehouse and big data workloads. The product is packaged as a database appliance built on industry standard hardware.
	     Speaker BioJosé Blakeley is Partner Architect in the Modern Data Warehousing Unit of the Server and Tools Division at Microsoft where he contributes to the development of the SQL Server Parallel Data Warehouse (PDW) DBMS product. José joined Microsoft in 1994. Some of his contributions at Microsoft include the development of the OLE DB data access interfaces, the integration of the .NET runtime with SQL Server 2005, the extensibility features in SQL Server, and the creation of the ADO.NET Entity Framework in Visual Studio 2008. José has authored many conference papers, book chapters and journal articles on design aspects of relational and object database management systems, and data access. Jose has 20 patents awarded and 22 patents pending. He became an ACM Fellow in 2009. Before joining Microsoft, José was a member of the technical staff with Texas Instruments where he was co-principal investigator of the DARPA Open-OODB system. He received a B. Eng from ITESM, Monterrey, Mexico, and a Ph.D. in computer science from University of Waterloo, Canada. Oct. 25, 2013SPEAKER: David Lomet (joint work with Justin Levandoski and Sudipta Sengupta)(MSR)LLAMA: A Cache/Storage Subsystem for Modern HardwareDetailsDate and TimeOct. 25, 2013 3 pmLocationDBH 3011SpeakerDavid Lomet (joint work with Justin Levandoski and Sudipta Sengupta)(MSR)TitleLLAMA: A Cache/Storage Subsystem for Modern HardwareAbstract
LLAMA is a subsystem designed for new hardware environments that supports an API for page-oriented access methods, providing both cache and storage management. Caching (CL) and storage (SL) layers use a common mapping table that separates a page’s logical and physical location. CL supports data updates and management updates (e.g., for index re-organization) via latch-free compare-and-swap atomic state changes on its mapping table. SL uses the same mapping table to cope with page location changes produced by log structuring on every page flush. To demonstrate LLAMA’s suitability, we tailored our latch-free Bw-tree implementation to use LLAMA. The Bw-tree is a B-tree style index. Layered on LLAMA, it has higher performance and scalability using real workloads compared with BerkeleyDB’s B-tree, which is known for good performance.
	     Speaker Bio
David Lomet (Ph.D from Penn) is a Principal Researcher and manager of the Database Group at Microsoft Research Redmond.  Earlier, he was at Digital's CRL, Wang Institute, and IBM Research.  Lomet has over 100 papers on databases, indexing, concurrency, and recovery, including two SIGMOD "best papers".  He is an inventor of transactions.  Lomet has served on SIGMOD, VLDB, and ICDE PCs, being co-chair of ICDE'2000 and VLDB'2006.  He won SIGMOD's Contributions Award for his service as Data Engineering Bulletin Editor-in-Chief since 1992. He has been editor of ACM TODS, VLDB Journal, and DAPD.  He has served on the VLDB Endowment and ICDE Steering Committee, has been IEEE TCDE Chair and is a Fellow of AAAS, ACM, and IEEE.
             Oct. 18, 2013SPEAKER: Tyson Condie (Microsoft and UCLA)Big Learning SystemsDetailsDate and TimeOct. 18, 2013 3 pmLocationDBH 3011SpeakerTyson Condie (Microsoft and UCLA)TitleBig Learning SystemsAbstract
A new wave of systems is emerging in the space of Big Data Analytics that open the door to programming models beyond Hadoop MapReduce (HMR). It is well understood that HMR is not ideal for applications in the domain of machine learning and graph processing. This realization is fueling a number of new (Big Data) system efforts: Berkeley Spark, Google Pregel, GraphLab (CMU), and AsterixDB (UC Irvine), to name a few. Each of these add unique capabilities, but form islands around key functionalities: fault-tolerance, resource allocation, and data caching. In this talk, I will provide an overview of some Big Data systems starting with Google's MapReduce, which defined the foundational architecture for processing large data sets. I will then identify a key limitation in this architecture; namely, its inability to efficiently support iterative workflows. I will then describe real-world examples of systems that aim to fill this computational void. I will conclude with a description of my own work on a layering that unifies key runtime functionalities (fault-tolerance, resource allocation, data caching, and more) for workflows (both iterative and acyclic) that process large data sets.
	     Speaker Bio
Tyson Condie is a principal scientist with the Cloud and Information Services Lab at Microsoft and an Assistant Professor at UCLA. He received his Ph.D. from Berkeley. His research focuses on data analytics, distributed systems, Internet-scale query processing and optimization, and declarative language design and implementation. His current work involves building a system software stack for large-scale data processing tasks on resource managers like Apache YARN, Berkeley Mesos, Google Omega, and Facebook Corona.             Oct. 11, 2013SPEAKER: Anhai Doan (University of Wisconsin and WalmartLabs)Toward Hands-Off Crowdsourcing: Crowdsourced Entity Matching for the MassesDetailsDate and TimeOct. 11, 2013 3 pmLocationDBH 3011SpeakerAnhai Doan (University of Wisconsin and WalmartLabs)TitleToward Hands-Off Crowdsourcing: Crowdsourced Entity Matching for the MassesAbstractEntity matching (EM) finds data records that refer to the same
real-world entity. Recent work has applied crowdsourcing to EM, and
has clearly established the promise of this approach. This work
however is limited in that it crowdsources only parts of the EM
workflow, requiring a developer who knows how to code to execute the
remaining parts. Consequently, this work does not scale to the growing
EM need at enterprises and crowdsourcing startups, and cannot handle
scenarios where ordinary users (i.e., the masses) want to leverage
crowdsourcing to match entities.

To address these problems, we propose the notion of hands-off
crowdsourcing (HOC), which crowdsources the entire workflow of a task,
thus requiring no developers. We show how HOC can represent a next
logical direction for crowdsourcing research, scale up EM at
enterprises and crowdsourcing startups, and open up crowdsourcing for
the masses. We describe Corleone, a HOC solution for EM. We show how
Corleone uses the crowd to generate blocking rules, applies active
learning to learn matchers, estimates accuracy given severe skew, and
identifies difficult-to-match pairs to which Corleone can apply more
complex matchers. Finally, we discuss the implications of our work to
executing crowdsourced RDBMS joins, cleaning learning models, and
soliciting complex information types from crowd workers.
	     Speaker BioAnHai Doan is an Associate Professor in the database group at the
University of Wisconsin, Madison. His current interests include
crowdsourcing, knowledge bases, data integration, and information
extraction. He received the ACM Doctoral Dissertation Award in 2003
and a Sloan fellowship in 2007. AnHai was Chief Scientist of Kosmix, a
social media startup acquired by Walmart in 2011. Currently he also
works as Chief Scientist of WalmartLabs, a research and development
lab devoted to analyzing and integrating data for e-commerce. AnHai is
a co-author of “Principles of Data Integration” (with Alon Halevy and
Zack Ives), a textbook published by Morgan Kaufmann in 2012. Sept. 20, 2013SPEAKER: Li Xiong (Emory University)Real-Time Aggregate Monitoring with Differential PrivacyDetailsDate and TimeSept. 20, 2013 2 pmLocationDBH 3011SpeakerLi Xiong (Emory University)TitleReal-Time Aggregate Monitoring with Differential PrivacyAbstract
While Big Data promises significant economic and social benefits, it also raises serious privacy concerns.  Real-time aggregate statistics of data collected from individuals can be shared to enable many applications such as disease surveillance and traffic monitoring.  However, it must be ensured that the privacy of individuals is not compromised.  While differential privacy has emerged as a de facto standard for private data analysis, directly applying the differential privacy mechanisms on time-series has limited utility due to high correlations between data values.  In this talk, I will present FAST, a novel Filtering and Adaptive Sampling based framework for monitoring aggregate Time-series under differential privacy.  FAST adaptively samples long time-series according to detected data dynamics and simultaneously uses filtering techniques to dynamically predict and correct released data values.  I will present experimental studies using real datasets demonstrating the feasibility and benefit of FAST and conclude with open questions. 
	     Speaker Bio
Li Xiong is an Associate Professor in the Department of Mathematics and Computer Science and the Department of Biomedical Informatics at Emory University where she directs the Assured Information Management and Sharing (AIMS) research group. She holds a PhD from Georgia Institute of Technology, an MS from Johns Hopkins University, and a BS from University of Science and Technology of China, all in Computer Science. She also worked as a software engineer in IT industry for several years prior to pursuing her doctorate. Her areas of research are in data privacy and security, distributed data management, and biomedical informatics. She is a recent recipient of the Career Enhancement Fellowship by Woodrow Wilson Foundation, a Cisco Research Award, and an IBM Faculty Innovation Award. Her current research is supported by NSF and AFOSR.
             Sep. 13, 2013SPEAKER: Raman Grover (ISG PhD candidate)Scalable Fault-tolerant Elastic Data FeedsDetailsDate and TimeSep. 13, 2013 3 pmLocationDBH 3011SpeakerRaman Grover (ISG PhD candidate)TitleScalable Fault-tolerant Elastic Data FeedsAbstractIn this ISG talk / thesis proposal, I describe and study
the support for data feed ingestion in AsterixDB, a Big Data
Management System (BDMS) that provides a platform for the
scalable storage, searching, and analysis of very large volumes
of semi-structured data. Data feeds are a mechanism for having
continuous data arrive into a database system from external
sources that produce data continuously, and to have that data
incrementally populate a persisted dataset and associated indexes.
To my knowledge, this will be the first system to explore the
challenges involved in building a data ingestion facility that deals
with semi-structured data and employs partitioned parallelism to
scale the facility and couple it with high-volume and/or parallel
external data sources. I describe language-level support for
modeling/defining a feed and present the methodology for providing
tolerance to software/hardware failures. Mechanisms by
which a feed can dynamically adapt to different workloads for
optimum usage of resources are provided.May 17, 2013SPEAKER: Charles Boicey (UCI Irvine Health and Information Services)Apache Hadoop in the Healthcare SettingDetailsDate and TimeMay 17, 2013 3 pmLocationDBH 3011SpeakerCharles Boicey (UCI Irvine Health and Information Services)TitleApache Hadoop in the Healthcare SettingAbstractApache Hadoop is open source software that enables distributed processing of large data sets across clusters of computers. Hadoop can scale up to thousands of computers, each able to store and process data. Hadoop is capable of ingesting and storing the types of data found in healthcare, structured, unstructured, image and video. Hadoop also has an advantage for healthcare in its ability to interoperate with other open source software. This interoperability combined with scalability makes Hadoop an ideal platform for the development of a software ecosystem that fills in the gaps left by the Electronic Medical Record and Enterprise Data Warehouse.Speaker Bio
Charles Boicey is the Informatics Solutions Architect for the UC Irvine Health. At UCI Charles is responsible for the development and implementation of the enterprise data warehouse, health information exchange, home health integration and UC Irvine Health’s “Big Data” initiative. Charles has 20 years of experience in the healthcare field.cope of clinical expertise encompasses trauma critical care nursing. Charles is Vice President of the American Nursing Informatics Association.
             May 10, 2013SPEAKER: No SeminarDetailsDate and TimeMay 10, 2013 3 pmLocationDBH 3011SpeakerTitleNo SeminarAbstract
No ISG seminar.  Leaving this time free so that ISG affiliates can attend today's ICS Trends in Society and Information Technology talk (see www.ics.uci.edu/trends).             
             Speaker Bio
             April 19, 2013SPEAKER: Michael J. Carey (with the AsterixDB dev team) Want To Kick My Asterix(DB)?DetailsDate and TimeApril 19, 2013 3 pmLocationDBH 3011SpeakerMichael J. Carey (with the AsterixDB dev team) TitleWant To Kick My Asterix(DB)?Abstract
		Due to several faculty traveling and thus being MIA this coming Friday, the planned ISG seminar by Teradata is being postponed until later in the quarter.  Instead, this week's Friday ISG seminar slot will be used to invite ISG (and ICS) community participation in the forthcoming Beta Release of a new open source BDMS (Big Data Management System) that members of UCI's ASTERIX project have been working on for nearly four years.  We want this new "product", to be called AsterixDB, to be very high quality - and we are hereby inviting interested helpers at UCI to come hear about it and then help us polish it by downloading it and playing with it - trying out its data model, query language, and API for apps - kicking its tires - this Friday!  Our goal is to get a handful of "outside the team" folks to join us in using the system ahead of the Beta Release and then filing any issues using the GoogleCode issue tracking infrastructure - so that when we release this publically, it's well-polished and well shaken out.  (To date we have only delivered an Alpha Release, and only very recently, to one of our industrial partners.)  So - if you like database technology and would like to help us deliver "Big Data 2.0" to the world in a month or so - and you have time/interest in playing a bit in the very near term - PLEASE COME FRIDAY and we will show you what AsterixDB is all about!  This will be an informal presentation, based on giving a tour of our Alpha documentation and the release info on the GoogleCode wiki, and then having the team give a demo and even help you get the system working in real time if you bring your favorite laptop when you come.  We hope to see some of you there! (Refreshments will be provided, as usual for the ISG seminar, but we might upgrade the refreshments a little for this event; we'll see. We will also surely do something nice later for any outside folks who do end up significantly contributing in this manner to the quality of the release.)  If you plan to come on Friday, please RSVP to the speaker (mjcarey@ics.uci.edu) so we know what to maybe expect in terms of potential turnout.  Thx!          
             Speaker Bio
		Mike Carey is a Professor in the ISG subgroup of the UCI CS department.  His goal is to eventually change the Big Data management landscape forever through the great work that our AsterixDB team has done and is now preparing to share.  :-)
             April 12, 2013SPEAKER: Shahram Ghandeharizadeh (USC)DetailsDate and TimeApril 12, 2013 3 pmLocationDBH 3011SpeakerShahram Ghandeharizadeh (USC)TitleAbstract          
             Speaker Bio
             March 1, 2013SPEAKER: Pat Helland (Salesforce)Immutability Changes EverythingDetailsDate and TimeMarch 1, 2013 3 pmLocationDBH 3011SpeakerPat Helland (Salesforce)TitleImmutability Changes EverythingAbstractFor a number of decades, I've been saying "Computing Is Like Hubble's Universe, Everything Is Getting Farther Away from Everything Else".   It
used to be that everything you cared about ran on a single database and the transaction system presented you the abstraction of a singularity;
your transaction happened at a single point in space (the database) and a single point in time (it looked like it was before or after all other transactions).

Now, we see a more complicated world.  Across the Internet, we put up HTML documents or send SOAP calls and these are not in a transaction.  Within a cluster, we typically write files in a file system and then read them later in a big map-reduce job that sucks up read-only files, crunches, and writes files as output.  Even inside the emerging many-core systems, we see high-performance computation on shared memory but increasing cost to using semaphores.  Indeed, it is clear that "Shared Memory Works Great as Long as You Don't Actually SHARE Memory".

There are emerging solutions which are based on immutable data.  It seems we need to look back to our grandparents and how they managed distributed work in the days before telephones.  We realize that "Accountants Don't Use Erasers" but rather accumulate immutable knowledge and then offer interpretations of their understanding based on the limited knowledge presented to them.  This talk will explore a number of the ways in which our new distributed systems leverage write-once and read-many immutable data.         
             Speaker BioPat Helland has been working on databases, transaction processing, messaging, and distributed systems for 34 years.  In the 1980s, he was chief architect of the Tandem NonStop's transaction system called TMF (Transaction Monitoring Facility).  From 1991 to 1994, he worked at HaL Computers (a subsidiary of Fujitsu) and designed and architected a CC-NUMA (Cache Coherent Non-Uniform Memory Architecture) multiprocessor which Fujitsu shipped.  Starting in 1994, Pat worked at Microsoft where he was the chief architect for MTS (Microsoft Transaction Server) and DTC (Distributed Transaction Coordinator).  Later, he built SQL Service Broker which offers high performance (>100K msg/sec) transactional exactly-once messaging integrated with SQL Server.  From 2005 to 2007, Pat work at Amazon on the product catalog and then returned in 2007 to Microsoft.  By 2009, he was working on Cosmos, the multi-peta-byte storage and computational plumbing behind Bing.  This year, Pat moved to San Francisco to be by the grandkids and joined Salesforce.com working on database and filesystem technology.
             Feb 22, 2013SPEAKER: Swaroop Jagadish and Kapil Surlaker (LinkedIn)On Brewing Fresh Espresso: LinkedIn’s Distributed Data Serving PlatformDetailsDate and TimeFeb 22, 2013 3 pmLocationDBH 3011SpeakerSwaroop Jagadish and Kapil Surlaker (LinkedIn)TitleOn Brewing Fresh Espresso: LinkedIn’s Distributed Data Serving PlatformAbstractAs LinkedIn has grown, our core data sets and request processing requirements have grown as well. The development of Espresso was motivated by our desire to migrate LinkedIn’s online serving infrastructure from monolithic, commercial, RDBMS systems running on high cost specialized hardware to elastic clusters of commodity servers running free software; and to improve agility by enabling rapid development by simplifying the programming model, separating scalability, routing, caching from business logic. Espresso is a document-oriented distributed data serving platform that has been built to address LinkedIn’s requirements for a scalable, performant, source-of-truth primary store. It provides a hierarchical document model, transactional support for modifications to related documents, real- time secondary indexing, on-the-fly schema evolution and provides a timeline consistent change capture stream. 
This talk describes the motivation and design principles involved in building Espresso, its architecture and presents a set of experimental results that characterize the performance of the system along various dimensions.         
             Speaker Bio
Swaroop Jagadish is a member of the Data Infrastructure team at
Linkedin, where he works on distributed data systems
such as Databus, Helix and Espresso. Prior to that, he worked at Yahoo where he 
built one of the first real-time bidding engines in the display-ads industry. 
He holds B.E. from BMS College of Engineering and M.S. from University of California, Santa Barbara.
Kapil Surlaker is a member of the Data Infrastructure team at
Linkedin, where he works on distributed data systems
such as Databus, Helix and Espresso. Prior to that, he worked at
Kickfire (acquired by Teradata) where he built high-performance
Database systems. Earlier in his career, he worked on replication
technology at Oracle where he was part of the team that built Oracle
Streams. He holds B.Tech. (CS) from IIT Bombay and M.S. From University
of Minnesota.
             Feb 8, 2013SPEAKER: Ronen Vaisenberg (Google)Practice Talk: Scheduling Sensors for Monitoring Sentient Spaces using an Approximate POMDP Policy (Percom2013)DetailsDate and TimeFeb 8, 2013 3 pmLocationDBH 3011SpeakerRonen Vaisenberg (Google)TitlePractice Talk: Scheduling Sensors for Monitoring Sentient Spaces using an Approximate POMDP Policy (Percom2013)AbstractWe present a framework for sensor actuation and control in sentient spaces, in which sensors are used to observe a physical phenomena. Our framework utilizes the spatio-temporal statistical properties of an observed phenomena, with the goal of maximizing an application specified reward. Specifically, we define an observation of a phenomena by assigning it a discrete value (state) and we model its semantics as the transition between these values (states). This semantic model is used to predict the future states in which the phenomena is likely to be at, based on partially observed past states. To accomplish real-time agility, we designed an approximate, adaptive-grid solution for POMDPs that yields practically good results, and in some cases, guarantees on the quality of the approximation. We instantiate the framework in a camera network and use it perform real- time actuation of large-scale sensor networks. To the best of our knowledge, we are the first to address the problem of actuating a large scale sensor network based on an approximated POMDP formulation. Our semantic model is simple enough to be implemented in real-time, yet powerful enough to capture meaningful semantics of typical behavior. Our action selection process is as fast as a table lookup in real-time.          
             Speaker Bio
             Feb 15, 2013SPEAKER: Hongzhi Wang (ISG)DetailsDate and TimeFeb 15, 2013 3 pmLocationDBH 3011SpeakerHongzhi Wang (ISG)TitleAbstract          
             Speaker Bio
             Feb. 1, 2013SPEAKER: Marco Sanvido (Pure Storage)The Why and How of an All-Flash Enterprise Storage ArrayDetailsDate and TimeFeb. 1, 2013 3 pmLocationDBH 3011SpeakerMarco Sanvido (Pure Storage)TitleThe Why and How of an All-Flash Enterprise Storage ArrayAbstractEnterprise storage is an $30 billion a year industry dominated by spinning disks. Flash storage is poised to take a large
chunk of the market, having grown significantly in capacity and production, driven by consumer electronics.
Flash's technical advantages over disk promise storage arrays that are faster and easier to use while consuming less power and costing less.

The downsides of flash (inc. large erase blocks, limited overwrites, and higher price) mean that using flash as a drop-in
replacement for disk leads to increased price, volatile performance, and decreased reliability.
In this talk, we describe the design of the Pure FlashArray, an enterprise storage array built around consumer flash storage.
The array and its software, Purity, play to the advantages of flash while minimizing the downsides. Purity writes to flash
in multiples of the erase block size and stores its metadata in a key-value store that minimizes overwrites and stores approximate
answers, trading extra reads for fewer writes. And, Purity reduces data stored on flash through a range of techniques, including
compression, deduplication, and thin provisioning.

The net result is a flash array that deliver a sustained read-write workload of over 100,000 4kb I/O requests per second while
maintaining sub-millisecond latency. With many customers seeing 4x or greater data reduction, the Pure FlashArray ends up
being cheaper than disk too.
             Speaker Bio
Dr. Marco Sanvido holds a Dipl.-Ing. degree (1996) and a Dr.techn. degree (2002) in Computer
Science from the Swiss Federal Institute of Technology in Zürich, Switzerland (ETHZ).
He was a co-founder of weControl, an ETHZ spin-off, where he developed low-power and
real-time embedded systems for autonomous flying vehicles. He was a postdoctoral
researcher in Computer Science at the University of California at Berkeley from 2002 to 2004,
and thereafter he worked on virtualization at VMware. In 2005 he then became a researcher
at Hitachi Global Storage Technologies, where he worked on hard disk drive and solid state drive
architectures. Since 2010 Marco joined Pure Storage as a Principal Software Engineer.
             Jan. 25, 2013SPEAKER: Silvius Rus (Quantcast)Petabyte Scale Data Processing at QuantcastDetailsDate and TimeJan. 25, 2013 3 pmLocationDBH 3011SpeakerSilvius Rus (Quantcast)TitlePetabyte Scale Data Processing at QuantcastAbstractThe talk will present the big data storage, processing and query systems
in production at Quantcast.  We receive up to 50 TB of new data every day,
respond to 500,000 events per second, process up to 30 PB per day and
store tens of petabytes of data.  We have implemented our own MapReduce
software stack that scales better and has significantly lower resource






requirements than Hadoop. The QFS file system is available open source
at https://github.com/quantcast/qfs/wiki.
			Speaker BioSilvius Rus leads Big Data Platforms at Quantcast.  He directs, manages
and participates in the development of cluster language runtimes (SQL,
Sawzall), petabyte scale map-reduce, interactive big data analytics,
cluster resource management, distributed file systems and large scale
realtime processing.  Before Quantcast he was at Google working on Gmail
load balancing across datacenters, parallel memory allocation performance,
server performance and C++ compiler and library optimization.
Silvius holds a PhD in computer science from Texas A and M University,
where he worked on full program optimization based on hybrid
(static and dynamic) analysis of memory reference patterns.Jan. 18, 2013 (Special Time)SPEAKER: Joe Hellerstein (UC Berkeley)Keep CALM and Query OnDetailsDate and TimeJan. 18, 2013 (Special Time) 11 amLocationDBH 6011SpeakerJoe Hellerstein (UC Berkeley)TitleKeep CALM and Query OnAbstract

Any modern software system of note has two key characteristics: it is a distributed system, and it manages significant amounts of data.  As a result, the topic of distributed data consistency has become a key problem in the engineering of modern software systems.  Conventional distributed systems wisdom dictates that perfect consistency is too expensive to guarantee in general, and consistency mechanisms—if they are used at all—should be reserved for infrequent, small-scale, mission-critical tasks.   Like most design maxims, these ideas are not so easy to translate into practice; all kinds of unavoidable tactical questions pop up.  For example:

 • Exactly where in my multifaceted system is loose consistency “good enough” to meet application needs?
 • How do I know that my “mission-critical” software isn’t tainted by my “best effort” components?
 • How do I ensure that my design maxims are maintained as software and developer teams evolve?

Until recently, answers to these questions have been more a matter of folklore than mathematics.

In this talk, I will describe the CALM Theorem, which links Consistency And Logical Monotonicity, and discuss how it can inform distributed software development.  I'll also describe Bloom, a "disorderly" distributed programming language developed in my group.  Bloom admits a form of automated CALM analysis, which enables a compiler to answer questions like the ones above.    

Time permitting, I will also point out some additional results from my two main research projects: the BOOM project on large-scale system development, and the d^p project on human interaction in the data analysis lifecycle.

             Speaker Bio
Joseph M. Hellerstein is a Chancellor's Professor of Computer Science at the University of California, Berkeley, whose research focuses on data-centric systems and the way they drive computing. A Fellow of the ACM, his work has been recognized via awards including an Alfred P. Sloan Research Fellowship, MIT Technology Review's TR10 and TR100 lists, Fortune Magazine's "Smartest in Tech" list, and two ACM-SIGMOD "Test of Time" awards. He has led a number of influential open source projects, including Bloom, MADlib, Telegraph, and TinyDB.

In 2012, Joe co-founded Trifacta, Inc, which develops productivity software for data analysts.
             Dec. 14, 2012SPEAKER: Bijit Hore (UCI ISG)Hide-and-Seek in the cloud: How to securely store and query your data in untrusted environmentsDetailsDate and TimeDec. 14, 2012 3 pmLocationDBH 3011SpeakerBijit Hore (UCI ISG)TitleHide-and-Seek in the cloud: How to securely store and query your data in untrusted environmentsAbstractSecurity and privacy of data is a major concern for organizations (and many individuals) that use cloud-based services to cater to their IT needs. This is cited as the central reason why many federal, healthcare, and financial organizations have  not embraced cloud computing in a major way in spite of its many benefits. In this talk we consider the central problem of "data confidentiality", that arises while storing sensitive data in the cloud. While data encryption is an obvious solution for ensuring confidentiality, standard algorithms like AES make the data unusable in the cloud. For example, keyword search or database queries cannot be evaluated against the encrypted data. Over the past decade, many new schemes have been developed, that admit a variety of computations directly on the encrypted representation.  We give a brief overview of some of the important techniques proposed in this arena, specifically, for evaluating keyword-match and range queries. Finally, we describe our own contributions to this area and conclude with a discussion about open problems and future directions.Speaker Bio
             Dec. 7, 2012SPEAKER: No SeminarDetailsDate and TimeDec. 7, 2012 3 pmLocationSpeakerTitleNo SeminarAbstract
			No ISG seminar.  Leaving this time free so that ISG affiliates can attend today's ICS Trends in Society and Information Technology talk (see www.ics.uci.edu/trends).             Speaker Bio
             Nov. 30, 2012SPEAKER: Peter Bailis (UC Berkeley)Probabilistically Bounded Staleness for Practical Partial QuorumsDetailsDate and TimeNov. 30, 2012 3 pmLocationDBH 3011SpeakerPeter Bailis (UC Berkeley)TitleProbabilistically Bounded Staleness for Practical Partial QuorumsAbstractData store replication results in a fundamental trade-off between operation latency and data consistency. In this talk, we examine this trade-off in the context of quorum-replicated data stores. Under partial, or non-strict quorum replication, a data store waits for responses from a subset of replicas before answering a query, without guaranteeing that read and write replica sets intersect. As deployed in practice, these configurations provide only basic eventual consistency guarantees, with no limit to the recency of data returned. However, anecdotally, partial quorums are often “good enough” for practitioners given their latency benefits.

We explain why partial quorums are regularly acceptable in practice, analyzing both the staleness of data they return and the latency benefits they offer. We introduce Probabilistically Bounded Staleness (PBS) consistency, which provides expected bounds on staleness with respect to both versions and wall clock time. We derive a closed-form solution for versioned staleness as well as model real-time staleness for representative Dynamo-style systems under internet-scale production workloads. Using PBS, we measure the latency-consistency trade-off for partial quorum systems. We quantitatively demonstrate how and why eventually consistent systems frequently return consistent data within tens of milliseconds while offering significant latency benefits.

This is joint work with Shivaram Venkataraman, Mike Franklin, Joe Hellerstein, and Ion Stoica at UC Berkeley. An earlier version of this work appeared at VLDB 2012 (selected for "Best of VLDB 2012"), and an implementation of PBS is slated for release in Cassandra 1.2.0. Demo: http://pbs.cs.berkeley.edu/#demoSpeaker BioPeter Bailis is a graduate student in Computer Science at UC Berkeley, where he works closely with Joe Hellerstein, Ion Stoica, and Ali Ghodsi. He currently studies distributed systems, with a particular focus on distributed consistency models. Peter received his A.B. from Harvard College in 2011, where he worked with Margo Seltzer and Matt Welsh and was a 2011 CRA Outstanding Undergraduate Researcher. He is the recipient of the NSF Graduate Research Fellowship and the Berkeley Fellowship for Graduate Study and is a co-founder of @TinyToCS, the premiere journal for Computer Science research of 140 characters or less.

             Nov. 9, 2012SPEAKER: Chaitan Baru (SDSC at UCSD)Data Initiatives at SDSCDetailsDate and TimeNov. 9, 2012 3 pmLocationDBH 3011SpeakerChaitan Baru (SDSC at UCSD)TitleData Initiatives at SDSCAbstract
			As a data-oriented supercomputer center, SDSC is engaged in a variety of
activities that support data-intensive computing and big data, from
research and development to fielding production systems, and enabling end
applications. This talk will provide an overview of several data
activities at SDSC including the Gordon supercomputer; data intensive
applications on Gordon; the SDSC Cloud, with Globus Online interface for
OpenStack; and new initiatives such as the Center for Large-scale Data
Systems Research (CLDS). We will present two CLDS programs, one on
establishing industry standards for Big Data Benchmarking and another on
Data Growth and Data Value. A new initiative targeted at long-tail
scientific data, motivated partly by needs identified by the NSF EarthCube
initiative and by the challenges faced by a typical research university,
will also be presented. There are many opportunities for joint
collaborations and student projects across these initiatives.
			Speaker Bio
Chaitan Baru, is Associate Director for Data Initiatives at the San
Diego Supercomputer Center Director, UC San Diego, where he also directs
the Center for Large-scale Data Systems research (CLDS). His technical
interests are in the areas of scientific
data management, large-scale data systems, data integration, data
analytics, and parallel database systems. He has been involved in
cyberinfrastructure projects across a range of science disciplines, e.g.
earth sciences, ecological sciences, hydrology, earthquake engineering,
 biomedical sciences, and others. He is PI of the OpenTopography project;
coordinator of the Data Discovery, Mining, and Access community group for
the NSF EarthCube project; and Chair, Coordinating Committee for Big Data
Benchmarking. Before joining SDSC 16 years ago, Baru led one of the
development teams at IBM for DB2 Parallel Edition (a shared-nothing
database engine). Prior to that, he was on the faculty of the EECS Dept,
University of Michigan. Baru has a B.Tech. in Electronics Engineering from
IIT Madras, and an ME and PhD in Electrical Engineering from the
University of Florida, Gainesville.
             Nov. 2, 2012SPEAKER: No SeminarDetailsDate and TimeNov. 2, 2012 3 pmLocationSpeakerTitleNo SeminarAbstract
			No ISG seminar.  Leaving this time free so that ISG affiliates can attend today's ICS Trends in Society and Information Technology talk (see www.ics.uci.edu/trends).             Speaker Bio
             Oct. 26, 2012SPEAKER: Yingyi Bu (ISG PhD student)Pregelix: Think Like a Vertex, Scale like SpandexDetailsDate and TimeOct. 26, 2012 3 pmLocationDBH 3011SpeakerYingyi Bu (ISG PhD student)TitlePregelix: Think Like a Vertex, Scale like SpandexAbstract
Recently, there are more and more demands for analyzing Big Graph Data.  For example,  the scale of the world wide web keeps expanding to billions of web pages and hyper-links, the key social network sites like Facebook, LinkedIn, Twitter all have a rapidly growing gigantic social graph, and the biology science people assemble genomes from huge de Bruijn graphs.  To analyze such Big graphs requires a system which can not only scale out to hundreds or thousands of machines, but also do the computation very efficiently.  In this talk, I will introduce the Pregelix system,  which supports easy programming and scales to large commodity machine clusters. I will first illustrate the programming model -- application programmers need zero knowledge of the parallel/distributed system,  but just "think like a vertex" and write a couple of functions that encapsulate the logic for what one graph vertex does.  After that, I will detail the shining internals of Pregelix,  including the system architecture,  the scalable dataflow runtime,  the execution strategies, and the out-of-core support.  Then,  I will walk through a few examples built on top of Pregelix, such as PageRank and connected components.  Finally I will demonstrate our performance numbers and conclude the talk. (Truth in lending disclosure: the programming model and API were shamelessly borrowed from Google's Pregel graph analytics platform, hence the name:-))
             Speaker Bio
 Yingyi Bu is a PhD student in the ISG group of UC Irvine.  He is working on the ASTERIX project that aims at an open source data-intensive computing platform, with new technologies for ingesting, storing, managing, indexing, querying, analyzing, and subscribing intensive semi-structured data.  Within the project, Yingyi has been working on the data-model independent algebra/optimization layer,  the ASTERIX query optimizer, and the Pregelix system.
             Oct. 19, 2012SPEAKER: David Lomet (Microsoft Research)The Bw-Tree: A B-tree for New Hardware PlatformsDetailsDate and TimeOct. 19, 2012 3 pmLocationDBH 3011SpeakerDavid Lomet (Microsoft Research)TitleThe Bw-Tree: A B-tree for New Hardware PlatformsAbstractThe emergence of new hardware and platforms has led to reconsideration of how data management 
            	systems are designed. However, certain basic functions such as key indexed access to records remain essential. 
            	While we exploit the common architectural layering of prior systems, we make radically new design decisions about each layer. 
            	Our new form of B-tree, called the Bw-tree achieves its very high performance via a latch-free approach that effectively 
            	exploits the processor caches of modern multi-core chips. Our storage manager uses a unique form of log structuring that 
            	blurs the distinction between a page and a record store and works well with flash storage. This paper describes the architecture 
            	and algorithms for the Bw-tree, focusing on the main memory aspects. The paper includes results of our experiments that demonstrate that this fresh approach produces outstanding performance.
            Speaker Bio
            	David Lomet has been a principal researcher managing the Microsoft Research Database Group at Microsoft Research since 1995. Earlier, he spent seven and a half years at Digital Equipment Corporation. He has been at IBM Research in Yorktown and a Professor at Wang Institute. Dr. Lomet spent a sabbatical at University of Newcastle-upon-Tyne working with Brian Randell. He has a Computer Science Ph.D from the University of Pennsylvania.
Dr. Lomet has done research and product development in architecture, programming languages, and distributed systems. His primary interest is database systems, focusing on access methods, concurrency control, and recovery. He is one of the inventors of the transaction concept and is an author of over 100 papers and 45 patents. Two papers won SIGMOD "best paper" awards. He received the 2010 SIGMOD Contributions Award for his work as editor-in-chief of the Data Engineering Bulletin since 1992.
Dr. Lomet has served on program committees, including SIGMOD, PODS, VLDB, and ICDE. He was ICDE'2000 PC co-chair and VLDB 2006 PC core chair. He is a member of the ICDE Steering Committee and VLDB Board. He is a past editor of ACM TODS and the VLDB Journal. Dr. Lomet is IEEE Golden Core Member and has received IEEE Outstanding Contribution and Meritorious Service Awards. Dr. Lomet is a Fellow of the ACM, IEEE, and AAAS.
            Oct. 5, 2012SPEAKER: Thomas Bodner (TU Berlin)A Taxonomy of Platforms for Analytics on Big Data (Stratosphere talk series 5)DetailsDate and TimeOct. 5, 2012 4:30 pm - 5pmLocationDBH 3011SpeakerThomas Bodner (TU Berlin)TitleA Taxonomy of Platforms for Analytics on Big Data (Stratosphere talk series 5)Abstract
Within the past few years, industrial and academic organizations designed a wealth of systems for data-intensive analytics including MapReduce, SCOPE/Dryad, ASTERIX, Stratosphere, Spark, and many others. These systems are being applied to new applications from diverse domains other than (traditional) relational OLAP, making it difficult to understand the tradeoffs between them and the workloads for which they were built. We present a taxonomy of existing system stacks based on their architectural components and the design choices made related to data processing and programmability to sort this space. We further demonstrate a web repository for sharing Big Data analytics platform information and use cases. The repository enables researchers and practitioners to store and retrieve data and queries for their use case, and to easily reproduce experiments from others on different platforms, simplifying comparisons.
             Speaker Bio
Thomas Bodner is a second year Master's student in the computer science department at the Technische Universität Berlin working in the Database Systems and Information Management (DIMA) group on the Stratosphere project. He received his B.S. from the University of Cooperative Education at Stuttgart. In the course of his studies, Thomas Bodner studied abroad at University of California, Irvine and Royal Melbourne Institute of Technology. He worked as an intern at the IBM Almaden Research Center and the IBM Böblingen Laboratory. His research interests include benchmarking of and query optimization for Big Data analytics systems.
             Oct. 5, 2012SPEAKER: Alexander AlexandrovGenerating a Myriad of Atoms in the Blink of an Eye (Stratosphere talk series 4)DetailsDate and TimeOct. 5, 2012 4 pm - 4:30pmLocationDBH 3011SpeakerAlexander AlexandrovTitleGenerating a Myriad of Atoms in the Blink of an Eye (Stratosphere talk series 4)Abstract
 Data from real-world applications is regarded as the golden standard for database systems evaluation. Unfortunately, finding appropriate real-world datasets is often hard due to various privacy-related constraints. To overcome this problem, we developed the Myriad Parallel Data Generator Toolkit - a generic toolkit for declarative specification of synthetic data generators that provides built-in parallelization support for the specified data generation programs. In this talk, I will motivate and present the main technical challenges solved by the highly-parallel execution model of the Myriad Toolkit. In addition, to demonstrate the usability of the toolkit, I will also give a brief overview of the supported data generator specification syntax and explain how different statistical constraints for the generated data can be implemented using the appropriate combination of specification routines.
             Speaker Bio
Alexander Alexandrov is a research associate at the Database Systems and Information Management research group at the Technische Universität Berlin. Before moving to Berlin for a Master in Computer Science at TU Berlin, he received his Bachelor of Science in Software and Internet Technologies at the University of Mannheim. Alexander has been working on the Stratosphere project both as student and research assistant since 2009. His research interests include data generation, evaluation, and query optimization for large-scale parallel batch processing systems with partial operator semantics.
             Oct. 5, 2012SPEAKER: Stephan Ewen (TU Berlin)Spinning Fast Iterative Data Flows (Stratosphere talk series 3)DetailsDate and TimeOct. 5, 2012 3:30 pm - 4pmLocationDBH 3011SpeakerStephan Ewen (TU Berlin)TitleSpinning Fast Iterative Data Flows (Stratosphere talk series 3)Abstract
 Parallel data flow systems are a central part of most analytic pipelines for big data. The iterative nature of many analysis and machine learning algorithms, however, is still a challenge for current systems. While certain types of bulk iterative algorithms are supported by novel data flow frameworks, these systems cannot exploit computational dependencies present in many algorithms, such as graph algorithms. As a result, these algorithms are inefficiently executed and have led to specialized systems based on other paradigms, such as message passing or shared memory. We propose a method to integrate "incremental iterations", a form of workset iterations, with parallel data flows. After showing how to integrate bulk iterations into a dataflow system and its optimizer, we present an extension to the programming model for incremental iterations. The extension alleviates for the lack of mutable state in dataflows and allows for exploiting the "sparse computational dependencies" inherent in many iterative algorithms. The evaluation of a prototypical implementation shows that those aspects lead to up to two orders of magnitude speedup in algorithm runtime, when exploited. In our experiments, the improved dataflow system is highly competitive with specialized systems while maintaining a transparent and unified data flow abstraction.
             Speaker Bio
Stephan Ewen is a research associate at the department for Database Systems and Information Management (DIMA) at the Technische Universität Berlin. He is working on the Stratosphere Project that aims at creating a versatile and efficient analytics engine for deep analysis of Big Data on cloud platforms. Within the project, Stephan works on the system's data flow programming abstraction, the data flow optimization and the parallel runtime system. Prior to joining the DIMA group, Stephan completed the "Applied Computer Science" program at the University of Cooperative Education Stuttgart jointly with IBM Germany and got his Diploma from the University of Stuttgart. In the course of his studies, Stephan Ewen worked, among others, for the IBM Almaden Research Centre and the IBM Development Laboratory Böblingen.
             Oct. 5, 2012SPEAKER: Kostas Tzoumas (TU Berlin)Query Optimization with MapReduce Functions (Stratosphere talk series 2)DetailsDate and TimeOct. 5, 2012 3 pm - 3:30 pmLocationDBH 3011SpeakerKostas Tzoumas (TU Berlin)TitleQuery Optimization with MapReduce Functions (Stratosphere talk series 2)Abstract
Many systems for big data analytics employ a data flow programming abstraction to define parallel data processing tasks. In this setting, custom operations expressed as user-defined functions are very common. We address the problem of performing data flow optimization at this level of abstraction, where the semantics of operators are not known. Traditionally, query optimization is applied to queries with known algebraic semantics. In this work, we find that a handful of properties, rather than a full algebraic specification, suffice to establish reordering conditions for data processing operators. We show that these properties can be accurately estimated for black box operators using a shallow static code analysis pass based on reverse data and control flow analysis over the general-purpose code of their user-defined functions. We design and implement an optimizer for parallel data flows that does not assume knowledge of semantics or algebraic properties of operators. Our evaluation confirms that the optimizer can apply common rewritings such as selection reordering, bushy join order enumeration, and limited forms of aggregation push-down, hence yielding similar rewriting power as modern relational DBMS optimizers. Moreover, it can optimize the operator order of non-relational data flows, a unique feature among today's systems.
             Speaker Bio
 Kostas Tzoumas is a postdoctoral researcher co-leading the Stratosphere research project at the Technische Universität Berlin. He received his PhD from Aalborg University in 2011 with a thesis on discovering and exploiting correlations for query optimization. He was a visiting researcher at the University of Maryland, College Park, and an intern at Microsoft Research. He received a Diploma in Electrical and Computer Engineering from the National Technical University of Athens in 2007. His research interests are centered around systems for data analytics, including query processing and optimization in massively parallel environments.
             Oct. 5, 2012SPEAKER: Volker Markl (TU Berlin)The Current State of the Stratosphere (Stratosphere talk series 1)DetailsDate and TimeOct. 5, 2012 3 pm - 5pmLocationDBH 3011SpeakerVolker Markl (TU Berlin)TitleThe Current State of the Stratosphere (Stratosphere talk series 1)Abstract
            Introduction to the Stratosphere system.
			Speaker Bio
            Volker Markl is a Full Professor and Chair of the Database Systems and Information Management (DIMA) group at the Technische Universität Berlin (TU-Berlin). Prior to joining TU-Berlin, Dr. Markl lead a research group at FORWISS, the Bavarian Research Center for Knowledge-based Systems in Munich, Germany, and was a Research Staff member and Project Leader at the IBM Almaden Research Center in San Jose, California, USA. His research interests include: information as a service, new hardware architectures for information management, information integration, autonomic computing, query processing, query optimization, data warehousing, electronic commerce, and pervasive computing. Volker has presented over 100 invited talks in numerous industrial settings and at major conferences and research institutions worldwide. He has authored and published more than 50 research papers at world-class scientific venues. Volker regularly serves as member and chair for program committees of major international database conferences. He also is a member of the Board of Trustees of the VLDB Endowment. Volker has 5 patent awards, and he has submitted over 20 invention disclosures to date. Over the course of his career, he has garnered many prestigious awards, including the European Information Society and Technology Prize, an IBM Outstanding Technological Achievement Award, an IBM Shared University Research Grant, an HP Open Innovation Award, and the Pat Goldberg Memorial Best Paper Award.
            Jun. 8, 2012SPEAKER:  Kerim Yasin Oktay and Bijit Hore (ISG)CloudProtecti and Risk-Aware Workload Distribution in Hybrid CloudsDetailsDate and TimeJun. 8, 2012 3 pmLocationDBH 3011Speaker Kerim Yasin Oktay and Bijit Hore (ISG)TitleCloudProtecti and Risk-Aware Workload Distribution in Hybrid CloudsAbstract
            In this talk, we describe the CloudProtect system from the recently accepted paper in IEEE Cloud 2012. The CloudProtect middleware empowers users to encrypt sensitive data stored within various cloud applications. However, most web applications require data in plaintext for implementing the various functionalities and in general, do not support encrypted data management. Therefore, CloudProtect strives to carry out the data transformations (encryption/decryption) in a manner that is transparent to the application, i.e., preserves all functionalities of the application, including those that require data to be in plaintext. Additionally, CloudProtect allows users flexibility in trading off performance for security in order to let them optimally balance their privacy needs and usage-experience.
            This paper explores an efficient and secure mechanism
            to partition computations across public and private
            machines in a hybrid cloud setting. We propose a principled
            framework for distributing data and processing in a hybrid
            cloud that meets the conflicting goals of performance, sensitive
            data disclosure risk and resource allocation costs. The proposed
            solution is implemented as an add-on tool for a Hadoop and
            Hive based cloud computing infrastructure. Our experiments
            demonstrate that the developed mechanism can lead to a
            major performance gain by exploiting both the hybrid cloud
            components without violating any pre-determined public cloud
            usage constraints.
            Jun. 1, 2012SPEAKER: Ken Slocum (UCSD)Scalable Lineage Capture for DISCDetailsDate and TimeJun. 1, 2012 3 pmLocationDBH 3011SpeakerKen Slocum (UCSD)TitleScalable Lineage Capture for DISCAbstract
            	Scale-out data processing architectures enable sophisticated ``big data''
analytics, but understanding and debugging multi-step dataflows that ingest
large volumes of data remains a fundamental challenge.  We are building a
system called Newt, a scalable architecture for capturing fine-grain,
record-level provenance from these data-intensive scalable compute (DISC)
systems in a generic manner.  Developers leverage a unique API to
instrument these systems, actively capturing fine-grain lineage across
multi-step, perhaps non-relational, transformations.   We report on our 
experiences instrumenting Hyracks and Hadoop, and find that Newt's capture 
incurs 16-26% time overheads for the PigMix benchmark and a 14% overhead 
on a complex 145-stage de novo genomic assembler.
            Speaker Bio
            	Ken Yocum is an associate research scientist in the Department of Computer
Science at UC San Diego where he runs the Synoptic Systems Lab.  
While he once worked on high-speed networking (briefly holding the land-speed record for gigabit TCP), he has since become enamored with the myriad systems challenges of "big data" processing and software-defined networks.  He received his Ph.D. from Duke University, and
his B.S from Stanford.  When he's not working, he enjoys his children,
cycling, and going to the race track.
            May. 29, 2012 (Special Time)SPEAKER: Murali Mani (University of Michigan, Flint) Algebraic Manipulation of Encrypted DatabasesDetailsDate and TimeMay. 29, 2012 (Special Time) 12 pmLocationDBH 3011SpeakerMurali Mani (University of Michigan, Flint)Title Algebraic Manipulation of Encrypted DatabasesAbstract
            	Can we improve on the work that received the 10 year ACM SIGMOD test of time award? In this talk, we will outline our preliminary approach at doing the entire query processing on the server/cloud, while the client is involved only with encryption, and decryption. Our work is based on Craig Gentry's revolutionary recent work on fully homomorphic encryption (first such scheme was published in 2009). We utilize Craig Gentry's scheme for query processing, while maintaining the algebraic framework that is a key aspect of database systems. There are several avenues for future investigation: exploring physical implementations for algebraic operators beyond what we have investigated; exploring query optimization and utilization of indexes; exploring feasibility of Craig Gentry's fully homomorphic encryption in the context of databases as some aspects of his scheme are very time consuming.
            Speaker Bio
            	Murali Mani finished his PhD in Computer Science from UCLA in 2003. Since then, he has worked at WPI, and is currently an assistant professor at University of Michigan, Flint. His areas of interest are database systems, and his significant projects have been on event stream processing, processing of XML streaming data, provenance metadata management, and data modeling using XML schemas. His research on XML stream processing and provenance have been supported by NSF.
            May. 25, 2012SPEAKER: Jimmy Lin (Twitter)Flexibility without Anarchy: Analytics Infrastructure at TwitterDetailsDate and TimeMay. 25, 2012 3 pmLocationDBH 3011SpeakerJimmy Lin (Twitter)TitleFlexibility without Anarchy: Analytics Infrastructure at TwitterAbstractThe data analytics infrastructure at Twitter supports a myriad of technologies: Hadoop, Pig (with Python and JRuby), 
            	Cascading/Scalding, HBase, MySQL, Vertica, and ZooKeeper. Our philosophy is to let developers and data scientists use whatever 
            	tools they are most comfortable with, while allowing individual components to be weaved together into complex analytic tapestries. 
            	Managing complex workflows that cross language boundaries (e.g. Java vs. Pig vs. Scala) as well as architectures with significant 
            	impedance mismatches (e.g., Hadoop vs. Vertica) has been and continues to remain a significant challenge. 
            	In this talk, I'll detail some of these issues and our present solutions.
            Speaker Bio
            	 Jimmy Lin is a visiting scientist at Twitter, currently on leave from the University of Maryland. His current research focuses on scalable algorithms for data analytics, particularly on text and graph data. At Twitter, he works on services designed to surface relevant content for users and the distributed infrastructure that supports mining relevance signals from massive amounts of data.
            May. 18, 2012SPEAKER: Raman Grover (ISG Ph.D. student)ASTERIX: Scalable Warehouse-Style Web Data IntegrationDetailsDate and TimeMay. 18, 2012 3 pmLocationDBH 3011SpeakerRaman Grover (ISG Ph.D. student)TitleASTERIX: Scalable Warehouse-Style Web Data IntegrationAbstract
            		A growing wealth of digital information is being generated on a
daily basis in social networks, blogs, online communities, etc. Organizations
and researchers in a wide variety of domains recognize
that there is tremendous value and insight to be gained by
warehousing this emerging data and making it available for querying,
analysis, and other purposes. This new breed of “Big Data”
applications poses challenging requirements against data management
platforms in terms of scalability, flexibility, manageability,
and analysis capabilities. At UC Irvine, we are building a nextgeneration
database system, called ASTERIX, in response to these
trends. We present ongoing work that approaches the following
questions: How does data get into the system? What primitives
should we provide to better cope with dirty/noisy data? How can
we support efficient data analysis on spatial data? Using real examples,
we show the capabilities of ASTERIX for ingesting data via
feeds, supporting set-similarity predicates for fuzzy matching, and
answering spatial aggregation queries.
            Speaker Bio
            May. 11, 2012SPEAKER: Inci Cetindil (ISG Ph.D. student)Analysis of Instant Search Query LogsDetailsDate and TimeMay. 11, 2012 3 pmLocationDBH 3011SpeakerInci Cetindil (ISG Ph.D. student)TitleAnalysis of Instant Search Query LogsAbstractInstant search is a new search paradigm that shows results as
a user types in a query. It has become increasingly popular
in recent years due to its simplicity and power. In an instant-
search system, every keystroke from a user triggers a new
request to the server. Therefore, its log has a richer content
than that of a traditional search system, and previous log
analysis research is not applicable to this type of log. In
this study, we present the problem of analyzing the query
log of an instant-search system. We propose a classification
scheme for user typing behaviors. We also compare the log
of an instant-search system and that of a traditional search
system on the same data. The results show that on a people
directory search system, instant search can typically save
2 seconds per search, reduce the typing effort by showing
the results with fewer characters entered, and increase the
success rate.
            Speaker Bio
            April. 27, 2012SPEAKER: Afsin Akdogan (University of Southern California)Voronoi-based Geospatial Query Processing with MapReduceDetailsDate and TimeApril. 27, 2012 3 pmLocationDBH 3011SpeakerAfsin Akdogan (University of Southern California)TitleVoronoi-based Geospatial Query Processing with MapReduceAbstract
            		Geospatial queries (GQ) have been used in a wide variety of applications such as decision support systems, profile-based marketing, bioinformatics and GIS. Most of the existing query-answering approaches assume non parallel processing on a single machine although GQs are intrinsically parallelizable. There are some approaches that have been designed for parallel databases and cluster systems; however, these only apply to the systems with limited parallel processing capability, far from that of cloud-based platforms. In this study, I present the problem of parallel geospatial query processing with MapReduce programming model.  Our approach creates a spatial index, Voronoi diagram, for given data points in 2D space and enables efficient processing of GQs. We evaluated the performance of our proposed techniques and correspondingly compared them with their closest related work while varying the number of employed nodes.
        		Speaker BioAfsin Akdogan received his master’s degree in computer science from Cornell University in 2009. He received a best paper award in IEEE Cloud Computing Technology and Science conference in 2010. He has also interned at Yahoo. He is currently working towards his Ph.D. degree in computer science at the University of Southern California and his research focuses on cloud computing, parallel data processing languages and geo-spatial databases.April. 20, 2012SPEAKER: Leila Jalali (Ph.D. student in ISG)A Reflective Approach to Synchronization for Consistent MultisimulationsDetailsDate and TimeApril. 20, 2012 3 pmLocationDBH 3011SpeakerLeila Jalali (Ph.D. student in ISG)TitleA Reflective Approach to Synchronization for Consistent MultisimulationsAbstractIn this talk, I consider the challenge of designing a framework that supports the integration of multiple existing autonomous simulation models into an integrated simulation environment (multisimulation). In particular, I focus on solutions for synchronization problem in multisimulation to orchestrate consistent information flow through multiple simulator: (1) a transaction-based approach to modeling the synchronization problem in multisimulations by mapping it to a problem similar to multidatabase concurrency; we express multisimulation synchronization as a scheduling problem where the goal is to generate “correct schedules” for time advancement and data exchange across simulators that meets the dependencies without loss of concurrency, (2) a hybrid scheduling strategy which adapts itself to the “right” level of pessimism/optimism based on the state of the execution and underlying dependencies, and (3) relaxation model for dependencies which guarantee bounded violation of consistency to support higher levels of concurrency. We also develop two key optimizations: (a) efficient checkpointing/rollback techniques, and (b) relaxation model for dependencies which guarantee bounded violation of consistency to support higher levels of concurrency. We evaluate our proposed techniques via a detailed case study from the emergency response domain by integrating three disparate simulators – a fire simulator (CFAST), an evacuation simulator (Drillsim) and a communication simulator (LTEsim).
						April. 13, 2012 (Special Time\Place)SPEAKER: Jennifer Widom (Stanford)Data-Centric Human Computation + From 100 Students to 100,000DetailsDate and TimeApril. 13, 2012 (Special Time\Place) 11 amLocationDBH 6011SpeakerJennifer Widom (Stanford)TitleData-Centric Human Computation + From 100 Students to 100,000AbstractThis talk will have two completely independent parts -- one related to research and the other to education.
In the first part of the talk, I'll describe our ongoing research in leveraging human computation for tasks related to data. Human computation ("crowdsourcing") augments traditional computation with the use of human abilities to solve sub-problems that are difficult for computers, e.g., object or image comparisons, information extraction, relevance judgements, and data gathering. We are addressing two different types of data-centric human computation: (1) Fundamental algorithms, such as sorting, clustering, and data cleaning, in which the basic operations (e.g., compare, filter) are performed by humans. (2) A database-system like platform in which declarative queries are posed by users, and the system orchestrates a combination of stored and crowdsourced data to answer them. Common to both areas is the need to formalize and optimize new tradeoffs among latency (humans are much slower than computers), cost (humans require real money to perform tasks), and quality (humans are inaccurate and inconsistent).
In the second part of the talk, I'll describe my recent experience teaching introductory databases to 60,000 students. Admittedly only 25,000 of them submitted their homework, and a mere 6500 achieved a strong final score. But even with 6500 students, I more than quadrupled the total number of students I've taught in my entire 18-year academic career. I began by "flipping" the way I teach my Stanford course and, as a side-effect, making all components of the course freely available online. But the big inflection point came when I offered the online course in a structured fashion with a schedule, automatically-graded assignments and exams, and most importantly a worldwide community of students. I'll cover a variety of topics related to the massive online course, both logistical and social, while avoiding speculation on the future of higher education.
Speaker BioJennifer Widom is the Fletcher Jones Professor and Chair of the Computer Science Department at Stanford University. She received her Bachelor's degree from the Indiana University School of Music in 1982 and her Computer Science Ph.D. from Cornell University in 1987. She was a Research Staff Member at the IBM Almaden Research Center before joining the Stanford faculty in 1993. Her research interests span many aspects of nontraditional data management. She is an ACM Fellow and a member of the National Academy of Engineering and the American Academy of Arts and Sciences; she received the ACM SIGMOD Edgar F. Codd Innovations Award in 2007 and was a Guggenheim Fellow in 2000; she has served on a variety of program committees, advisory boards, and editorial boards.March. 16, 2012 (Special Time\Place)SPEAKER: Cyrus Shahabi (USC)TransDec:
A Data-Driven Framework for Decision-Making in Transportation SystemsDetailsDate and TimeMarch. 16, 2012 (Special Time\Place) 11 amLocationDBH 6011SpeakerCyrus Shahabi (USC)TitleTransDec:
A Data-Driven Framework for Decision-Making in Transportation SystemsAbstractThe vast amounts of transportation datasets (traffic flow, incidents, etc.) collected by various federal and
state agencies are extremely valuable in 1) real-time decision-making, planning, and management of the
transportation systems, and 2) conducting research to develop new policies to enhance the efficacy of the
transportation systems. In this talk, I will present our data-driven framework, dubbed TransDec (short
for Transportation Decision-Making), which enables real-time integration, visualization, querying, and
analysis of dynamic and archived transportation data. I will show that considering the large size of the
transportation data, variety of the data (different modalities and resolutions), and frequent changes of the
data, implementation of such a scalable system that allows for effective querying and analysis of both
archived and real-time data is an intrinsically challenging data management task. Subsequently, I will
focus on a route-planning problem where the weights on the road-network edges vary as a function of
time due to the variability of traffic congestion. I will show that naïve approaches to address this problem
are either inaccurate or slow, motivating the need for new solutions. Consequently, I will discuss our
initial approach to this problem and demonstrate its implementation within the TransDec framework.Speaker BioCyrus Shahabi is a Professor and the Director of the Information Laboratory (InfoLAB) at the Computer
Science Department and also the Director of the NSF's Integrated Media
Systems Center (IMSC) at the University of Southern California. He is also the
CTO and co-founder of a USC spin-off, Geosemble Technologies. He
received his B.S. in Computer Engineering from Sharif University of
Technology in 1989 and then his M.S. and Ph.D. Degrees in Computer
Science from the University of Southern California in May 1993 and
August 1996, respectively. He authored two books and more than hundred-
fifty research papers in the areas of databases, GIS and multimedia. Dr. Shahabi has received funding from several agencies such as NIJ, NSF, NASA, NIH, DARPA, AFRL,
and DHS as well as several industries such as Google, Microsoft, NCR, NGC, and Chevron. He was an
Associate Editor of IEEE Transactions on Parallel and Distributed Systems (TPDS) from 2004 to 2009.
He is currently on the editorial board of the VLDB Journal, IEEE Transactions on Knowledge and Data
Engineering (TKDE), ACM Computers in Entertainment and Journal of Spatial Information Science. He
is the founding chair of IEEE NetDB workshop and also the general co-chair of ACM GIS 2007, 2008
and 2009. He chaired the nomination committee of ACM SIGSPATIAL for the 2011-2014 terms. He
regularly serves on the program committee of major conferences such as VLDB, ACM SIGMOD, IEEE
ICDE, ACM SIGKDD, and ACM Multimedia. Dr. Shahabi is a recipient of the ACM Distinguished
Scientist award in 2009, the 2003 U.S. Presidential Early Career Awards for Scientists and Engineers
(PECASE), the NSF CAREER award in 2002, and the 2001 Okawa Foundation Research Grant for
Information and Telecommunications. He was the recipient of US Vietnam Education Foundation (VEF)
faculty fellowship award in 2011, an organizer of the 2011 National Academy of Engineering “Japan-
America Frontiers of Engineering” program, an invited speaker in the 2010 National Research Council
(of the National Academies) Committee on New Research Directions for the National Geospatial-
Intelligence Agency, and a participant in the 2005 National Academy of Engineering “Frontiers of
Engineering” program.March. 9, 2012SPEAKER: Nga Dang (Ph.D. student in ISG)QuARES: A Quality-Aware Renewable Energy-driven Sensing FrameworkDetailsDate and TimeMarch. 9, 2012 3 pmLocationDBH 3011SpeakerNga Dang (Ph.D. student in ISG)TitleQuARES: A Quality-Aware Renewable Energy-driven Sensing FrameworkAbstract Mobile devices, such as smartphones and tablets, are getting increasingly popular, and continue to generate record-high amount of mobile data traffic. For example a recent Cisco report indicates that mobile data traffic will increase 39 times by 2015, while 66% of such boost is due to video traffic. Network capacity issue may be partially coped by deploying more cellular base stations, installing dedicated broadcast networks, or upgrading the cellular base stations to support 4G. However, these approaches all result in additional costs on new network infrastructure, and might not be fully compatible with existing
obile devices. Also, according to the report, the network capacity provided by cellular network providers is predicted to be only 10 time increasing by 2015, which implies that the above methods do not still meet the requirement for increasing mobile traffic. A better way is moving data to other networks to reduce heavy traffic in cellular networks. In our research, we study motivations and methods to offload part of mobile traffic from cellular networks to other networks such as WiFi or Ad Hoc, which are available in most modern smartphones. Such these methods are cheap, practical, and easily implemented.March. 1, 2012 (Special Time\Place)SPEAKER: Archan Misra (Singapore Management University) Real-time Mobile Sensing/Analytics and the LiveLabs Experimentation PlatformDetailsDate and TimeMarch. 1, 2012 (Special Time\Place) 11 amLocationDBH 4011SpeakerArchan Misra (Singapore Management University) TitleReal-time Mobile Sensing/Analytics and the LiveLabs Experimentation PlatformAbstract
            This talk explores the ongoing transformation of the mobile device into a combined “sensing and
analytics” platform, distinguished by two key features: a) efficient localized processing of sensor data
streams and b) localized coordination and distributed computation among a set of proximal mobile
nodes. I will first introduce the LiveLabs Experimentation Platform, a unique “urban behavioral testbed”
that combines innovations in wireless networks, mobile sensing and App deployment to enable an
ecosystem of industry partners to test next-generation context-based applications on approx. 30,000 real-
life users in urban environments, such as the SMU campus, 2 major shopping malls and a resort theme
park. I will then describe ongoing research on offline and near-real time energy-efficient, continuous
smartphone-based human context estimation or “activity mining”, with a special focus on how such
analytics can utilize proximity-driven social interactions. I will then briefly cover two ongoing projects
that exploit such context-sensing to: a) optimize the delivery of mobile advertising and b) perform real-
time adaptation of femtocellular indoor networks.
            Feb. 17, 2012SPEAKER: Russell Sears (Yahoo! Research)A general purpose Log Structured Merge TreeDetailsDate and TimeFeb. 17, 2012 3 pmLocationDBH 3011SpeakerRussell Sears (Yahoo! Research)TitleA general purpose Log Structured Merge TreeAbstract
            Data management workloads are increasingly write-intensive and subject
to strict latency SLAs.  This presents a dilemma: Traditional update
in place systems have unmatched latency properties but poor write
throughput.  In contrast, existing log structured techniques
significantly improve write throughput but generally sacrifice read
performance and exhibit unacceptable latency spikes.

We begin by presenting a new performance metric: read fanout, and
argue that, along with read amplification and write amplification, it
better characterizes the real-world performance of index algorithms than
existing approaches such as asymptotic analysis and price/performance.

We then present a Log Structured Merge (LSM) tree implementation that
combines the best properties of B-Trees and log structured approaches:
(1) Unlike existing log structured trees, our implementation has
near-optimal read and scan performance, and (2) we present merge
algorithms that bound write latencies without impacting write
throughput or allowing merges to block application writes for extended
periods of time.  We do this by introducing a new ``spring and gear''
scheduler that ensures merges at each level of the tree make steady
progress.  This allows us to avoid blocking application writes without
resorting to techniques that degrade read performance.

We use Bloom filters to improve index performance, and find that a
number of subtleties arise.  First, it is important to ensure that
reads can safely stop after finding the first version of a record.
Otherwise, frequently written items will incur multiple disk
lookups.  Second, many applications and data management architectures
check for preexisting values at insertion time.  Avoiding the disk
seek performed by the check is crucial for such applications.

This work will appear in Sigmod 2012.
			Feb. 10, 2012 (Special Time\Place)SPEAKER: Anhai Doan (U.  Wisconsin and Walmart Labs - ex Kosmix)Social Media, Data Integration, and Human ComputationDetailsDate and TimeFeb. 10, 2012 (Special Time\Place) 11 amLocationDBH 6011SpeakerAnhai Doan (U.  Wisconsin and Walmart Labs - ex Kosmix)TitleSocial Media, Data Integration, and Human ComputationAbstractSocial media has emerged as a major frontier on the World-Wide Web, with applications ranging from helping teenagers track Justin Bieber to e-commerce to fostering revolutions. In this talk I will discuss our work in this area, as carried out at Wisconsin, Kosmix, and @WalmartLabs. I describe how we integrate data from 'traditional' Web sources to build a global taxonomy, greatly expand it with social-media data, then leverage it to build consumer-facing applications. Example applications include building topic pages, detecting Twitter events, and monitoring these events. I discuss the critical role of data integration and human computation in processing social media. Finally, I discuss how all of these can help the emerging area of social commerce, and why Walmart recently acquired Kosmix to make inroads into this new and exciting area.Speaker BioAnHai Doan is an Associate Professor at the University of Wisconsin-Madison. His interests cover databases, AI, and Web, with a current focus on data integration, large-scale knowledge bases, social media, crowdsourcing, human computation, and information extraction. He received the ACM Doctoral Dissertation Award in 2003, a CAREER Award in 2004, and a Sloan Fellowship in 2007. AnHai was Chief Scientist of Kosmix, a social media startup acquired by Walmart in 2011. Currently he also works as Chief Scientist of @WalmartLabs, a research and development lab devoted to integrating social and mobile data for e-commerce.Feb. 3, 2012 (Special Time\Place)SPEAKER: Yannis Papakonstantinou (UCSD)Declarative, optimizable data-driven specifications of web and mobile applicationsDetailsDate and TimeFeb. 3, 2012 (Special Time\Place) 11 amLocationDBH 6011SpeakerYannis Papakonstantinou (UCSD)TitleDeclarative, optimizable data-driven specifications of web and mobile applicationsAbstractDevelopers of web and mobile application development write too much low level "plumbing" code to efficiently access, integrate and coordinate application state that resides on multiple sub-systems of the architecture, and is accessed using different languages: SQL at the database server; HTML and Javascript at the browser, which in HTML5 includes its own database state; Java or other programming languages at the application server.
The FORWARD project replaces such low level code with declarative specifications. Its cornerstones are 
(i) the unified application state virtual database, which enables modeling and manipulating the entire application state in an extension of SQL, named SQL++ 
(ii) specification of Ajax pages as essentially rendered views over the unified application state.
Consequently the following three problems are resolved by appropriate reduction to data management problems, where prior database research literature is leveraged and extended.
1. The partial change of Ajax pages, in response to application state changes, is reduced to an incremental view maintenance problem. Id's that retain the provenance of the page data play an instrumental efficiency role.
2. Efficient data access is reduced to semistructured query processing over an integrated view that involves large database(s) and small main memory-based sources.
3. The inherent location transparency of the specifications is exploited in order to perform computation at the appropriate location (browser vs server). More broadly, the talk discusses ongoing and future work in utilizing the increased abilities of HTML5 clients towards achieving low latency mobile web applications applications, while location transparency of the specifications is retained.Speaker Bio
Yannis Papakonstantinou is a Professor of Computer Science and Engineering at the University of California, San Diego. His research is in the intersection of data management technologies and the web, where he has published over eighty research articles. He has given multiple tutorials and invited talks, has served on journal editorial boards and has chaired and participated in program committees for many international conferences and workshops.
Yannis was the CEO and Chief Scientist of Enosys Software, which built and commercialized an early XML-based Enterprise Information Integration platform. Enosys Software was acquired in 2003 by BEA Systems. He was the CEO and is the Chief Scientist of app2you, which has commercialized UCSD R and D on rapid development of web applications for data-driven analytics and business process management. He is the Chief Computer Scientist of a pharmaceutical spin-off startup in the area of data analytics for the pharmaceutical industry. He has been in the technical advisory board of multiple startups, currently including Brightscope Inc.
Yannis holds a Diploma of Electrical Engineering from the National Technical University of Athens, MS and Ph.D. in Computer Science from Stanford University (1997) and an NSF CAREER award for his work on data integration.Jan. 27, 2012 SPEAKER: Kurt Brown (EMC/Greenplum)The Future of Big Data AnalyticsDetailsDate and TimeJan. 27, 2012  3 pmLocationDBH 3011SpeakerKurt Brown (EMC/Greenplum)TitleThe Future of Big Data AnalyticsAbstract"Big Data" and analytics have both existed in some form for as long as computing itself, but only now has technology advanced to the point that, together, they are starting to qualitatively change the way organizations and individuals perceive, understand, and predict the world around them.  In this talk, I'll set Big Data Analytics in a historical context to help sort out what aspects of current technologies (hardware, software, and programming models) are simply transient artifacts or long-term trends, and to project where Big Data Analytics is possibly headed (from the perspective of Greenplum and EMC).Speaker BioKurt Brown is currently Director of Advanced R and D at Greenplum/EMC.  
            	Prior to EMC, he co-directed Intel's Berkeley Research Lab, spent 13 years with IBM in operating systems 
            	and database R and D on the East and West coasts, and co-founded three startups in database middleware, 
            	small business marketing services, and residential energy management.  
            	He received his PhD in 1995 from the University of Wisconsin for work in automated database performance  tuning.
            Jan. 13, 2011SPEAKER: Thomas BodnerThe Stratosphere Parallel Analysis Framework, Present and FutureDetailsDate and TimeJan. 13, 2011 3:00 pmLocationDBH 3011SpeakerThomas BodnerTitleThe Stratosphere Parallel Analysis Framework, Present and FutureAbstractData-intensive computing is a much investigated topic in current research. 
            	Next to parallel databases, new flavors of data processors have established themselves - most prominently the MapReduce programming and execution model. 
            	The new systems provide key features that current parallel databases lack, such as flexibility in the data models, the ability to 
            	parallelize custom functions, and fault tolerance that enables them to scale out to thousands of machines. 
            	This talk presents the current state of Stratosphere system, a cloud data and query processor that has been released as open-source in spring 2011. 
            	The system consists of the parallel data programming model PACT, an extension of the MapReduce programming model for the specification of complex data-intensive tasks in the cloud, 
            	and the elastic, massively parallel execution engine Nephele, a Dryad-like parallel data processor. Furthermore, I give a demo of the most recent Stratosphere release. 
            	And finally, I report on future enhancements for Stratosphere, particularly, for the compilation, optimization and parallel execution of data-intensive operations in the system.
Speaker Bio  Since October 2010, Thomas Bodner is a Master's student at the department for Database Systems and Information Management (DIMA) at the Technical University of Berlin. 
        	Between 2007 and 2010, Thomas Bodner completed the Applied Computer Science program at the University of Cooperative Education, Stuttgart, jointly with IBM Germany as partner. 
        	In the course of his undergraduate studies, he studied abroad for one semester at the Royal Melbourne Institute of Technology, Australia and worked as an intern at the IBM Almaden Research Center, 
        	California, USA and the IBM Böblingen Laboratory in Germany, exploring query optimization and in-memory technologies for database management systems. His research interests include architectures 
        	for information management, query processing and optimization, benchmarking and machine learning.
        Dec. 9, 2011 (Special Time\Place)SPEAKER: Pat Helland (Microsoft)If You Have Too Much Data, then "Good Enough" Is Good EnoughDetailsDate and TimeDec. 9, 2011 (Special Time\Place) 11 amLocationDBH 6011SpeakerPat Helland (Microsoft)TitleIf You Have Too Much Data, then "Good Enough" Is Good EnoughAbstractClassic database systems offer crisp answers for a relatively small amount of data. These systems hold their data in one or a relatively small number of computers. With a tightly defined schema and transactional consistency, the results returned from queries are crisp and accurate.
New systems have humongous amounts of data content, change rates, and querying rates and take lots of computers to hold and process. The data quality and meaning are fuzzy. The schema, if present, is likely to vary across the data. The origin of the data may be suspect, and its staleness may vary.
Today's data systems coalesce data from many sources. The Internet, B2B, and enterprise application integration (EAI) combine data from different places. No computer is an island. This large amount of interconnectivity and interdependency has led to a relaxation of many database principles. 
In this talk, consider the some of the ways in which today's answers differ from what we used to expect.
					Speaker Bio
						Pat Helland has been working in distributed systems, transaction processing, databases, and similar areas since 1978. 
						For most of the 1980s, he was the chief architect of Tandem Computers' TMF (Transaction Monitoring Facility), which provided distributed transactions for the NonStop System. 
						With the exception of a two-year stint at Amazon, Helland has worked at Microsoft Corporation since 1994 where he was the architect for Microsoft Transaction Server and SQL Service Broker. 
						Until September, 2011, he was working on Cosmos, a distributed computation and storage system that provides back-end support for Bing.  
						Pat recently relocated to San Francisco with his wife to be close to the grandchildren and to explore new opportunities in "Big Data" and/or "Cloud Computing".
					Nov. 18, 2011SPEAKER: Yi Pan and Masood Mortazavi (Yahoo!)Scalability and Programming Model in Serving Storage SystemsDetailsDate and TimeNov. 18, 2011 3pmLocationDBH 3011SpeakerYi Pan and Masood Mortazavi (Yahoo!)TitleScalability and Programming Model in Serving Storage SystemsAbstractWe will review some of the storage technologies Yahoo applications use in Yahoo's cloud platform. These serving storage systems can scale to extremely large numbers of records. After discussing overall architecture of these scalable storage systems, we will focus on Sherpa (PNUTS). Sherpa is a multi-tenant, distributed, highly elastic key-value store with a well-defined transaction semantics that serves data for 100s of Yahoo applications. To exemplify the type of scalability challenges we face, we will describe how we're evolving Sherpa along various dimensions. We will then focus on the programmability dimension and explain how we have implemented a highly scalable, eventually consistent indexing system for Sherpa. Design decisions we have made to balance concerns related to consistency and availability will be discussed, 
            	and we hope to elucidate the basic questions that come up, repeatedly, when evolving such massively scalable systems while they are in operation.Speaker BioDr. Masood Mortazavi works as a senior principal architect at Yahoo's serving storage systems group. His interests include distributed systems, scalability, multi-tenancy and cloud serving systems. Masood has also worked for Huawei Technologies, Sun Microsystems, Tecknowledge and Hughes Aircrafts.  Masood's LinkedIn profile can be found here: http://www.linkedin.com/in/mortazavi . . . At Yahoo, he helps advance cloud platform and storage technologies.
						Dr. Yi Pan graduated with a Ph.D. degree in computer science from University of California at Irvine. He got his B.S. and M.S. Degree from Fudan University in Shanghai, China. His main interests expand across many areas in large scale distributed computer networks and applications.  Currently, he works as a principal software engineer in Yahoo!’s Cloud Platform Group. His main goal is to push forward Yahoo!’s state-of-art cloud storage systems with innovative features.Nov. 4, 2011SPEAKER: Thomas BodnerMyriad - Parallel Data Generation on Shared-Nothing ArchitecturesDetailsDate and TimeNov. 4, 2011 3:30 pmLocationDBH 3011SpeakerThomas BodnerTitleMyriad - Parallel Data Generation on Shared-Nothing ArchitecturesAbstract
The need for efficient data generation for the purposes of testing and benchmarking newly developed data-intensive computing systems has increased with the emergence of
big data problems. As synthetic data model specifications evolve over time the data generator programs implementing these models have to be continuously adapted –
 a task that might become complex as the set of model constraints grows. This talk presents Myriad - a new parallel data generation toolkit. Data generators created
  with the toolkit can produce very large datasets by exploiting a completely parallel execution model, while at the same time maintain cross-partition dependencies, correlations and distributions in the generated data. 
In addition, I report on our efforts towards a benchmark suite for large-scale parallel analysis systems that uses Myriad for the generation of large social network graphs and OLAP-style relational datasets.
						Speaker Bio  Since October 2010, Thomas Bodner is a Master's student at the department for Database Systems and Information Management (DIMA) at the Technical University of Berlin. 
        	Between 2007 and 2010, Thomas Bodner completed the Applied Computer Science program at the University of Cooperative Education, Stuttgart, jointly with IBM Germany as partner. 
        	In the course of his undergraduate studies, he studied abroad for one semester at the Royal Melbourne Institute of Technology, Australia and worked as an intern at the IBM Almaden Research Center, 
        	California, USA and the IBM Böblingen Laboratory in Germany, exploring query optimization and in-memory technologies for database management systems. His research interests include architectures 
        	for information management, query processing and optimization, benchmarking and machine learning.
        Oct. 21, 2011SPEAKER: David Lomet (Microsoft Research)Deuteronomy: Transaction Support for Cloud DataDetailsDate and TimeOct. 21, 2011 3pmLocationDBH 3011SpeakerDavid Lomet (Microsoft Research)TitleDeuteronomy: Transaction Support for Cloud DataAbstractThe Deuteronomy system supports efficient and scalable ACID transactions in the cloud by decomposing the storage engine
            	 into: (a) a transactional component (TC) that manages transactions and their ``logical" concurrency control and undo/redo recovery, 
            	 and (b) a data component (DC) that knows about the access methods and supports a record-oriented interface with atomic operations, 
            	 but knows nothing about transactions.  The Deuteronomy TC can be applied to data anywhere, in the cloud, local, etc. with a variety 
            	 of deployments for both the TC and DC components.  In this talk, we first describe the architecture of our TC, and the considerations 
            	 that led to it.  We next describe the contract between TC and DC, how we changed the operation protocol to simplify it and make it more efficient.   
            	 We have implemented both TC and multiple DCs, and will describe our TC implementation in detail.  
            	 We will end a few words about observed performance and scalability.Speaker Bio
            	David Lomet is a principal researcher managing the Microsoft Research Database Group.  Earlier, he worked at Digital, IBM Research, and Wang Institute.  
            	He has a CS Ph.D from the University of Pennsylvania.  He is author of over 100 papers (two SIGMOD "best papers") and has 45 patents.  
            	He has served on program committees (SIGMOD, PODS, VLDB, ICDE...), was ICDE'2000 PC co-chair, VLDB'2006 PC core chair, and is on the ICDE Steering Committee, 
            	the VLDB Board, is TCDE Chair and has been an editor for TODS, VLDBJ, and JDPD. He is the Data Engineering Bulletin EIC, for which he received the SIGMOD Contributions Award.  
            	He received IEEE Golden Core, Outstanding, and Meritorious Service Awards and is a Fellow of IEEE, ACM, and AAAS.
            Oct. 21, 2011 (Special Time\Place)SPEAKER: Danny Sullivan (Editor In Chief, Search Engine Land)From Search 1.0 to Search 4.0DetailsDate and TimeOct. 21, 2011 (Special Time\Place) 11amLocationDBH 6011SpeakerDanny Sullivan (Editor In Chief, Search Engine Land)TitleFrom Search 1.0 to Search 4.0AbstractWhen search engines first began, they focused on crawling web pages
and "words on the page" ranking analysis. That system quickly failed,
being far too easy to game. Search 2.0 gave us ranking where links
were used as votes; Search 3.0, a third generational system,
introduced blending vertical search results with web matches.
Currently underway, the fourth generational trend of Search 4.0 taps
into human signals, from social networks and personalization, to
refine search results. The "how and why" of this evolution has
unfolded.Speaker BioWidely considered a leading "search engine guru," Danny Sullivan has
been helping webmasters, marketers and everyday web users understand
how search engines work for over a decade. Danny's expertise about
search engines is often sought by the media, and he has been quoted in
places like The Wall St. Journal, USA Today, The Los Angeles Times,
Forbes, The New Yorker and Newsweek and ABC's Nightline. Danny began
covering search engines in late 1995, when he undertook a study of how
they indexed web pages. The results were published online as "A
Webmaster's Guide To Search Engines," a pioneering effort to answer
the many questions site designers and Internet publicists had about
search engines. Danny currently heads up Search Engine Land as
editor-in-chief, which covers all aspects of search marketing and
search engine news. Danny also serves as Third Door Media's chief
content officer, which owns Search Engine Land and the SMX: Search
Marketing Expo conference series. Danny also maintains a personal blog
called Daggle and microblogs on Twitter: @dannysullivan.Oct. 14, 2011SPEAKER: Tyson Condie (Yahoo! Research)Scal(a)ing up Machine Learning and Graph-based AnalyticsDetailsDate and TimeOct. 14, 2011 3pmLocationDBH 3011SpeakerTyson Condie (Yahoo! Research)TitleScal(a)ing up Machine Learning and Graph-based AnalyticsAbstract
Machine learning practitioners are increasingly interested in applying their algorithms to Big Data. Unfortunately, current high-level 
languages for data analytics (e.g., Hive, Pig, Sawzall, Scope) do not fully cover this domain. One key missing ingredient is the means to 
efficiently support iteration over the data. Zaharia et al., were the first to answer this call from a systems perspective with Spark. 
Spark adds the notion of a working set to data-parallel workflows and has published speed-ups of 30x over Hadoop MapReduce for many machine learning and graph algorithms.

Unfortunately, Spark does cover the whole pipeline of Big Data analytics; at Yahoo!, it is common to compose Pig, MPI and direct MapReduce program modules into workflows. 
This fractioning of individual processing steps can be a major pain e.g., for optimization, debugging, and code readability. Our prescription to this dilemma is a new DSL 
for data analytics called ScalOps. Like Pig, ScalOps combines the declarative style of SQL and the low-level procedural style of MapReduce. Like Spark, ScalOps can optimize 
its runtime—the Hyracks parallel-database engine—for repeated access to data collections. ScalOps is part of a broader research agenda to explore new abstractions
 for machine learning and graph-based analytics. In this talk, I will present example workflows from the machine learning domain expressed in ScalOps and their translation to Hyracks recursive query plans. Sept. 30, 2011SPEAKER: Grad. studentsSystem DemoDetailsDate and TimeSept. 30, 2011 3pmLocationDBH 3011SpeakerGrad. studentsTitleSystem DemoSept. 23, 2011SPEAKER: ISG memebersISG GatheringDetailsDate and TimeSept. 23, 2011 3pmLocationDBH 3011SpeakerISG memebersTitleISG GatheringJune 3, 2011SPEAKER: Donald KossmanPredictable Performance for Unpredictable WorkloadsDetailsDate and TimeJune 3, 2011 2pmLocationDBH 3011SpeakerDonald KossmanTitlePredictable Performance for Unpredictable WorkloadsAbstractThis talk presents the design of SwissBox. 
            	SwissBox is a database appliance designed to process thousands of concurrent queries and updates with bounded 
            	query response times and strict data freshness guarantees. The system was designed to aggressively share operations 
            	between concurrent queries and updates. This talk shows the design of the storage manager (called Crescando)
            	 and the design of the query processor (called SharedDB). Furthermore, the talk presents the results of 
            	 performance experiments with workloads from an airline reservation system.Speaker BioDonald Kossmann is a professor for Computer Science at ETH Zurich (Switzerland). He received his MS from the University of Karlsruhe and completed his PhD at the University of Aachen. After that, he held positions at the University of Maryland, the IBM Almaden Research Center, the University of Passau, the University of Munich, and the University of Heidelberg. He is an ACM fellow, member of the board of trustees of the VLDB endowment, and was the program committee chair of the ACM SIGMOD Conf., 2009. He is a co-founder of i-TV-T (1998), XQRL Inc. 
            	(acquired by BEA in 2002), and 28msec Inc. (2007). 
            	His research interests lie in the area of databases and information systems.May 20, 2011SPEAKER: Ronen VaisenbergScheduling and Actuating Camera Networks to Maximize Event DetectionDetailsDate and TimeMay 20, 2011 2pmLocationDBH 3011SpeakerRonen VaisenbergTitleScheduling and Actuating Camera Networks to Maximize Event DetectionAbstractA distributed camera network allows for many compelling applications, such as large-scale tracking, face recognition, occupancy monitoring or event detection. 
					In most practical systems, resources are either constrained or mutually exclusive. Constraints arise from network bandwidth restrictions, I/O and disk usage from writing images, 
					and CPU usage needed to extract features from the images. Detecting events in real time requires dynamically choosing a subset of the available sensors for processing at any given time. 
Furthermore, certain camera configurations are not feasible. For example, a camera cannot zoom into two different regions in its field of view.
Zooming into a specific area in the field of view of a camera would generate a high resolution image of the region in the expense of a wider field of view. 
Thus, the field of view needs to be changed dynamically to get a higher resolution images of certain regions of the space at the expanse other regions. 
In order to illustrate the complexity of this problem, consider a face recognition application, which is only interested in high resolution (by means of optical zoom) 
facial images. If we always zoom into a region to look for a high res face, we might miss presence of a person in different region and hence opportunity for zooming later to get the face in next time step.
In this talk we examine the problem of scheduling sensors for data collection and actuating them on real time to maximize some user-specified objective - e.g., 
detecting as much motion as possible or collect as many high resolution facial images. 
The main idea behind our approach is the use of sensor semantics to guide the scheduling process. We learn a dynamic probabilistic model of motion correlations 
between cameras, and use the model to guide resource allocation for our sensor network.
Although previous work has leveraged probabilistic models for sensor-scheduling, our work is distinct in its focus on real-time building-monitoring using a camera network.  
We validate our approach using a sensor network of a dozen cameras spread throughout a university building, recording measurements of unscripted human activity over a two week period. 
We automatically learn a semantic model of typical behaviors, and show that one can significantly improve efficiency of resource allocation and actuation by exploiting this model.
May 13, 2011 (Special)SPEAKER: Prof. John Ousterhout (Stanford)RAMCloud: Scalable High-Performance Storage Entirely in DRAMDetailsDate and TimeMay 13, 2011 (Special) 11amLocationDBH 6011SpeakerProf. John Ousterhout (Stanford)TitleRAMCloud: Scalable High-Performance Storage Entirely in DRAMAbstractDisk-oriented approaches to online storage are becoming increasingly problematic: they do not scale gracefully to meet the needs of new large-scale Web applications, and improvements in disk capacity have out-stripped improvements in access speed.  In this talk I will describe a new approach to datacenter storage called RAMCloud, where information is kept entirely in DRAM and large-scale systems are created by aggregating the main memories of thousands of commodity servers.  A RAMCloud can provide durable and available storage with 100-1000x the throughput of disk-based systems and 100-1000x lower access latency.  By combining low latency and large scale, RAMClouds will enable a new class of applications that manipulate large datasets more intensively than has ever been possible.
Speaker BioJohn Ousterhout is Professor (Research) of Computer Science at Stanford University.  His current research focuses on infrastructure
for Web applications and cloud computing.  Ousterhout's prior positions include 14 years in industry where he founded two companies (Scriptics and Electric Cloud), preceded by 14 years as Professor of Computer Science at U.C. Berkeley.  He is the creator of the Tcl scripting language and is also well known for his work in distributed operating systems and file systems.  Ousterhout received a BS degree in Physics from Yale University and a PhD in Computer Science from Carnegie Mellon University.  He is a member of the National Academy of Engineering and has received numerous awards, including the ACM Software System Award, the ACM Grace Murray Hopper Award, the National Science Foundation Presidential Young Investigator Award, and the U.C. Berkeley Distinguished Teaching Award.
May 9, 2011 (Special)SPEAKER: Prof. Barton P. MillerScaling Up to Large (Really Large) SystemsDetailsDate and TimeMay 9, 2011 (Special) 11amLocationDBH 3011SpeakerProf. Barton P. MillerTitleScaling Up to Large (Really Large) SystemsAbstractI will discuss the problem of developing tools and middleware for large scale
parallel environments.  We are especially interested in systems, both leadership
class parallel computers and clusters that have 100,000's or even millions
of processors.  The infrastructure that we have developed to address this
problem is called MRNet, the Multicast/Reduction Network. MRNet's approach
to scale is to structure control and data flow in a tree-based overlay
network (TBON) that allows for efficient request distribution and flexible
data reductions.

I will then present an overview of the MRNet design, architecture, and
computational model and then discuss several of the applications of MRNet.
The applications include scalable automated performance analysis, a vision
clustering application and, most recently, an effort to develop our first
petascale debugging tool, STAT, a scalable stack trace analyzer running
currently on 100,000's of processors on both the Cray XT and IBM BlueGene.Speaker BioProf. Barton Miller is a Professor of Computer Sciences at the University of Wisconsin.
Bart is a product of the UC System: he received his BA degree from UC San Diego in 1977
and his MS and PhD in Computer Science from UC Berkeley in 1980 and 1984, respectively.
His research interests include distributed and parallel program performance and tools,
binary code analysis and instrumentation, computer security, scalable systems, operating
systems, and software testing. Bart is a Fellow of the ACM.May 6, 2011SPEAKER: Matthias Nicola, IBM A Matter of Time: Temporal Data Management in DB2 for z/OS DetailsDate and TimeMay 6, 2011 2pmLocationDBH 3011SpeakerMatthias Nicola, IBMTitle A Matter of Time: Temporal Data Management in DB2 for z/OS AbstractTime is a critical dimension in data management. For many enterprises
		 it is useful or even required to have the ability to go back in time and look at a 
		 past state of the database. Many applications also need to manage time in their 
		 business records, such as contract start and end dates, expiration dates, or 
		 "effective dates" to indicate that information is valid for a certain period in the past, presence, or future. This presentation
		 describes typical use cases for temporal data management and describes 
		  the temporal capabilities in DB2, including system time, business time, and bitemporal support. 
Speaker BioMatthias Nicola is a senior software engineer at IBM's Silicon Valley Lab, in 
	San Jose, CA, USA. He focuses on DB2 performance and benchmarking, XML, temporal data 
	management, in-database analytics, and other emerging technologies. Matthias also works 
	closely with customers and business partners to help them design, optimize and implement 
	DB2 solutions. Previously Matthias worked on data warehouse performance at Informix Software. 
	Matthias received his PhD in computer science from the Technical University of Aachen, Germany. 
April 25, 2011 (Special)SPEAKER: Prof. Christos Faloutsos, CMUMining Billion-node GraphsDetailsDate and TimeApril 25, 2011 (Special) 11amLocationDBH 6011SpeakerProf. Christos Faloutsos, CMUTitleMining Billion-node GraphsAbstractWhat do graphs look like? How do they evolve over time? How to handle a 
graph with a billion nodes? We present a comprehensive list of static 
and temporal laws, and some recent observations on real graphs (like, 
e.g., ``eigenSpokes''). We present tools, and specifically ``oddBall'' 
for discovering anomalies and patterns, as well as fast algorithms for 
immunization. Finally, we present an overview of the PEGASUS system 
which is designed to handle billion-node graphs, running on top of the 
"hadoop" system.Speaker BioChristos Faloutsos is a Professor at Carnegie Mellon University. He has 
received the Presidential Young Investigator Award by the National 
Science Foundation (1989), the Research Contributions Award in ICDM 
2006, the SIGKDD Innovations Award (2010), seventeen ``best paper'' 
awards, (including two ``test of time'') and four teaching awards. He 
has served as a member of the executive committee of SIGKDD; he is an 
ACM Fellow; he has published over 200 refereed articles, 11 book 
chapters and one monograph.  He holds five patents and he has given over 
30 tutorials and over 10 invited distinguished lectures. His research 
interests include data mining for graphs and streams, fractals, database 
performance, and indexing for multimedia and bio-informatics data.
April 22, 2011SPEAKER: Jerome Simeon, IBM Research T.J. WatsonAlgebraic Comprehensions (Database Optimization for Web 2.0 Queries)DetailsDate and TimeApril 22, 2011 2pmLocationDBH 3011SpeakerJerome Simeon, IBM Research T.J. WatsonTitleAlgebraic Comprehensions (Database Optimization for Web 2.0 Queries)AbstractDirect support for querying is becoming a "must have" for programming languages targeting Web 2.0 and Cloud development. Most of those languages (Microsoft's Linq, University of Edinburgh's Links, EPFL's Scala, Yahoo!'s Pig Latin, IBM's Thorn, etc) rely on the classic notion of comprehensions over collections. At the language level, comprehensions are a perfect choice, being well understood programming constructs, and capturing the expressive power of SQL iterators. At the compiler level, however, they are at odds with database optimizers which mostly rely on relational (or nested-relational) algebras. That mismatch was clearly on display during the design of XQuery, whose semantics is based on comprehensions, and for which most implementations target relational backends. We propose a alternative functional semantic formulation of XQuery to the one proposed by W3C, which is also based on comprehensions but has the benefit of corresponding precisely to compilation into a typed algebra that supports traditional database optimizations. First, this provides a formal foundation for XQuery implementations that want to ensure semantics integrity with the standard, along with modern database optimization techniques. Also, it provides key insights into the nature of database compilers that we believe is essential for the integration of database and programming languages technology. We notably discover that type systems for database algebras require an original solution to the old problem of subtyping with record concatenation, and that such a type system can eliminate the need for complex side conditions used in query language optimization.
	Speaker BioJerome Simeon is a Researcher for the Scalable XML Infrastructure Group at IBM T.J. Watson. He holds a degree in Engineering from EcolePolytechnique, and a Ph.D. from Universite d'Orsay. Previously, Jerome worked at INRIA from 1995 to 1999, and Bell Laboratories from 1999 to 2004. His research interests include databases, programming languages, compilers, and semantics, with a focus on Web development. He has put his work into practice in areas ranging from telecommunication infrastructure, to music. He is a co-editor for five of the W3C XML Query specifications, and has published more than 50 papers in scientific journals and international conferences. He is also a project lead for the Galax open-source XQuery implementation, and a co-author of "XQuery from the Experts" (Addison Wesley, 2004).
        April 15, 2011 SPEAKER: Tyson Condie, Yahoo! ResearchRubySky: Exploring Big Data with Transparency and AdjustabilityDetailsDate and TimeApril 15, 2011  2pmLocationDBH 3011SpeakerTyson Condie, Yahoo! ResearchTitleRubySky: Exploring Big Data with Transparency and AdjustabilityAbstractIn this talk, I will introduce a new scripting language for ad-hoc exploration of large data sets, called RubySky.  
            	As with several prior efforts, RubySky scripts execute either in a local environment or in the cloud (Hadoop). 
            	Typically, cloud-based execution is highly opaque and hands-off, rendering debugging and iterative code development 
            	very difficult. RubySky, on the other hand, aims for a more transparent and adjustable paradigm.  
            	It includes the ability to ``peek into'' intermediate cloud execution pathways, integrated as a first-class language construct.  
            	Also integrated into the language is a way for the user to make last-minute code revisions, at any point at which troublesome
            	 data is encountered in the cloud. 
            	 Combined, these features aim to improve usability for users who develop and run single-use scripts that
            	  explore new data sets. This is joint work with Christopher Olston at Yahoo! Research.
            April 1, 2011 SPEAKER: Doug Terry, Microsoft ResearchReplicated Data Consistency Explained through BaseballDetailsDate and TimeApril 1, 2011  2pmLocationDBH 3011SpeakerDoug Terry, Microsoft ResearchTitleReplicated Data Consistency Explained through BaseballAbstract A variety of relaxed consistency models for replicated data have
been proposed and studied as an alternative to one-copy serializability, and
some of these are being used in cloud storage systems.  The designers of
such systems particularly avoid two-phase commit for updates to
geo-replicated data that spans multiple data centers on different
continents.  Instead, many cloud services, including systems from Amazon,
Yahoo, and Microsoft, have adopted techniques that provide eventual
consistency.  This talk explores the hows and whys of different consistency
models.  The discussion will be driven by a simple example: maintaining the
score of a baseball game. We'll see that people with various roles in the
game can tolerate and benefit from different types of consistency when
accessing the score.
Speaker BioDoug Terry is a Principal Researcher in the Microsoft Research Silicon
Valley lab.  His research focuses on the design and implementation of novel
distributed systems including mobile and cloud services.  He currently is
serving as Chair of ACM's Special Interest Group on Operating Systems
(SIGOPS) and as a member of the ACM Council.  Prior to joining Microsoft,
Doug was the co-founder and CTO of a start-up company named Cogenia, Chief
Scientist of the Computer Science Laboratory at Xerox PARC, and an Adjunct
Professor in the Computer Science Division at U. C. Berkeley, where he still
occasionally teaches a graduate course on distributed systems.  Doug has a
Ph.D. in Computer Science from U.C. Berkeley and is an ACM Fellow.Mar 31, 2011 SPEAKER: Doug Terry, Microsoft ResearchCimbiosys: Content-based Replication for Mobile Devices and the CloudDetailsDate and TimeMar 31, 2011  11amLocationDBH 6011SpeakerDoug Terry, Microsoft ResearchTitleCimbiosys: Content-based Replication for Mobile Devices and the CloudAbstract As people increasingly use mobile devices and cloud services to
share large data collections, exploiting communication proximity and
selectively replicating content is essential.  Cimbiosys is a replicated
storage platform that permits each device to define its own content-based
filtering criteria and to exchange data directly with other devices.  This
talk focuses on the key challenge of ensuring eventual consistency in the
face of fluid network connectivity, redefinable content filters, and
arbitrary updates.  Notably, Cimbiosys guarantees that each device
eventually stores precisely those items whose latest version matches its
custom filter and represents its replication-specific metadata in a compact
form, resulting in low data synchronization overhead.  This permits ad hoc
replication between newly encountered devices and frequent synchronization
between established partners, even over low bandwidth wireless networks or
across geo-distributed data centers.  (This talk will be a Ted and Janice Smith
Distinguished lecture, and not at the normal time or place for ISG Seminars.)
Speaker BioDoug Terry is a Principal Researcher in the Microsoft Research Silicon
Valley lab.  His research focuses on the design and implementation of novel
distributed systems including mobile and cloud services.  He currently is
serving as Chair of ACM's Special Interest Group on Operating Systems
(SIGOPS) and as a member of the ACM Council.  Prior to joining Microsoft,
Doug was the co-founder and CTO of a start-up company named Cogenia, Chief
Scientist of the Computer Science Laboratory at Xerox PARC, and an Adjunct
Professor in the Computer Science Division at U. C. Berkeley, where he still
occasionally teaches a graduate course on distributed systems.  Doug has a
Ph.D. in Computer Science from U.C. Berkeley and is an ACM Fellow.
Mar 25, 2011 SPEAKER: Alexander Behm, UCI PhD studentAnswering Approximate String Queries on Large Data Sets Using External MemoryDetailsDate and TimeMar 25, 2011  2pmLocationDBH 3011SpeakerAlexander Behm, UCI PhD studentTitleAnswering Approximate String Queries on Large Data Sets Using External MemoryAbstractAn approximate string query is to find from a
collection of strings those that are similar to a given query string.
Answering such queries is important in many applications such
as data cleaning and record linkage, where errors could occur
in queries as well as the data. Many existing algorithms have
focused on in-memory indexes. In this paper we investigate how
to efficiently answer such queries in a disk-based setting, by
systematically studying the effects of storing data and indexes
on disk. We devise a novel physical layout for an inverted
index to answer queries and we study how to construct it with
limited buffer space. To answer queries, we develop a cost-based,
adaptive algorithm that balances the I/O costs of retrieving
candidate matches and accessing inverted lists. Experiments
on large, real datasets verify that simply adapting existing
algorithms to a disk-based setting does not work well and that our
new techniques answer queries efficiently. Further, our solutions
significantly outperform a recent tree-based index, BED-tree.
This talk is a ICDE practice talk.
            Mar 18, 2011 SPEAKER: Pinaki SinhaSummarization of  Personal Photo CollectionsDetailsDate and TimeMar 18, 2011  2pmLocationDBH 3011SpeakerPinaki SinhaTitleSummarization of  Personal Photo CollectionsAbstractThe volume of personal photos hosted on photo archives and social sharing platforms 
            	has been increasing exponentially. According to recent estimates, 6 Billion photos are uploaded
            	 on Facebook per month. It is difficult to get an overview of a large collection of personal 
            	 photos without browsing though the entire database manually. In this talk, I will discuss a 
            	 framework to generate representative subset summaries from photo collections present on personal 
            	 archives or social networks. I will define salient properties of an effective photo summary 
            	 and model summarization as an optimization of these properties, given the size constraints. 
            	 Computer vision, and IR  based techniques will be used to generate summaries that "look good" as well as 
            	 are informative. I will also introduce information theory based metrics for evaluating photo 
            	 summaries based on their information content and the ability to satisfy user's information needs. 
            	 I will also discuss the manual evaluation experiments that were done to evaluate summaries.
            Mar 11, 2011 SPEAKER:  Dmitri V. Kalashniknov Entity resolutionDetailsDate and TimeMar 11, 2011  2pmLocationDBH 3011Speaker Dmitri V. Kalashniknov TitleEntity resolutionMar 4, 2011 SPEAKER: Rares Vernica Efficient Processing of Set-Similarity Joins on Large ClustersDetailsDate and TimeMar 4, 2011  2pmLocationDBH 3011SpeakerRares VernicaTitle Efficient Processing of Set-Similarity Joins on Large ClustersFeb 23, 2011 SPEAKER:  Dr. Terence SimGetting More From FisherDetailsDate and TimeFeb 23, 2011  3pmLocationDBH 3011Speaker Dr. Terence SimTitleGetting More From FisherAbstractThe Fisher Linear Discriminant (FLD) is commonly used
in classification to find a subspace that maximally separates
class patterns according to the Fisher Criterion.   It was
previously proven that a pre-whitening step can be used to
truly optimize the Fisher Criterion. In this talk, we show that
more insight and more applications may be derived from this
classical technique.

First, we explore the subspaces induced by this whitened FLD.
In particular, we show how the Identity Space and Variation
Space are useful for decomposing and representing data.
We give sufficient conditions for these spaces to exist.  Through
experiments we also show how these spaces may
be used for classification and image synthesis.

Second, we further extend classical Fisher to handle data exhibiting
multiple factors (modes), e.g. face images that exhibit personal
identity, illumination, and pose.  We call our method Multimodal
Discriminant Analysis (MMDA), which is useful for decomposing
a dataset into independent modes. For face images, MMDA
effectively separates identity, illumination and pose into mutually
orthogonal subspaces.  MMDA is based on maximizing the
Fisher Criterion on all modes simultaneously, and is therefore
well-suited for multimodal and mode-invariant pattern recognition.
We also show that MMDA may be used for dimension reduction,
and for synthesizing face images under novel illumination, and
even novel personal identity.Speaker BioTerence Sim is an Asst. Prof. at the School of Computing, National University of Singapore.  He teaches an undergraduate course in computer vision, as well as a graduate course in multimedia fundamentals.  For research, he works primarily in these areas:  face recognition, biometrics, and computational photography.   He is also interested in computer vision problems in general, such as shape-from-shading, photometric stereo, object recognition.  On the side, he dabbles with some aspects of music processing, such as polyphonic music transcription.  Dr. Sim serves as Vice-Chairman of the Biometrics Technical Committee (BTC), Singapore, and Chairman of the Cross-Jurisdictional and Societal Aspects Working Group (WG6) within the BTC.  The interesting issues here are the legal and privacy aspects of using biometrics.   He also serves as Vice-President of the Pattern Recognition and Machine Intelligence Association (PREMIA), a national professional body for pattern recognition.  Dr. Sim obtained his PhD from Carnegie Mellon in 2002, his MSc from Stanford University in 1991, and his SB from MIT in 1990.Feb 16, 2011SPEAKER:  Laura Haas (IBM)New Principles for Information Integration DetailsDate and TimeFeb 16, 2011 11amLocationDBH 4011Speaker Laura Haas (IBM)TitleNew Principles for Information Integration Abstract Ten years ago, Clio introduced nonprocedural schema mappings to describe the relationship between data in heterogeneous schemas. This enabled powerful tools for mapping discovery and integration code generation, greatly simplifying the integration process.  However, further progress is needed. We see an opportunity to raise the level of abstraction further, and propose two new principles that the next generation of integration systems should embody. Holistic information integration supports iteration across the various integration tasks, leveraging information about both schema and data to improve the integrated result. Integration independence allows applications to be independent of how, when, and where information integration takes place, making materialization and the timing of transformations an optimization decision that is transparent to applications. This talk introduces these principles and describes some promising recent work in these directions. Speaker Bio Laura Haas is an IBM Fellow and has been director of computer science at IBM Almaden Research Center since 2005.  Previously, Dr. Haas was responsible for Information Integration Solutions (IIS) architecture in IBM's Software Group after leading the IIS development team through its first two years.  She joined the development team in 2001 as manager of DB2 UDB Query Compiler development.  Before that, Dr. Haas was a research staff member and manager at the Almaden lab for nearly twenty years.   In IBM Research, she worked on and managed a number of exploratory projects in distributed database systems.  Dr. Haas is best known for her work on the Starburst query processor (from which DB2 UDB was developed); on Garlic, a system which allowed federation of heterogeneous data sources; and on Clio, the first semi-automatic tool for heterogeneous schema mapping.  Garlic technology, married with DB2 UDB query processing, is the basis for the IBM WebSphere Information Server's federation capabilities, while Clio capabilities are a core differentiator in IBM’s Rational Data Architect.  Dr. Haas has received several IBM awards for Outstanding Technical Achievement and Outstanding Innovation, and an IBM Corporate Award for her work on federated database technology. In 2010 she was recognized with the Anita Borg Institute Technical Leadership Award. She is a member of the National Academy of Engineering and the IBM Academy of Technology, an ACM Fellow, and Vice Chair of the board of the Computing Research Association.  Dr. Haas received her PhD from the University of Texas at Austin, and her bachelor degree from Harvard University.Feb 4, 2011 (POSTPONED)SPEAKER:  Amy VoidaHomebrew DatabasesDetailsDate and TimeFeb 4, 2011 (POSTPONED) 2pmLocationDBH 3011Speaker Amy VoidaTitleHomebrew Databases For more information on CS distinguished lectures, please visit Computer Science Department Seminar Series.^ topLast Updated on January 07, 2011