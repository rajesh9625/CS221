Username: University of California Irvine User Book: Hadoop: The Definitive Guide, 3rd Edition. No part of any chapter or book may be reproduced or transmitted in any form by any means without the prior written permission for reprints and excerpts from the publisher of the book or chapter. Redistribution or other use that violates the fair use privilege under U.S. copyright laws (see 17 USC107) or that otherwise violates these Terms of Service is strictly prohibited. Violators will be prosecuted to the full extent of U.S. Federal and Massachusetts laws.Cluster Specification






Hadoop is designed to run on commodity hardware. That means
    that you are not tied to expensive, proprietary offerings from a single
    vendor; rather, you can choose standardized, commonly available hardware
    from any of a large range of vendors to build your cluster.
“Commodity” does not mean “low-end.” Low-end machines often have
    cheap components, which have higher failure rates than more expensive (but
    still commodity-class) machines. When you are operating tens, hundreds, or
    thousands of machines, cheap components turn out to be a false economy, as
    the higher failure rate incurs a greater maintenance cost. On the other
    hand, large database-class machines are not recommended either, since they
    don’t score well on the price/performance curve. And even though you would
    need fewer of them to build a cluster of comparable performance than one
    built of mid-range commodity hardware, when one did fail, it would have a
    bigger impact on the cluster because a larger proportion of the cluster
    hardware would be unavailable.
Hardware specifications rapidly become obsolete, but for the sake of
    illustration, a typical choice of machine for running a Hadoop datanode
    and tasktracker in mid-2010 would have the following
    specifications:Processor
Two quad-core 2-2.5 GHz CPUsMemory
16-24 GB ECC RAM[75]Storage
Four 1 TB SATA disksNetwork
Gigabit Ethernet

Although the hardware specification for your cluster will assuredly
    be different, Hadoop is designed to use multiple cores and disks, so it
    will be able to take full advantage of more powerful hardware.
Why Not Use RAID?
HDFS clusters do not benefit from using RAID (Redundant Array of Independent Disks) for datanode
      storage (although RAID is recommended for the namenode’s disks, to
      protect against corruption of its metadata). The redundancy that RAID
      provides is not needed, since HDFS handles it by replication between
      nodes.
Furthermore, RAID striping (RAID 0), which is commonly used to
      increase performance, turns out to be slower than the JBOD (Just a Bunch Of Disks)
      configuration used by HDFS, which round-robins HDFS blocks between all
      disks. This is because RAID 0 read and write operations are limited by
      the speed of the slowest disk in the RAID array. In JBOD, disk
      operations are independent, so the average speed of operations is
      greater than that of the slowest disk. Disk performance often shows
      considerable variation in practice, even for disks of the same model. In
      some benchmarking carried out on a Yahoo! cluster (http://markmail.org/message/xmzc45zi25htr7ry), JBOD
      performed 10% faster than RAID 0 in one test (Gridmix) and 30% better in
      another (HDFS write throughput).
Finally, if a disk fails in a JBOD configuration, HDFS can
      continue to operate without the failed disk, whereas with RAID, failure
      of a single disk causes the whole array (and hence the node) to become
      unavailable.

The bulk of Hadoop is written in Java and can therefore run on any
    platform with a JVM, although there are enough parts that harbor Unix
    assumptions (the control scripts, for example) to make it unwise to run on
    a non-Unix platform in production. In fact, Windows operating systems are
    not supported production platforms (although they can be used with Cygwin
    as a development platform; see Appendix A).
How large should your cluster be? There isn’t an exact answer to
    this question, but the beauty of Hadoop is that you can start with a small
    cluster (say, 10 nodes) and grow it as your storage and computational
    needs grow. In many ways, a better question is this: how fast does my
    cluster need to grow? You can get a good feel for this by considering
    storage capacity.
For example, if your data grows by 1 TB a week and you have
    three-way HDFS replication, you need an additional 3 TB of raw storage per
    week. Allow some room for intermediate files and logfiles (around 30%,
    say), and this works out at about one (2010-vintage) machine per week, on
    average. In practice, you wouldn’t buy a new machine each week and add it
    to the cluster. The value of doing a back-of-the-envelope calculation like
    this is that it gives you a feel for how big your cluster should be. In
    this example, a cluster that holds two years of data needs 100
    machines.
For a small cluster (on the order of 10 nodes), it is usually
    acceptable to run the namenode and the jobtracker on a single master
    machine (as long as at least one copy of the namenode’s metadata is stored
    on a remote filesystem). As the cluster and the number of files stored in
    HDFS grow, the namenode needs more memory, so the namenode and jobtracker
    should be moved onto separate machines.
The secondary namenode can be run on the same machine as the
    namenode, but again for reasons of memory usage (the secondary has the
    same memory requirements as the primary), it is best to run it on a
    separate piece of hardware, especially for larger clusters. (This topic is
    discussed in more detail in Master node scenarios.)
    Machines running the namenodes
    should typically run on 64-bit hardware to avoid the 3 GB limit on Java
    heap size in 32-bit architectures.[76]


Network Topology






A common Hadoop cluster architecture consists of a
      two-level network topology, as illustrated in Figure 9-1. Typically there are 30 to 40 servers per
      rack, with a 1 GB switch for the rack (only three are shown in the
      diagram) and an uplink to a core switch or router (which is normally 1
      GB or better). The salient point is that the aggregate bandwidth between
      nodes on the same rack is much greater than that between nodes on
      different racks.




Figure 9-1. Typical two-level network architecture for a Hadoop cluster



Rack awareness






To get maximum performance out of Hadoop, it is
        important to configure Hadoop so that it knows the topology of your
        network. If your cluster runs on a single rack, then there is nothing
        more to do, since this is the default. However, for multirack
        clusters, you need to map nodes to racks. By doing this, Hadoop will
        prefer within-rack transfers (where there is more bandwidth available)
        to off-rack transfers when placing MapReduce tasks on nodes. HDFS will be
        able to place replicas more intelligently to trade off performance and
        resilience.
Network locations such as nodes and racks are represented in a
        tree, which reflects the network “distance” between locations. The
        namenode uses the network location when determining where to place
        block replicas (see Network Topology and Hadoop); the
        MapReduce scheduler uses network location to determine where the
        closest replica is as input to a map task.
For the network in Figure 9-1, the rack
        topology is described by two network locations, say, /switch1/rack1 and /switch1/rack2. Because there is only one
        top-level switch in this cluster, the locations can be simplified to
        /rack1 and /rack2.
The Hadoop configuration must specify a map between node
        addresses and network locations. The map is described by a Java
        interface, DNSToSwitchMapping,
        whose signature is:
public interface DNSToSwitchMapping {
  public List<String> resolve(List<String> names);
}
The names parameter is a
        list of IP addresses, and the return value is a list of corresponding
        network location strings. The topology.node.switch.mapping.impl
        configuration property defines an implementation of the DNSToSwitchMapping
        interface that the namenode and the jobtracker use to resolve worker
        node network locations.
For the network in our example, we would map node1, node2, and node3 to /rack1, and node4, node5, and node6 to /rack2.
Most installations don’t need to implement the interface
        themselves, however, since the default implementation is ScriptBasedMapping,
        which runs a user-defined script to determine the mapping. The
        script’s location is controlled by the property topology.script.file.name. The script
        must accept a variable number of arguments that are the hostnames or
        IP addresses to be mapped, and it must emit the corresponding network
        locations to standard output, separated by whitespace. The Hadoop wiki
        has an example at http://wiki.apache.org/hadoop/topology_rack_awareness_scripts.
If no script location is specified, the default behavior is to
        map all nodes to a single network location, called /default-rack.